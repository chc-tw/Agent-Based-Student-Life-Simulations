["1.1 What Operating Systems Do 5\nto monopolize its resources. The goal is to maximize the work (or play) that\nthe user is performing. In this case, the operating system is designed mostly\nfor ease of use ,w i t hs o m ea t t e n t i o np a i dt op e r f o r m a n c ea n dn o n ep a i d\nto resource utilization \u2014how various hardware and software resources are\nshared. Performance is, of course, important to the user; but such systems\nare optimized for the single-user experience rather than the requirements of\nmultiple users.\nIn other cases, a user sits at a terminal connected to a mainframe or a\nminicomputer.O t h e ru s e r sa r ea c c e s s i n gt h es a m ec o m p u t e rt h r o u g ho t h e r\nterminals. These users share resources and may exchange information. The\noperating system in such cases is designed to maximize resource utilization\u2014\nto assure that all available CPU time, memory, and I/O are used ef\ufb01ciently and\nthat no individual user takes more than her fair share.\nIn still other cases, users sit at workstations connected to networks of\nother workstations and servers.T h e s eu s e r sh a v ed e d i c a t e dr e s o u r c e sa t\ntheir disposal, but they also share resources such as networking and servers,\nincluding \ufb01le, compute, and print servers. Therefore, their operating system is\ndesigned to compromise between individual usability and resource utilization.\nRecently, many varieties of mobile computers, such as smartphones and\ntablets, have come into fashion. Most mobile computers are standalone units for\nindividual users. Quite often, they are connected to networks through cellular\nor other wireless technologies. Increasingly,t h e s em o b i l ed e v i c e sa r er e p l a c i n g\ndesktop and laptop computers for people who are primarily interested in\nusing computers for e-mail and web browsing. The user interface for mobile\ncomputers generally features a touch screen,w h e r et h eu s e ri n t e r a c t sw i t ht h e\nsystem by pressing and swiping \ufb01ngers across the screen rather than using a\nphysical keyboard and mouse.\nSome computers have little or no user view. For example, embedded\ncomputers in home devices and automobiles may have numeric keypads and\nmay turn indicator lights on or off to show status, but they and their operating\nsystems are designed primarily to run without user intervention.\n1.1.2 System View\nFrom the computer\u2019s point of view, the operating system is the program\nmost intimately involved with the hardware. In this context, we can view\nan operating system as a resource allocator .Ac o m p u t e rs y s t e mh a sm a n y\nresources that may be required to solve a problem: CPU time, memory space,\n\ufb01le-storage space, I/O devices, and so on. The operating system acts as the\nmanager of these resources. Facing numerous and possibly con\ufb02icting requests\nfor resources, the operating system must decide how to allocate them to speci\ufb01c\nprograms and users so that it can operate the computer system ef\ufb01ciently and\nfairly. As we have seen, resource allocation is especially important where many\nusers access the same mainframe or minicomputer.\nAs l i g h t l yd i f f e r e n tv i e wo fa no p e r a t i n gs y s t e me m p h a s i z e st h en e e dt o\ncontrol the various I/O devices and user programs. An operating system is a\ncontrol program. A control program manages the execution of user programs\nto prevent errors and improper use of thec o m p u t e r .I ti se s p e c i a l l yc o n c e r n e d\nwith the operation and control of I/O devices.6 Chapter 1 Introduction\n1.1.3 De\ufb01ning Operating Systems\nBy now, you can probably see that the termoperating systemcovers many roles\nand functions. That is the case, at least in part, because of the myriad designs\nand uses of computers. Computers are present within toasters, cars, ships,\nspacecraft, homes, and businesses. They are the basis for game machines, music\nplayers, cable TV tuners, and industrial control systems. Although computers\nhave a relatively short history, they have evolved rapidly. Computing started\nas an experiment to determine what could be done and quickly moved to\n\ufb01xed-purpose systems for military uses, such as code breaking and trajectory\nplotting, and governmental uses, such as census calculation. Those early\ncomputers evolved into general-purpose, multifunction mainframes, and\nthat\u2019s when operating systems were born. In the 1960s,Moore\u2019s Lawpredicted\nthat the number of transistors on an integrated circuit would double every\neighteen months, and that prediction has held true. Computers gained in\nfunctionality and shrunk in size, leading to a vast number of uses and a vast\nnumber and variety of operating systems. (See Chapter 20 for more details on\nthe history of operating systems.)\nHow, then, can we de\ufb01ne what an operating system is? In general, we have\nno completely adequate de\ufb01nition of an operating system. Operating systems\nexist because they offer a reasonable way to solve the problem of creating a\nusable computing system. The fundamental goal of computer systems is to\nexecute user programs and to make solving user problems easier. Computer\nhardware is constructed toward this goal. Since bare hardware alone is not\nparticularly easy to use, application programs are developed. These programs\nrequire certain common operations, such as those controlling the I/O devices.\nThe common functions of controlling and allocating resources are then brought\ntogether into one piece of software: the operating system.\nIn addition, we have no universally accepted de\ufb01nition of what is part of the\noperating system. A simple viewpoint is that it includes everything a vendor\nships when you order\u201cthe operating system.\u201d The features included, however,\nvary greatly across systems. Some systems take up less than a megabyte of\nspace and lack even a full-screen editor, whereas others require gigabytes of\nspace and are based entirely on graphical windowing systems. A more common\nde\ufb01nition, and the one that we usually follow, is that the operating system\nis the one program running at all times on the computer\u2014usually called\nthe kernel.( A l o n gw i t ht h ek e r n e l ,t h e r ea r et w oo t h e rt y p e so fp r o g r a m s :\nsystem programs,w h i c ha r ea s s o c i a t e dw i t ht h eo p e r a t i n gs y s t e mb u ta r en o t\nnecessarily part of the kernel, and application programs, which include all\nprograms not associated with the operation of the system.)\nThe matter of what constitutes an operating system became increasingly\nimportant as personal computers became more widespread and operating\nsystems grew increasingly sophisticated. In 1998, the United States Department\nof Justice \ufb01led suit against Microsoft, in essence claiming that Microsoft\nincluded too much functionality in its operating systems and thus prevented\napplication vendors from competing. (For example, a Web browser was an\nintegral part of the operating systems.) As a result, Microsoft was found guilty\nof using its operating-system monopoly to limit competition.\nToday, however, if we look at operating systems for mobile devices, we\nsee that once again the number of features constituting the operating system1.2 Computer-System Organization 7\nis increasing. Mobile operating systems often include not only a core kernel\nbut also middleware\u2014a set of software frameworks that provide additional\nservices to application developers. For example, each of the two most promi-\nnent mobile operating systems\u2014Apple\u2019s i OS and Google\u2019s Android\u2014features\nac o r ek e r n e la l o n gw i t hm i d d l e w a r et h a ts u p p o r t sd a t a b a s e s ,m u l t i m e d i a ,a n d\ngraphics (to name a only few).\n1.2 Computer-System Organization\nBefore we can explore the details of how computer systems operate, we need\ngeneral knowledge of the structure of a computer system. In this section,\nwe look at several parts of this structure. The section is mostly concerned\nwith computer-system organization, so you can skim or skip it if you already\nunderstand the concepts.\n1.2.1 Computer-System Operation\nAm o d e r ng e n e r a l - p u r p o s ec o m p u t e rs y s t e mc o n s i s t so fo n eo rm o r eCPUs\nand a number of device controllers connected through a common bus that\nprovides access to shared memory (Figure 1.2). Each device controller is in\ncharge of a speci\ufb01c type of device (for example, disk drives, audio devices,\nor video displays). The CPU and the device controllers can execute in parallel,\ncompeting for memory cycles. To ensure orderly access to the shared memory,\na memory controller synchronizes access to the memory.\nFor a computer to start running\u2014for instance, when it is powered up or\nrebooted\u2014it needs to have an initial program to run. This initial program,\nor bootstrap program ,t e n d st ob es i m p l e .T y p i c a l l y ,i ti ss t o r e dw i t h i n\nthe computer hardware in read-only memory ( ROM)o re l e c t r i c a l l ye r a s a b l e\nprogrammable read-only memory ( EEPROM), known by the general term\n\ufb01rmware.I ti n i t i a l i z e sa l la s p e c t so ft h es y s t e m ,f r o mCPU registers to device\ncontrollers to memory contents. The bootstrap program must know how to load\nthe operating system and how to start executing that system. To accomplish\nUSB controller\nkeyboard printermouse monitor\ndisks\ngraphics\nadapter\ndisk\ncontroller\nmemory\nCPU\non-line\nFigure 1.2 Am o d e r nc o m p u t e rs y s t e m .", "8 Chapter 1 Introduction\nuser\nprocess\nexecuting\nCPU\nI/O interrupt\nprocessing\nI/O\nrequest\ntransfer\ndone\nI/O\nrequest\ntransfer\ndone\nI/O\ndevice\nidle\ntransferring\nFigure 1.3 Interrupt timeline for a single process doing output.\nthis goal, the bootstrap program must locate the operating-system kernel and\nload it into memory.\nOnce the kernel is loaded and executing, it can start providing services to\nthe system and its users. Some services are provided outside of the kernel, by\nsystem programs that are loaded into memory at boot time to become system\nprocesses,o r system daemons that run the entire time the kernel is running.\nOn UNIX,t h e\ufb01 r s ts y s t e mp r o c e s si s\u201cinit,\u201d and it starts many other daemons.\nOnce this phase is complete, the system is fully booted, and the system waits\nfor some event to occur.\nThe occurrence of an event is usually signaled by an interrupt from either\nthe hardware or the software. Hardware may trigger an interrupt at any time\nby sending a signal to the CPU,u s u a l l yb yw a yo ft h es y s t e mb u s .S o f t w a r e\nmay trigger an interrupt by executing a special operation called a system call\n(also called a monitor call).\nWhen the CPU is interrupted, it stops what it is doing and immediately\ntransfers execution to a \ufb01xed location. The \ufb01xed location usually contains\nthe starting address where the service routine for the interrupt is located.\nThe interrupt service routine executes; on completion, the CPU resumes the\ninterrupted computation. A timeline of this operation is shown in Figure 1.3.\nInterrupts are an important part of a computer architecture. Each computer\ndesign has its own interrupt mechanism, but several functions are common.\nThe interrupt must transfer control to the appropriate interrupt service routine.\nThe straightforward method for handling this transfer would be to invoke\nag e n e r i cr o u t i n et oe x a m i n et h ei n t e r r u p ti n f o r m a t i o n .T h er o u t i n e ,i nt u r n ,\nwould call the interrupt-speci\ufb01c handler. However, interrupts must be handled\nquickly. Since only a prede\ufb01ned number of interrupts is possible, a table of\npointers to interrupt routines can be used instead to provide the necessary\nspeed. The interrupt routine is called indirectly through the table, with no\nintermediate routine needed. Generally, the table of pointers is stored in low\nmemory (the \ufb01rst hundred or so locations). These locations hold the addresses\nof the interrupt service routines for the various devices. This array, orinterrupt\nvector,o fa d d r e s s e si st h e ni n d e x e db yaunique device number, given with\nthe interrupt request, to provide the address of the interrupt service routine for1.2 Computer-System Organization 9\nSTORAGE DEFINITIONS AND NOTATION\nThe basic unit of computer storage is the bit.Ab i tc a nc o n t a i no n eo ft w o\nvalues, 0 and 1. All other storage in a computer is based on collections of bits.\nGiven enough bits, it is amazing how many things a computer can represent:\nnumbers, letters, images, movies, sounds, documents, and programs, to name\naf e w .Abyte is 8 bits, and on most computers it is the smallest convenient\nchunk of storage. For example, most computers don\u2019t have an instruction to\nmove a bit but do have one to move a byte. A less common term is word,\nwhich is a given computer architecture\u2019s native unit of data. A word is made\nup of one or more bytes. For example, a computer that has 64-bit registers and\n64-bit memory addressing typically has 64-bit (8-byte) words. A computer\nexecutes many operations in its native word size rather than a byte at a time.\nComputer storage, along with most computer throughput, is generally\nmeasured and manipulated in bytes and collections of bytes. A kilobyte,o r\nKB,i s1 , 0 2 4b y t e s ;amegabyte,o r MB,i s1 , 0 2 42 bytes; a gigabyte,o r GB,i s\n1,0243 bytes; a terabyte,o r TB,i s1 , 0 2 44 bytes; and a petabyte,o r PB,i s1 , 0 2 45\nbytes. Computer manufacturers often round off these numbers and say that\nam e g a b y t ei s1m i l l i o nb y t e sa n dag i g a b y t ei s1b i l l i o nb y t e s .N e t w o r k i n g\nmeasurements are an exception to this general rule; they are given in bits\n(because networks move data a bit at a time).\nthe interrupting device. Operating systems as different as Windows and UNIX\ndispatch interrupts in this manner.\nThe interrupt architecture must also save the address of the interrupted\ninstruction. Many old designs simply stored the interrupt address in a\n\ufb01xed location or in a location indexed by the device number. More recent\narchitectures store the return address on the system stack. If the interrupt\nroutine needs to modify the processor state\u2014for instance, by modifying\nregister values\u2014it must explicitly save the current state and then restore that\nstate before returning. After the interrupt is serviced, the saved return address\nis loaded into the program counter, and the interrupted computation resumes\nas though the interrupt had not occurred.\n1.2.2 Storage Structure\nThe CPU can load instructions only from memory, so any programs to run must\nbe stored there. General-purpose computers run most of their programs from\nrewritable memory, called main memory (also calledrandom-access memory,\nor RAM). Main memory commonly is implemented in a semiconductor\ntechnology called dynamic random-access memory (DRAM).\nComputers use other forms of memory as well. We have already mentioned\nread-only memory, ROM)a n de l e c t r i c a l l ye r a s a b l ep r o g r a m m a b l er e a d - o n l y\nmemory,EEPROM). BecauseROM cannot be changed, only static programs, such\nas the bootstrap program described earlier, are stored there. The immutability\nof ROM is of use in game cartridges. EEPROM can be changed but cannot\nbe changed frequently and so contains mostly static programs. For example,\nsmartphones have EEPROM to store their factory-installed programs.10 Chapter 1 Introduction\nAll forms of memory provide an array of bytes. Each byte has its\nown address. Interaction is ac hieved through a sequence of load or store\ninstructions to speci\ufb01c memory addresses. The load instruction moves a byte\nor word from main memory to an internal register within theCPU,w h e r e a st h e\nstore instruction moves the content of a register to main memory. Aside from\nexplicit loads and stores, the CPU automatically loads instructions from main\nmemory for execution.\nAt y p i c a li n s t r u c t i o n \u2013 e x e c u t i o nc y c l e ,a se x e c u t e do nas y s t e mw i t havon\nNeumann architecture,\ufb01 r s tf e t c h e sa ni n s t r u c t i o nf r o mm e m o r ya n ds t o r e s\nthat instruction in the instruction register .T h ei n s t r u c t i o ni st h e nd e c o d e d\nand may cause operands to be fetched from memory and stored in some\ninternal register. After the instruction on the operands has been executed, the\nresult may be stored back in memory. Notice that the memory unit sees only\nas t r e a mo fm e m o r ya d d r e s s e s .I td o e sn o tk n o wh o wt h e ya r eg e n e r a t e d( b y\nthe instruction counter, indexing, indirection, literal addresses, or some other\nmeans) or what they are for (instructions or data). Accordingly, we can ignore\nhow am e m o r ya d d r e s si sg e n e r a t e db yap r o g r a m .W ea r ei n t e r e s t e do n l yi n\nthe sequence of memory addresses generated by the running program.\nIdeally, we want the programs and data to reside in main memory\npermanently. This arrangement usually is not possible for the following two\nreasons:\n1. Main memory is usually too small to store all needed programs and data\npermanently.\n2. Main memory is a volatile storage device that loses its contents when\npower is turned off or otherwise lost.\nThus, most computer systems provide secondary storage as an extension of\nmain memory. The main requirement for secondary storage is that it be able to\nhold large quantities of data permanently.\nThe most common secondary-storage device is a magnetic disk ,w h i c h\nprovides storage for both programs and data. Most programs (system and\napplication) are stored on a disk until they are loaded into memory. Many\nprograms then use the disk as both the source and the destination of their\nprocessing. Hence, the proper management of disk storage is of central\nimportance to a computer system, as we discuss in Chapter 10.\nIn a larger sense, however, the storage structure that we have described\u2014\nconsisting of registers, main memory, and magnetic disks\u2014is only one of many\npossible storage systems. Others include cache memory, CD-ROM,m a g n e t i c\ntapes, and so on. Each storage system provides the basic functions of storing\nad a t u ma n dh o l d i n gt h a td a t u mu n t i li ti sr e t r i e v e da tal a t e rt i m e .T h em a i n\ndifferences among the various storage systems lie in speed, cost, size, and\nvolatility.\nThe wide variety of storage systems can be organized in a hierarchy (Figure\n1.4) according to speed and cost. The higher levels are expensive, but they are\nfast. As we move down the hierarchy, the cost per bit generally decreases,\nwhereas the access time generally in creases. This trade-off is reasonable; if a\ngiven storage system were both faster and less expensive than another\u2014other\nproperties being the same\u2014then there would be no reason to use the slower,\nmore expensive memory. In fact, many early storage devices, including paper", "1.2 Computer-System Organization 11\nregisters\ncache\nmain memory\nsolid-state disk\nmagnetic disk\noptical disk\nmagnetic tapes\nFigure 1.4 Storage-device hierarchy.\ntape and core memories, are relegated to museums now that magnetic tape and\nsemiconductor memory have become faster and cheaper. The top four levels\nof memory in Figure 1.4 may be constructed using semiconductor memory.\nIn addition to differing in speed and cost, the various storage systems are\neither volatile or nonvolatile. As mentioned earlier, volatile storage loses its\ncontents when the power to the device is removed. In the absence of expensive\nbattery and generator backup systems, data must be written to nonvolatile\nstorage for safekeeping. In the hierarchy shown in Figure 1.4, the storage\nsystems above the solid-state disk are volatile, whereas those including the\nsolid-state disk and below are nonvolatile.\nSolid-state disks have several variants but in general are faster than\nmagnetic disks and are nonvolatile. One type of solid-state disk stores data in a\nlarge DRAM array during normal operation but also contains a hidden magnetic\nhard disk and a battery for backup power. If external power is interrupted, this\nsolid-state disk\u2019s controller copies the data from RAM to the magnetic disk.\nWhen external power is restored, the controller copies the data back intoRAM.\nAnother form of solid-state disk is \ufb02ash memory, which is popular in cameras\nand personal digital assistants (PDAs),i nr o b o t s ,a n di n c r e a s i n g l yf o rs t o r a g e\non general-purpose computers. Flash memory is slower than DRAM but needs\nno power to retain its contents. Another form of nonvolatile storage isNVRAM,\nwhich is DRAM with battery backup power. This memory can be as fast as\nDRAM and (as long as the battery lasts) is nonvolatile.\nThe design of a complete memory system must balance all the factors just\ndiscussed: it must use only as much expensive memory as necessary while\nproviding as much inexpensive, nonvolatile memory as possible. Caches can12 Chapter 1 Introduction\nbe installed to improve performance where a large disparity in access time or\ntransfer rate exists between two components.\n1.2.3 I/O Structure\nStorage is only one of many types of I/O devices within a computer. A large\nportion of operating system code is dedicated to managing I/O,b o t hb e c a u s e\nof its importance to the reliability and performance of a system and because of\nthe varying nature of the devices. Next, we provide an overview of I/O.\nAg e n e r a l - p u r p o s ec o m p u t e rs y s t e mc o n s i s t so fCPUsa n dm u l t i p l ed e v i c e\ncontrollers that are connected through a common bus. Each device controller\nis in charge of a speci\ufb01c type of device. Depending on the controller, more\nthan one device may be attached. For instance, seven or more devices can be\nattached to the small computer-systems interface (SCSI) controller. A device\ncontroller maintains some local buffer storage and a set of special-purpose\nregisters. The device controller is respon sible for moving the data between\nthe peripheral devices that it controls and its local buffer storage. Typically,\noperating systems have a device driver for each device controller. This device\ndriver understands the device controllerand provides the rest of the operating\nsystem with a uniform interface to the device.\nTo start an I/O operation, the device driver loads the appropriate registers\nwithin the device controller. The device controller, in turn, examines the\ncontents of these registers to determine what action to take (such as \u201cread\nac h a r a c t e rf r o mt h ek e y b o a r d\u201d). The controller starts the transfer of data from\nthe device to its local buffer. Once the transfer of data is complete, the device\ncontroller informs the device driver via an interrupt that it has \ufb01nished its\noperation. The device driver then returns control to the operating system,\npossibly returning the data or a pointer to the data if the operation was a read.\nFor other operations, the device driver returns status information.\nThis form of interrupt-driven I/O is \ufb01ne for moving small amounts of data\nbut can produce high overhead when used for bulk data movement such as disk\nI/O.T os o l v et h i sp r o b l e m ,direct memory access (DMA) is used. After setting\nup buffers, pointers, and counters for the I/O device, the device controller\ntransfers an entire block of data directly to or from its own buffer storage to\nmemory, with no intervention by theCPU.O n l yo n ei n t e r r u p ti sg e n e r a t e dp e r\nblock, to tell the device driver that the operation has completed, rather than\nthe one interrupt per byte generated fo rl o w - s p e e dd e v i c e s .W h i l et h ed e v i c e\ncontroller is performing these operations, the CPU is available to accomplish\nother work.\nSome high-end systems use switch rather than bus architecture. On these\nsystems, multiple components can talk to other components concurrently,\nrather than competing for cycles on a shared bus. In this case, DMA is even\nmore effective. Figure 1.5 shows theinterplay of all components of a computer\nsystem.\n1.3 Computer-System Architecture\nIn Section 1.2, we introduced the general structure of a typical computer system.\nA computer system can be organized in a number of different ways, which we1.3 Computer-System Architecture 13\nthread of execution\ninstructions\nand\ndata\ninstruction execution\ncycle\ndata movement\nDMA\nmemory\ninterrupt\ncache\ndata\nI/O request\nCPU (*N)\ndevice\n(*M)\nFigure 1.5 How a modern computer system works.\ncan categorize roughly according to the number of general-purpose processors\nused.\n1.3.1 Single-Processor Systems\nUntil recently, most computer systems used a single processor. On a single-\nprocessor system, there is one mainCPU capable of executing a general-purpose\ninstruction set, including instructions from user processes. Almost all single-\nprocessor systems have other special-purpose processors as well. They may\ncome in the form of device-speci\ufb01c processors, such as disk, keyboard, and\ngraphics controllers; or, on mainframes, they may come in the form of more\ngeneral-purpose processors, such as I/O processors that move data rapidly\namong the components of the system.\nAll of these special-purpose processors run a limited instruction set and\ndo not run user processes. Sometimes, they are managed by the operating\nsystem, in that the operating system sends them information about their next\ntask and monitors their status. For example, a disk-controller microprocessor\nreceives a sequence of requests from the mainCPU and implements its own disk\nqueue and scheduling algorithm. This arrangement relieves the main CPU of\nthe overhead of disk scheduling.PCsc o n t a i nam i c r o p r o c e s s o ri nt h ek e y b o a r d\nto convert the keystrokes into codes to be sent to the CPU.I no t h e rs y s t e m s\nor circumstances, special-purpose processors are low-level components built\ninto the hardware. The operating system cannot communicate with these\nprocessors; they do their jobs autonomously. The use of special-purpose\nmicroprocessors is common and does not turn a single-processor system into", "14 Chapter 1 Introduction\nam u l t i p r o c e s s o r .I ft h e r ei so n l yo n eg e n e r a l - p u r p o s eCPU, then the system is\nas i n g l e - p r o c e s s o rs y s t e m .\n1.3.2 Multiprocessor Systems\nWithin the past several years,multiprocessor systems (also known as parallel\nsystems or multicore systems ) have begun to dominate the landscape of\ncomputing. Such systems have two or more processors in close communication,\nsharing the computer bus and sometimes the clock, memory, and peripheral\ndevices. Multiprocessor systems \ufb01rst appeared prominently appeared in\nservers and have since migrated to desktop and laptop systems. Recently,\nmultiple processors have appeared on mobile devices such as smartphones\nand tablet computers.\nMultiprocessor systems have three main advantages:\n1. Increased throughput.B yi n c r e a s i n gt h en u m b e ro fp r o c e s s o r s ,w ee x p e c t\nto get more work done in less time. The speed-up ratio withN processors\nis not N, however; rather, it is less than N. When multiple processors\ncooperate on a task, a certain amount of overhead is incurred in keeping\nall the parts working correctly. This overhead, plus contention for shared\nresources, lowers the expected gain from additional processors. Similarly,\nN programmers working closely together do not produce N times the\namount of work a single programmer would produce.\n2. Economy of scale.M u l t i p r o c e s s o rs y s t e m sc a nc o s tl e s st h a ne q u i v a l e n t\nmultiple single-processor systems, because they can share peripherals,\nmass storage, and power supplies. If several programs operate on the\nsame set of data, it is cheaper to store those data on one disk and to have\nall the processors share them than to have many computers with local\ndisks and many copies of the data.\n3. Increased reliability . If functions can be distributed properly among\nseveral processors, then the failure of one processor will not halt the\nsystem, only slow it down. If we have ten processors and one fails, then\neach of the remaining nine processors can pick up a share of the work of\nthe failed processor. Thus, the entire system runs only 10 percent slower,\nrather than failing altogether.\nIncreased reliability of a computer system is crucial in many applications.\nThe ability to continue providing service proportional to the level of surviving\nhardware is called graceful degradation.S o m es y s t e m sg ob e y o n dg r a c e f u l\ndegradation and are called fault tolerant,b e c a u s et h e yc a ns u f f e raf a i l u r eo f\nany single component and still continue operation. Fault tolerance requires\nam e c h a n i s mt oa l l o wt h ef a i l u r et ob ed e t e c t e d ,d i a g n o s e d ,a n d ,i fp o s s i b l e ,\ncorrected. TheHP NonStop (formerly Tandem) system uses both hardware and\nsoftware duplication to ensure continued operation despite faults. The system\nconsists of multiple pairs of CPUs, working in lockstep. Both processors in the\npair execute each instruction and compare the results. If the results differ, then\none CPU of the pair is at fault, and both are halted. The process that was being\nexecuted is then moved to another pair ofCPUs, and the instruction that failed1.3 Computer-System Architecture 15\nis restarted. This solution is expensive, since it involves special hardware and\nconsiderable hardware duplication.\nThe multiple-processor systems in use today are of two types. Some\nsystems use asymmetric multiprocessing, in which each processor is assigned\nas p e c i \ufb01 ct a s k .Aboss processor controls the system; the other processors either\nlook to the boss for instruction or have prede\ufb01ned tasks. This scheme de\ufb01nes\na boss\u2013worker relationship. The boss processor schedules and allocates work\nto the worker processors.\nThe most common systems use symmetric multiprocessing (SMP),i n\nwhich each processor performs all tasks within the operating system. SMP\nmeans that all processors are peers; no boss\u2013worker relationship exists\nbetween processors. Figure 1.6 illustrates a typical SMP architecture. Notice\nthat each processor has its own set of registers, as well as a private\u2014or local\n\u2014cache. However , all processors share physical memory . An example of an\nSMP system is AIX,ac o m m e r c i a lv e r s i o no fUNIX designed by IBM.A n AIX\nsystem can be con\ufb01gured to employ dozens of processors. The bene\ufb01t of this\nmodel is that many processes can run simultaneously\u2014 N processes can run\nif there are N CPUs\u2014without causing performance to deteriorate signi\ufb01cantly .\nHowever, we must carefully control I/O to ensure that the data reach the\nappropriate processor. Also, since the CPUs are separate, one may be sitting\nidle while another is overloaded, resulting in inef\ufb01ciencies. These inef\ufb01ciencies\ncan be avoided if the processors share certain data structures. A multiprocessor\nsystem of this form will allow processes and resources\u2014such as memory\u2014\nto be shared dynamically among the various processors and can lower the\nvariance among the processors. Such a system must be written carefully, as\nwe shall see in Chapter 5. Virtually all modern operating systems\u2014including\nWindows, Mac OS X,a n dL i n u x \u2014 n o wp r o v i d es u p p o r tf o rSMP.\nThe difference between symmetric and asymmetric multiprocessing may\nresult from either hardware or software. Special hardware can differentiate the\nmultiple processors, or the software can be written to allow only one boss and\nmultiple workers. For instance, Sun Microsystems\u2019 operating system SunOS\nVersion 4 provided asymmetric multiprocessing, whereas Version 5 (Solaris) is\nsymmetric on the same hardware.\nMultiprocessing adds CPUst oi n c r e a s ec o m p u t i n gp o w e r .I ft h eCPU has an\nintegrated memory controller, then adding CPUsc a na l s oi n c r e a s et h ea m o u n t\nCPU0\nregisters\ncache\nCPU1\nregisters\ncache\nCPU2\nregisters\ncache\nmemory\nFigure 1.6 Symmetric multiprocessing architecture.16 Chapter 1 Introduction\nof memory addressable in the system. Either way, multiprocessing can cause\nas y s t e mt oc h a n g ei t sm e m o r ya c c e s sm o d e lf r o mu n i f o r mm e m o r ya c c e s s\n(UMA)t on o n - u n i f o r mm e m o r ya c c e s s(NUMA). UMA is de\ufb01ned as the situation\nin which access to anyRAM from any CPU takes the same amount of time. With\nNUMA, some parts of memory may take longer to access than other parts,\ncreating a performance penalty. Operating systems can minimize the NUMA\npenalty through resource management, as discussed in Section 9.5.4.\nAr e c e n tt r e n di nCPU design is to include multiple computing cores\non a single chip. Such multiprocessor systems are termed multicore.T h e y\ncan be more ef\ufb01cient than multiple chips with single cores because on-chip\ncommunication is faster than between-chip communication. In addition, one\nchip with multiple cores uses signi\ufb01cantly less power than multiple single-core\nchips.\nIt is important to note that while multicore systems are multiprocessor\nsystems, not all multiprocessor systems are multicore, as we shall see in Section\n1.3.3. In our coverage of multiprocessor systems throughout this text, unless\nwe state otherwise, we generally use the more contemporary term multicore,\nwhich excludes some multiprocessor systems.\nIn Figure 1.7, we show a dual-core design with two cores on the same\nchip. In this design, each core has its own register set as well as its own local\ncache. Other designs might use a shared cache or a combination of local and\nshared caches. Aside from architectural considerations, such as cache, memory,\nand bus contention, these multicore CPUsa p p e a rt ot h eo p e r a t i n gs y s t e ma s\nN standard processors. This characteristic puts pressure on operating system\ndesigners\u2014and application programmers\u2014to make use of those processing\ncores.\nFinally,blade serversare a relatively recent development in which multiple\nprocessor boards, I/O boards, and networking boards are placed in the same\nchassis. The difference between these and traditional multiprocessor systems\nis that each blade-processor board boots independently and runs its own\noperating system. Some blade-server boards are multiprocessor as well, which\nblurs the lines between types of computers.In essence, these servers consist of\nmultiple independent multiprocessor systems.\nCPU core0\nregisters\ncache\nCPU core1\nregisters\ncache\nmemory\nFigure 1.7 Ad u a l - c o r ed e s i g nw i t ht w oc o r e sp l a c e do nt h es a m ec h i p .", "1.3 Computer-System Architecture 17\n1.3.3 Clustered Systems\nAnother type of multiprocessor system is a clustered system,w h i c hg a t h e r s\ntogether multiple CPUs. Clustered systems differ from the multiprocessor\nsystems described in Section 1.3.2 in that they are composed of two or more\nindividual systems\u2014or nodes\u2014joined together. Such systems are considered\nloosely coupled.E a c hn o d em a yb eas i n g l ep r o c e s s o rs y s t e mo ram u l t i c o r e\nsystem. We should note that the de\ufb01nition of clustered is not concrete; many\ncommercial packages wrestle to de\ufb01ne a clustered system and why one form\nis better than another. The generally accepted de\ufb01nition is that clustered\ncomputers share storage and are closely linked via a local-area network LAN\n(as described in Chapter 17) or a fasteri n t e r c o n n e c t ,s u c ha sI n \ufb01 n i B a n d .\nClustering is usually used to provide high-availability service\u2014that is,\nservice will continue even if one or more systems in the cluster fail. Generally,\nwe obtain high availability by adding a level of redundancy in the system.\nAl a y e ro fc l u s t e rs o f t w a r er u n so nt h ec l u s t e rn o d e s .E a c hn o d ec a nm o n i t o r\none or more of the others (over the LAN). If the monitored machine fails,\nthe monitoring machine can take ownership of its storage and restart the\napplications that were running on the failed machine. The users and clients of\nthe applications see only a brief interruption of service.\nClustering can be structured asymmetrically or symmetrically. In asym-\nmetric clustering ,o n em a c h i n ei si nhot-standby mode while the other is\nrunning the applications. The hot-standby host machine does nothing but\nmonitor the active server. If that server fails, the hot-standby host becomes\nthe active server. In symmetric clustering ,t w oo rm o r eh o s t sa r er u n n i n g\napplications and are monitoring each other. This structure is obviously more\nef\ufb01cient, as it uses all of the available hardware. However it does require that\nmore than one application be available to run.\nSince a cluster consists of several computer systems connected via a\nnetwork, clusters can also be used to provide high-performance computing\nenvironments. Such systems can supply signi\ufb01cantly greater computational\npower than single-processor or even SMP systems because they can run an\napplication concurrently on all computers in the cluster. The application must\nhave been written speci\ufb01cally to take advantage of the cluster, however. This\ninvolves a technique known as parallelization,w h i c hd i v i d e sap r o g r a mi n t o\nseparate components that run in parallel on individual computers in the cluster.\nTypically, these applications are designed so that once each computing node in\nthe cluster has solved its portion of the problem, the results from all the nodes\nare combined into a \ufb01nal solution.\nOther forms of clusters include parallel clusters and clustering over a\nwide-area network (WAN)( a sd e s c r i b e di nC h a p t e r1 7 ) .P a r a l l e lc l u s t e r sa l l o w\nmultiple hosts to access the same data on shared storage. Because most\noperating systems lack support for simultaneous data access by multiple hosts,\nparallel clusters usually require the use of special versions of software and\nspecial releases of applications. For example, Oracle Real Application Cluster\nis a version of Oracle\u2019s database that has been designed to run on a parallel\ncluster. Each machine runs Oracle, and a layer of software tracks access to the\nshared disk. Each machine has full access to all data in the database. To provide\nthis shared access, the system must also supply access control and locking to18 Chapter 1 Introduction\nBEOWULF CLUSTERS\nBeowulf clusters are designed to solve high-performance computing tasks.\nAB e o w u l fc l u s t e rc o n s i s t so fc o m m o d i t yh a r d w a r e \u2014 s u c ha sp e r s o n a l\ncomputers\u2014connected via a simple local-area network. No single speci\ufb01c\nsoftware package is required to construct a cluster. Rather, the nodes use a\nset of open-source software libraries to communicate with one another. Thus,\nthere are a variety of approaches to constructing a Beowulf cluster. Typically,\nthough, Beowulf computing nodes run the Linux operating system. Since\nBeowulf clusters require no special hardware and operate using open-source\nsoftware that is available free, they offer a low-cost strategy for building\nah i g h - p e r f o r m a n c ec o m p u t i n gc l u s t e r .I nf a c t ,s o m eB e o w u l fc l u s t e r sb u i l t\nfrom discarded personal computers are using hundreds of nodes to solve\ncomputationally expensive scienti\ufb01c computing problems.\nensure that no con\ufb02icting operations occur. This function, commonly known\nas a distributed lock manager (DLM),i si n c l u d e di ns o m ec l u s t e rt e c h n o l o g y .\nCluster technology is changing rapidly. Some cluster products support\ndozens of systems in a cluster, as well as clustered nodes that are separated\nby miles. Many of these improvements are made possible by storage-area\nnetworks (SANs),a sd e s c r i b e di nS e c t i o n1 0 . 3 . 3 ,w h i c ha l l o wm a n ys y s t e m s\nto attach to a pool of storage. If the applications and their data are stored on\nthe SAN,t h e nt h ec l u s t e rs o f t w a r ec a na s s i g nt h ea p p l i c a t i o nt or u no na n y\nhost that is attached to the SAN.I ft h eh o s tf a i l s ,t h e na n yo t h e rh o s tc a nt a k e\nover. In a database cluster, dozens of hosts can share the same database, greatly\nincreasing performance and reliability. Figure 1.8 depicts the general structure\nof a clustered system.\ncomputer interconnect computer interconnect computer\nstorage area\nnetwork\nFigure 1.8 General structure of a clustered system.1.4 Operating-System Structure 19\njob 1\n0\nMax\noperating system\njob 2\njob 3\njob 4\nFigure 1.9 Memory layout for a multiprogramming system.\n1.4 Operating-System Structure\nNow that we have discussed basic computer-system organization and archi-\ntecture, we are ready to talk about operating systems. An operating system\nprovides the environment within which programs are executed. Internally,\noperating systems vary greatly in their makeup, since they are organized\nalong many different lines. There a re, however, many commonalities, which\nwe consider in this section.\nOne of the most important aspects of operating systems is the ability\nto multiprogram. A single program cannot, in general, keep either the CPU\nor the I/O devices busy at all times. Single users frequently have multiple\nprograms running.Multiprogramming increases CPU utilization by organizing\njobs (code and data) so that the CPU always has one to execute.\nThe idea is as follows: The operating system keeps several jobs in memory\nsimultaneously (Figure 1.9). Since, in general, main memory is too small to\naccommodate all jobs, the jobs are kept initially on the disk in the job pool.\nThis pool consists of all processes residing on disk awaiting allocation of main\nmemory.\nThe set of jobs in memory can be a subset of the jobs kept in the job\npool. The operating system picks and begins to execute one of the jobs in\nmemory. Eventually, the job may have to wait for some task, such as an I/O\noperation, to complete. In a non-multiprogrammed system, the CPU would sit\nidle. In a multiprogrammed system, the operating system simply switches to,\nand executes, another job. When that job needs to wait, the CPU switches to\nanother job, and so on. Eventually, the \ufb01rst job \ufb01nishes waiting and gets the\nCPU back. As long as at least one job needs to execute, the CPU is never idle.\nThis idea is common in other life situations. A lawyer does not work for\nonly one client at a time, for example. While one case is waiting to go to trial\nor have papers typed, the lawyer can work on another case. If he has enough\nclients, the lawyer will never be idle for lack of work. (Idle lawyers tend to\nbecome politicians, so there is a certain social value in keeping lawyers busy.)", "20 Chapter 1 Introduction\nMultiprogrammed systems provide an environment in which the various\nsystem resources (for example, CPU,m e m o r y ,a n dp e r i p h e r a ld e v i c e s )a r e\nutilized effectively, but they do not provide for user interaction with the\ncomputer system. Time sharing (or multitasking)i sal o g i c a le x t e n s i o no f\nmultiprogramming. In time-sharing systems, the CPU executes multiple jobs\nby switching among them, but the switches occur so frequently that the users\ncan interact with each program while it is running.\nTime sharing requires an interactive computer system, which provides\ndirect communication between the user and the system. The user gives\ninstructions to the operating system or to a program directly, using a input\ndevice such as a keyboard, mouse, touch pad, or touch screen, and waits for\nimmediate results on an output device. Accordingly, theresponse time should\nbe short\u2014typically less than one second.\nAt i m e - s h a r e do p e r a t i n gs y s t e ma l l o w sm a n yu s e r st os h a r et h ec o m p u t e r\nsimultaneously. Since each action or command in a time-shared system tends\nto be short, only a littleCPU time is needed for each user. As the system switches\nrapidly from one user to the next, each user is given the impression that the\nentire computer system is dedicated to his use, even though it is being shared\namong many users.\nAt i m e - s h a r e do p e r a t i n gs y s t e mu s e sCPU scheduling and multiprogram-\nming to provide each user with a small portion of a time-shared computer.\nEach user has at least one separate program in memory. A program loaded into\nmemory and executing is called aprocess.W h e nap r o c e s se x e c u t e s ,i tt y p i c a l l y\nexecutes for only a short time before ite i t h e r\ufb01 n i s h e so rn e e d st op e r f o r mI/O.\nI/O may be interactive; that is, output goes to a display for the user, and input\ncomes from a user keyboard, mouse, or other device. Since interactive I/O\ntypically runs at \u201cpeople speeds,\u201d it may take a long time to complete. Input,\nfor example, may be bounded by the user\u2019s typing speed; seven characters per\nsecond is fast for people but incredibly slow for computers. Rather than let\nthe CPU sit idle as this interactive input takes place, the operating system will\nrapidly switch the CPU to the program of some other user.\nTime sharing and multiprogramming require that several jobs be kept\nsimultaneously in memory. If several jobs are ready to be brought into memory,\nand if there is not enough room for all of them, then the system must choose\namong them. Making this decision involvesjob scheduling,w h i c hw ed i s c u s s\nin Chapter 6. When the operating system selects a job from the job pool, it loads\nthat job into memory for execution. Having several programs in memory at\nthe same time requires some form of memory management, which we cover in\nChapters 8 and 9. In addition, if several jobs are ready to run at the same time,\nthe system must choose which job will run \ufb01rst. Making this decision is CPU\nscheduling,w h i c hi sa l s od i s c u s s e di nC h a p t e r6 .F i n a l l y ,r u n n i n gm u l t i p l e\njobs concurrently requires that their ability to affect one another be limited in\nall phases of the operating system, including process scheduling, disk storage,\nand memory management. We discuss these considerations throughout the\ntext.\nIn a time-sharing system, the operating system must ensure reasonable\nresponse time. This goal is sometimes accomplished through swapping,\nwhereby processes are swapped in and out of main memory to the disk. A more\ncommon method for ensuring reasonable response time is virtual memory,a\ntechnique that allows the execution of a process that is not completely in1.5 Operating-System Operations 21\nmemory (Chapter 9). The main advantage of the virtual-memory scheme is that\nit enables users to run programs that are larger than actual physical memory.\nFurther, it abstracts main memory into a large, uniform array of storage,\nseparating logical memory as viewed by the user from physical memory.\nThis arrangement frees programmers from concern over memory-storage\nlimitations.\nAt i m e - s h a r i n gs y s t e mm u s ta l s op r o v i d ea\ufb01 l es y s t e m( C h a p t e r s1 1a n d\n12). The \ufb01le system resides on a collection of disks; hence, disk management\nmust be provided (Chapter 10). In addition, a time-sharing system provides\nam e c h a n i s mf o rp r o t e c t i n gr e s o u r c e sf r o mi n a p p r o p r i a t eu s e( C h a p t e r1 4 ) .\nTo ensure orderly execution, the system must provide mechanisms for job\nsynchronization and communication (Chapter 5), and it may ensure that jobs\ndo not get stuck in a deadlock, forever waiting for one another (Chapter 7).\n1.5 Operating-System Operations\nAs mentioned earlier, modern operating systems areinterrupt driven.I ft h e r e\nare no processes to execute, no I/O devices to service, and no users to whom\nto respond, an operating system will sit quietly, waiting for something to\nhappen. Events are almost always signaled by the occurrence of an interrupt\nor a trap. A trap (or an exception) is a software-generated interrupt caused\neither by an error (for example, division by zero or invalid memory access)\nor by a speci\ufb01c request from a user program that an operating-system service\nbe performed. The interrupt-driven nature of an operating system de\ufb01nes\nthat system\u2019s general structure. For each type of interrupt, separate segments\nof code in the operating system determine what action should be taken. An\ninterrupt service routine is provided to deal with the interrupt.\nSince the operating system and the users share the hardware and software\nresources of the computer system, we need to make sure that an error in a\nuser program could cause problems only for the one program running. With\nsharing, many processes could be adversely affected by a bug in one program.\nFor example, if a process gets stuck in an in\ufb01nite loop, this loop could prevent\nthe correct operation of many other processes. More subtle errors can occur\nin a multiprogramming system, where one erroneous program might modify\nanother program, the data of another program, or even the operating system\nitself.\nWithout protection against these sorts of errors, either the computer must\nexecute only one process at a time or all output must be suspect. A properly\ndesigned operating system must ensure that an incorrect (or malicious)\nprogram cannot cause other programs to execute incorrectly.\n1.5.1 Dual-Mode and Multimode Operation\nIn order to ensure the proper execution of the operating system, we must be\nable to distinguish between the execution of operating-system code and user-\nde\ufb01ned code. The approach taken by most computer systems is to provide\nhardware support that allows us to differentiate among various modes of\nexecution.22 Chapter 1 Introduction\nuser process executing\nuser process\nkernel\ncalls system call return from system call\nuser mode\n(mode bit = 1)\ntrap\nmode bit = 0\nreturn\nmode bit = 1 kernel mode\n(mode bit = 0)execute system call\nFigure 1.10 Transition from user to kernel mode.\nAt the very least, we need two separate modes of operation: user mode\nand kernel mode (also called supervisor mode, system mode,o r privileged\nmode). A bit, called the mode bit,i sa d d e dt ot h eh a r d w a r eo ft h ec o m p u t e r\nto indicate the current mode: kernel (0) or user (1). With the mode bit, we can\ndistinguish between a task that is exec uted on behalf of the operating system\nand one that is executed on behalf of the user. When the computer system is\nexecuting on behalf of a user application, the system is in user mode. However,\nwhen a user application requests a service from the operating system (via a\nsystem call), the system must transition from user to kernel mode to ful\ufb01ll\nthe request. This is shown in Figure 1.10. As we shall see, this architectural\nenhancement is useful for many other aspects of system operation as well.\nAt system boot time, the hardware starts in kernel mode. The operating\nsystem is then loaded and starts user applications in user mode. Whenever a\ntrap or interrupt occurs, the hardware switches from user mode to kernel mode\n(that is, changes the state of the mode bit to 0). Thus, whenever the operating\nsystem gains control of the computer, it is in kernel mode. The system always\nswitches to user mode (by setting the mode bit to 1) before passing control to\nau s e rp r o g r a m .\nThe dual mode of operation provides us with the means for protecting the\noperating system from errant users\u2014and errant users from one another. We\naccomplish this protection by designating some of the machine instructions that\nmay cause harm as privileged instructions. The hardware allows privileged\ninstructions to be executed only in kernel mode. If an attempt is made to\nexecute a privileged instruction in user mode, the hardware does not execute\nthe instruction but rather treats it as illegal and traps it to the operating system.\nThe instruction to switch to kernel mode is an example of a privileged\ninstruction. Some other examples include I/O control, timer management, and\ninterrupt management. As we shall see throughout the text, there are many\nadditional privileged instructions.\nThe concept of modes can be extended beyond two modes (in which case\nthe CPU uses more than one bit to set and test the mode). CPUst h a ts u p p o r t\nvirtualization (Section 16.1) frequently have a separate mode to indicate when\nthe virtual machine manager (VMM)\u2014and the virtualization management\nsoftware\u2014is in control of the system. In this mode, the VMM has more\nprivileges than user processes but fewer than the kernel. It needs that level\nof privilege so it can create and manage virtual machines, changing the CPU\nstate to do so. Sometimes, too, different modes are used by various kernel", "1.5 Operating-System Operations 23\ncomponents. We should note that, as an alternative to modes, theCPU designer\nmay use other methods to diffe rentiate operational privileges. The Intel 64\nfamily of CPUs supports four privilege levels, for example, and supports\nvirtualization but does not have a separate mode for virtualization.\nWe can now see the life cycle of instruction execution in a computer system.\nInitial control resides in the operating system, where instructions are executed\nin kernel mode. When control is given to a user application, the mode is set to\nuser mode. Eventually, control is switched back to the operating system via an\ninterrupt, a trap, or a system call.\nSystem calls provide the means for a user program to ask the operating\nsystem to perform tasks reserved for the operating system on the user\nprogram\u2019s behalf. A system call is invoked in a variety of ways, depending\non the functionality provided by the underlying processor. In all forms, it is the\nmethod used by a process to request action by the operating system. A system\ncall usually takes the form of a trap to a speci\ufb01c location in the interrupt vector.\nThis trap can be executed by a generictrap instruction, although some systems\n(such as MIPS)h a v eas p e c i \ufb01 csyscall instruction to invoke a system call.\nWhen a system call is executed, it is typically treated by the hardware\nas a software interrupt. Control passes through the interrupt vector to a\nservice routine in the operating system, and the mode bit is set to kernel\nmode. The system-call service routine is a part of the operating system. The\nkernel examines the interrupting instruction to determine what system call\nhas occurred; a parameter indicates what type of service the user program is\nrequesting. Additional information needed for the request may be passed in\nregisters, on the stack, or in memory (with pointers to the memory locations\npassed in registers). The kernel veri\ufb01es th at the parameters are correct and\nlegal, executes the request, and returns control to the instruction following the\nsystem call. We describe system calls more fully in Section 2.3.\nThe lack of a hardware-supported dual mode can cause serious shortcom-\nings in an operating system. For instance, MS-DOS was written for the Intel\n8088 architecture, which has no mode bit and therefore no dual mode. A user\nprogram running awry can wipe out the operating system by writing over it\nwith data; and multiple programs are able to write to a device at the same\ntime, with potentially disastrous results. Modern versions of the Intel CPU\ndo provide dual-mode operation. Accordingly, most contemporary operating\nsystems\u2014such as Microsoft Windows 7, as well as Unix and Linux\u2014take\nadvantage of this dual-mode feature and provide greater protection for the\noperating system.\nOnce hardware protection is in place, it detects errors that violate modes.\nThese errors are normally handled by the operating system. If a user program\nfails in some way\u2014such as by making an attempt either to execute an illegal\ninstruction or to access memory that is not in the user\u2019s address space\u2014then\nthe hardware traps to the operating system. The trap transfers control through\nthe interrupt vector to the operating system, just as an interrupt does. When\nap r o g r a me r r o ro c c u r s ,t h eo p e r a t i n gs y s t e mm u s tt e r m i n a t et h ep r o g r a m\nabnormally. This situation is handled by the same code as a user-requested\nabnormal termination. An appropriate error message is given, and the memory\nof the program may be dumped. The memory dump is usually written to a\n\ufb01le so that the user or programmer can examine it and perhaps correct it and\nrestart the program.24 Chapter 1 Introduction\n1.5.2 Timer\nWe must ensure that the operating system maintains control over the CPU.\nWe cannot allow a user program to get stuck in an in\ufb01nite loop or to fail\nto call system services and never return control to the operating system. To\naccomplish this goal, we can use a timer.At i m e rc a nb es e tt oi n t e r r u p t\nthe computer after a speci\ufb01ed period. The period may be \ufb01xed (for example,\n1/60 second) or variable (for example, from 1 millisecond to 1 second). A\nvariable timer is generally implemented by a \ufb01xed-rate clock and a counter.\nThe operating system sets the counter. Every time the clock ticks, the counter\nis decremented. When the counter reaches 0, an interrupt occurs. For instance,\na1 0 - b i tc o u n t e rw i t ha1 - m i l l i s e c o n dc l o c ka l l o w si n t e r r u p t sa ti n t e r v a l sf r o m\n1m i l l i s e c o n dt o1 , 0 2 4m i l l i s e c o n d s ,i ns t e p so f1m i l l i s e c o n d .\nBefore turning over control to the user, the operating system ensures\nthat the timer is set to interrupt. If the timer interrupts, control transfers\nautomatically to the operating system, which may treat the interrupt as a fatal\nerror or may give the program more time. Clearly, instructions that modify the\ncontent of the timer are privileged.\nWe can use the timer to prevent a user program from running too long.\nA simple technique is to initialize a counter with the amount of time that a\nprogram is allowed to run. A program with a 7-minute time limit, for example,\nwould have its counter initialized to 420. Every second, the timer interrupts,\nand the counter is decremented by 1. As long as the counter is positive, control\nis returned to the user program. When the counter becomes negative, the\noperating system terminates the program for exceeding the assigned time\nlimit.\n1.6 Process Management\nA program does nothing unless its instructions are executed by a CPU.A\nprogram in execution, as mentioned,is a process. A time-shared user program\nsuch as a compiler is a process. A word-processing program being run by an\nindividual user on a PC is a process. A system task, such as sending output\nto a printer, can also be a process (or at least part of one). For now, you can\nconsider a process to be a job or a time-shared program, but later you will learn\nthat the concept is more general. As we shall see in Chapter 3, it is possible\nto provide system calls that allow processes to create subprocesses to execute\nconcurrently.\nAp r o c e s sn e e d sc e r t a i nr e s o u r c e s \u2014 i n c l u d i n gCPU time, memory, \ufb01les,\nand I/O devices\u2014to accomplish its task. These resources are either given to\nthe process when it is created or allocated to it while it is running. In addition\nto the various physical and logical resources that a process obtains when it is\ncreated, various initialization data (input) may be passed along. For example,\nconsider a process whose function is to display the status of a \ufb01le on the screen\nof a terminal. The process will be given the name of the \ufb01le as an input and will\nexecute the appropriate instructions and system calls to obtain and display\nthe desired information on the terminal. When the process terminates, the\noperating system will reclaim any reusable resources.\nWe emphasize that a program by itself is not a process. A program is a\npassive entity, like the contents of a \ufb01le stored on disk, whereas a process1.7 Memory Management 25\nis an active entity. A single-threaded process has one program counter\nspecifying the next instruction to execute. (Threads are covered in Chapter\n4.) The execution of such a process must be sequential. The CPU executes one\ninstruction of the process after anoth er, until the process completes. Further,\nat any time, one instruction at most is executed on behalf of the process. Thus,\nalthough two processes may be associated with the same program, they are\nnevertheless considered two separate execution sequences. A multithreaded\nprocess has multiple program counters, each pointing to the next instruction\nto execute for a given thread.\nAp r o c e s si st h eu n i to fw o r ki nas y s t e m .As y s t e mc o n s i s t so fac o l l e c t i o n\nof processes, some of which are operating-system processes (those that execute\nsystem code) and the rest of which are user processes (those that execute\nuser code). All these processes can potentially execute concurrently\u2014by\nmultiplexing on a single CPU,f o re x a m p l e .\nThe operating system is responsible for the following activities in connec-\ntion with process management:\n\u2022 Scheduling processes and threads on the CPUs\n\u2022 Creating and deleting both user and system processes\n\u2022 Suspending and resuming processes\n\u2022 Providing mechanisms for process synchronization\n\u2022 Providing mechanisms for process communication\nWe discuss process-management techniques in Chapters 3 through 5.\n1.7 Memory Management\nAs we discussed in Section 1.2.2, the main memory is central to the operation\nof a modern computer system. Main memory is a large array of bytes, ranging\nin size from hundreds of thousands to billions. Each byte has its own address.\nMain memory is a repository of quickly accessible data shared by theCPU and\nI/O devices. The central processor readsinstructions from main memory during\nthe instruction-fetch cycle and both reads and writes data from main memory\nduring the data-fetch cycle (on a von Neumann architecture). As noted earlier,\nthe main memory is generally the only large storage device that theCPU is able\nto address and access directly. For example, for the CPU to process data from\ndisk, those data must \ufb01rst be transferred to main memory by CPU-generated\nI/O calls. In the same way, instructions must be in memory for the CPU to\nexecute them.\nFor a program to be executed, it must be mapped to absolute addresses and\nloaded into memory. As the program executes, it accesses program instructions\nand data from memory by generating these absolute addresses. Eventually,\nthe program terminates, its memory space is declared available, and the next\nprogram can be loaded and executed.\nTo improve both the utilization of theCPU and the speed of the computer\u2019s\nresponse to its users, general-purpose computers must keep several programs\nin memory, creating a need for memory management. Many different memory-", "26 Chapter 1 Introduction\nmanagement schemes are used. Theseschemes re\ufb02ect various approaches, and\nthe effectiveness of any given algorithm depends on the situation. In selecting a\nmemory-management scheme for a speci\ufb01c system, we must take into account\nmany factors\u2014especially the hardware design of the system. Each algorithm\nrequires its own hardware support.\nThe operating system is responsible for the following activities in connec-\ntion with memory management:\n\u2022 Keeping track of which parts of memory are currently being used and who\nis using them\n\u2022 Deciding which processes (or parts of processes) and data to move into\nand out of memory\n\u2022 Allocating and deallocating memory space as needed\nMemory-management techniques are discussed in Chapters 8 and 9.\n1.8 Storage Management\nTo make the computer system convenient for users, the operating system\nprovides a uniform, logical view of information storage. The operating system\nabstracts from the physical properties of its storage devices to de\ufb01ne a logical\nstorage unit, the \ufb01le.T h eo p e r a t i n gs y s t e mm a p s\ufb01 l e so n t op h y s i c a lm e d i aa n d\naccesses these \ufb01les via the storage devices.\n1.8.1 File-System Management\nFile management is one of the most visible components of an operating system.\nComputers can store information on several different types of physical media.\nMagnetic disk, optical disk, and magnetic tape are the most common. Each\nof these media has its own characteristics and physical organization. Each\nmedium is controlled by a device, such as a disk drive or tape drive, that\nalso has its own unique characteristics. These properties include access speed,\ncapacity, data-transfer rate, and access method (sequential or random).\nA\ufb01 l ei sac o l l e c t i o no fr e l a t e di n f o r m a t i o nd e \ufb01 n e db yi t sc r e a t o r .C o m m o n l y ,\n\ufb01les represent programs (both source and object forms) and data. Data \ufb01les may\nbe numeric, alphabetic, alphanumeric, or binary. Files may be free-form (for\nexample, text \ufb01les), or they may be formatted rigidly (for example, \ufb01xed \ufb01elds).\nClearly, the concept of a \ufb01le is an extremely general one.\nThe operating system implements the abstract concept of a \ufb01le by managing\nmass-storage media, such as tapes and disks, and the devices that control them.\nIn addition, \ufb01les are normally organized into directories to make them easier\nto use. Finally, when multiple users have access to \ufb01les, it may be desirable\nto control which user may access a \ufb01le and how that user may access it (for\nexample, read, write, append).\nThe operating system is responsible for the following activities in connec-\ntion with \ufb01le management:\n\u2022 Creating and deleting \ufb01les1.8 Storage Management 27\n\u2022 Creating and deleting directories to organize \ufb01les\n\u2022 Supporting primitives for manipulating \ufb01les and directories\n\u2022 Mapping \ufb01les onto secondary storage\n\u2022 Backing up \ufb01les on stable (nonvolatile) storage media\nFile-management techniques are discussed in Chapters 11 and 12.\n1.8.2 Mass-Storage Management\nAs we have already seen, because main memory is too small to accommodate\nall data and programs, and because the data that it holds are lost when power\nis lost, the computer system must provide secondary storage to back up main\nmemory. Most modern computer systems use disks as the principal on-line\nstorage medium for both programs and data. Most programs\u2014including\ncompilers, assemblers, word processors, editors, and formatters\u2014are stored\non a disk until loaded into memory. They then use the disk as both the source\nand destination of their processing. Hence, the proper management of disk\nstorage is of central importance to a computer system. The operating system is\nresponsible for the following activities in connection with disk management:\n\u2022 Free-space management\n\u2022 Storage allocation\n\u2022 Disk scheduling\nBecause secondary storage is used frequently, it must be used ef\ufb01ciently. The\nentire speed of operation of a computer may hinge on the speeds of the disk\nsubsystem and the algorithms that manipulate that subsystem.\nThere are, however, many uses for storage that is slower and lower in\ncost (and sometimes of higher capacity) than secondary storage. Backups of\ndisk data, storage of seldom-used data, and long-term archival storage are\nsome examples. Magnetic tape drives and their tapes and CD and DVD drives\nand platters are typical tertiary storage devices. The media (tapes and optical\nplatters) vary between WORM (write-once, read-many-times) and RW (read\u2013\nwrite) formats.\nTertiary storage is not crucial to system performance, but it still must\nbe managed. Some operating systems take on this task, while others leave\ntertiary-storage management to application programs. Some of the functions\nthat operating systems can provide include mounting and unmounting media\nin devices, allocating and freeing the devices for exclusive use by processes,\nand migrating data from secondary to tertiary storage.\nTechniques for secondary and tertiary storage management are discussed\nin Chapter 10.\n1.8.3 Caching\nCaching is an important principle of computer systems. Here\u2019s how it works.\nInformation is normally kept in some storage system (such as main memory).\nAs it is used, it is copied into a faster storage system\u2014the cache\u2014on a28 Chapter 1 Introduction\ntemporary basis. When we need a particular piece of information, we \ufb01rst\ncheck whether it is in the cache. If it is, we use the information directly from\nthe cache. If it is not, we use the information from the source, putting a copy\nin the cache under the assumption that we will need it again soon.\nIn addition, internal programmable registers, such as index registers,\nprovide a high-speed cache for main memory. The programmer (or compiler)\nimplements the register-allocation and register-replacement algorithms to\ndecide which information to keep in registers and which to keep in main\nmemory.\nOther caches are implemented totally in hardware. For instance, most\nsystems have an instruction cache to hold the instructions expected to be\nexecuted next. Without this cache, the CPU would have to wait several cycles\nwhile an instruction was fetched from main memory. For similar reasons, most\nsystems have one or more high-speed data caches in the memory hierarchy.\nWe are not concerned with these hardware-only caches in this text, since they\nare outside the control of the operating system.\nBecause caches have limited size, cache management is an important\ndesign problem. Careful selection of the cache size and of a replacement policy\ncan result in greatly increased performance. Figure 1.11 compares storage\nperformance in large workstations and small servers. Various replacement\nalgorithms for software-controlled caches are discussed in Chapter 9.\nMain memory can be viewed as a fast cache for secondary storage, since\ndata in secondary storage must be copied into main memory for use and\ndata must be in main memory before being moved to secondary storage for\nsafekeeping. The \ufb01le-system data, which resides permanently on secondary\nstorage, may appear on several levels in the storage hierarchy. At the highest\nlevel, the operating system may maintain a cache of \ufb01le-system data in main\nmemory. In addition, solid-state disks may be used for high-speed storage that\nis accessed through the \ufb01le-system interface. The bulk of secondary storage\nis on magnetic disks. The magnetic-disk storage, in turn, is often backed up\nonto magnetic tapes or removable disks to protect against data loss in case\nof a hard-disk failure. Some systems automatically archive old \ufb01le data from\nsecondary storage to tertiary storage, such as tape jukeboxes, to lower the\nstorage cost (see Chapter 10).\nLevel\nName\nTypical size\nImplementation\ntechnology\nAccess time (ns)\nBandwidth (MB/sec)\nManaged by\nBacked by\n1\nregisters\n< 1 KB\ncustom memory\nwith multiple\nports CMOS\n0.25 - 0.5\n20,000 - 100,000\ncompiler\ncache\n2\ncache\n< 16MB\non-chip or\noff-chip\nCMOS SRAM\n0.5 - 25\n5,000 - 10,000\nhardware\nmain memory\n3\nmain memory\n< 64GB\nCMOS SRAM\n80 - 250\n1,000 - 5,000\noperating system\ndisk\n4\nsolid state disk\n< 1 TB\nflash memory\n25,000 - 50,000\n500\noperating system\ndisk\n5\nmagnetic disk\n< 10 TB\nmagnetic disk\n5,000,000\n20 - 150\noperating system\ndisk or tape\nFigure 1.11 Performance of various levels of storage.", "1.8 Storage Management 29\nA A Amagnetic\ndisk\nmain\nmemory\nhardware\nregistercache\nFigure 1.12 Migration of integer A from disk to register.\nThe movement of information between levels of a storage hierarchy may\nbe either explicit or implicit, depending on the hardware design and the\ncontrolling operating-system software. For instance, data transfer from cache\nto CPU and registers is usually a hardware function, with no operating-system\nintervention. In contrast, transfer of data from disk to memory is usually\ncontrolled by the operating system.\nIn a hierarchical storage structure, the same data may appear in different\nlevels of the storage system. For example, suppose that an integer A that is to\nbe incremented by 1 is located in \ufb01le B, and \ufb01le B resides on magnetic disk.\nThe increment operation proceeds by \ufb01rst issuing anI/O operation to copy the\ndisk block on which A resides to main memory. This operation is followed by\ncopying A to the cache and to an internal register. Thus, the copy of A appears\nin several places: on the magnetic disk, in main memory, in the cache, and in an\ninternal register (see Figure 1.12). Once the increment takes place in the internal\nregister, the value of A differs in the various storage systems. The value of A\nbecomes the same only after the new value of A is written from the internal\nregister back to the magnetic disk.\nIn a computing environment where only one process executes at a time,\nthis arrangement poses no dif\ufb01culties, since an access to integer A will always\nbe to the copy at the highest level of the hierarchy. However, in a multitasking\nenvironment, where the CPU is switched back and forth among various\nprocesses, extreme care must be taken to ensure that, if several processes wish\nto access A, then each of these processes will obtain the most recently updated\nvalue of A.\nThe situation becomes more complicated in a multiprocessor environment\nwhere, in addition to maintaining internal registers, each of the CPUsa l s o\ncontains a local cache (Figure 1.6). In such an environment, a copy of A may\nexist simultaneously in several caches. Since the various CPUs can all execute\nin parallel, we must make sure that an update to the value of A in one cache\nis immediately re\ufb02ected in all other caches where A resides. This situation is\ncalled cache coherency, and it is usually a hardware issue (handled below the\noperating-system level).\nIn a distributed environment, the situation becomes even more complex.\nIn this environment, several copies (or replicas) of the same \ufb01le can be kept on\ndifferent computers. Since the various replicas may be accessed and updated\nconcurrently, some distributed systems ensure that, when a replica is updated\nin one place, all other replicas are brought up to date as soon as possible. There\nare various ways to achieve this guarantee, as we discuss in Chapter 17.\n1.8.4 I/O Systems\nOne of the purposes of an operating system is to hide the peculiarities of speci\ufb01c\nhardware devices from the user. For example, in UNIX,t h ep e c u l i a r i t i e so fI/O30 Chapter 1 Introduction\ndevices are hidden from the bulk of the operating system itself by the I/O\nsubsystem.T h eI/O subsystem consists of several components:\n\u2022 Am e m o r y - m a n a g e m e n tc o m p o n e n tt h a ti n c l u d e sb u f f e r i n g ,c a c h i n g ,a n d\nspooling\n\u2022 Ag e n e r a ld e v i c e - d r i v e ri n t e r f a c e\n\u2022 Drivers for speci\ufb01c hardware devices\nOnly the device driver knows the pecu liarities of the speci\ufb01c device to which\nit is assigned.\nWe discussed in Section 1.2.3 how interrupt handlers and device drivers are\nused in the construction of ef\ufb01cient I/O subsystems. In Chapter 13, we discuss\nhow the I/O subsystem interfaces to the other system components, manages\ndevices, transfers data, and detects I/O completion.\n1.9 Protection and Security\nIf a computer system has multiple users and allows the concurrent execution\nof multiple processes, then access to data must be regulated. For that purpose,\nmechanisms ensure that \ufb01les, memory segments,CPU,a n do t h e rr e s o u r c e sc a n\nbe operated on by only those processes that have gained proper authoriza-\ntion from the operating system. For example, memory-addressing hardware\nensures that a process can execute on ly within its own address space. The\ntimer ensures that no process can gain control of the CPU without eventually\nrelinquishing control. Device-control registers are not accessible to users, so\nthe integrity of the various peripheral devices is protected.\nProtection,t h e n ,i sa n ym e c h a n i s mf o rc o n t r o l l i n gt h ea c c e s so fp r o c e s s e s\nor users to the resources de\ufb01ned by a computer system. This mechanism must\nprovide means to specify the controls to be imposed and to enforce the controls.\nProtection can improve reliability by detecting latent errors at the interfaces\nbetween component subsystems. Early detection of interface errors can often\nprevent contamination of a healthy subsystem by another subsystem that is\nmalfunctioning. Furthermore, an unprotected resource cannot defend against\nuse (or misuse) by an unauthorized or incompetent user. A protection-oriented\nsystem provides a means to distinguish between authorized and unauthorized\nusage, as we discuss in Chapter 14.\nAs y s t e mc a nh a v ea d e q u a t ep r o t e c t i o nb u ts t i l lb ep r o n et of a i l u r ea n d\nallow inappropriate access. Consider a user whose authentication information\n(her means of identifying herself to the system) is stolen. Her data could be\ncopied or deleted, even though \ufb01le and memory protection are working. It is\nthe job of security to defend a system from external and internal attacks. Such\nattacks spread across a huge range and include viruses and worms, denial-of-\nservice attacks (which use all of a system\u2019s resources and so keep legitimate\nusers out of the system), identity theft, and theft of service (unauthorized\nuse of a system). Prevention of some of these attacks is considered an\noperating-system function on some systems, while other systems leave it to\npolicy or additional software. Due to the alarming rise in security incidents,1.10 Kernel Data Structures 31\noperating-system security features represent a fast-growing area of research\nand implementation. We discuss security in Chapter 15.\nProtection and security require the system to be able to distinguish among\nall its users. Most operating systems maintain a list of user names and\nassociated user identi\ufb01ers (user IDs).I nW i n d o w sp a r l a n c e ,t h i si sasecurity\nID (SID).T h e s en u m e r i c a lIDsa r eu n i q u e ,o n ep e ru s e r .W h e nau s e rl o g si n\nto the system, the authentication stage determines the appropriate user ID for\nthe user. That userID is associated with all of the user\u2019s processes and threads.\nWhen an ID needs to be readable by a user, it is translated back to the user\nname via the user name list.\nIn some circumstances, we wish to distinguish among sets of users rather\nthan individual users. For example, the owner of a \ufb01le on aUNIX system may be\nallowed to issue all operations on that \ufb01le, whereas a selected set of users may\nbe allowed only to read the \ufb01le. To accomplish this, we need to de\ufb01ne a group\nname and the set of users belonging to that group. Group functionality can\nbe implemented as a system-wide list of group names and group identi\ufb01ers.\nAu s e rc a nb ei no n eo rm o r eg r o u p s ,d e p e n d i n go no p e r a t i n g - s y s t e md e s i g n\ndecisions. The user\u2019s group IDsa r ea l s oi n c l u d e di ne v e r ya s s o c i a t e dp r o c e s s\nand thread.\nIn the course of normal system use, the user ID and group ID for a user\nare suf\ufb01cient. However, a user sometimes needs to escalate privileges to gain\nextra permissions for an activity. The user may need access to a device that is\nrestricted, for example. Operating systems provide various methods to allow\nprivilege escalation. On UNIX,f o ri n s t a n c e ,t h esetuid attribute on a program\ncauses that program to run with the userID of the owner of the \ufb01le, rather than\nthe current user\u2019sID.T h ep r o c e s sr u n sw i t ht h i seffective UID until it turns off\nthe extra privileges or terminates.\n1.10 Kernel Data Structures\nWe turn next to a topic central to operating-system implementation: the way\ndata are structured in the system. In this section, we brie\ufb02y describe several\nfundamental data structures used extensively in operating systems. Readers\nwho require further details on these structures, as well as others, should consult\nthe bibliography at the end of the chapter.\n1.10.1 Lists, Stacks, and Queues\nAn array is a simple data structure in which each element can be accessed\ndirectly. For example, main memory is constructed as an array. If the data item\nbeing stored is larger than one byte, then multiple bytes can be allocated to the\nitem, and the item is addressed as item number \u00d7 item size. But what about\nstoring an item whose size may vary? And what about removing an item if the\nrelative positions of the remaining items must be preserved? In such situations,\narrays give way to other data structures.\nAfter arrays, lists are perhaps the most fundamental data structures in\ncomputer science. Whereas each item in an array can be accessed directly, the\nitems in a list must be accessed in a particular order. That is, a list represents\na collection of data values as a sequence. The most common method for", "32 Chapter 1 Introduction\ndata data data null\n\u2022\u2022 \u2022\nFigure 1.13 Singly linked list.\nimplementing this structure is a linked list,i nw h i c hi t e m sa r el i n k e dt oo n e\nanother. Linked lists are of several types:\n\u2022 In a singly linked list, each item points to its successor, as illustrated in\nFigure 1.13.\n\u2022 In a doubly linked list, ag i v e ni t e mc a nr e f e re i t h e rt oi t sp r e d e c e s s o ro r\nto its successor, as illustrated in Figure 1.14.\n\u2022 In a circularly linked list, the last element in the list refers to the \ufb01rst\nelement, rather than to null, as illustrated in Figure 1.15.\nLinked lists accommodate items of varying sizes and allow easy insertion\nand deletion of items. One potential disadvantage of using a list is that\nperformance for retrieving a speci\ufb01ed item in a list of size n is linear \u2014 O(n),\nas it requires potentially traversing all n elements in the worst case. Lists\nare sometimes used directly by kernel algorithms. Frequently, though, they\nare used for constructing more powerful data structures, such as stacks and\nqueues.\nA stack is a sequentially ordered data structure that uses the last in, \ufb01rst\nout (LIFO) principle for adding and removing items, meaning that the last item\nplaced onto a stack is the \ufb01rst item removed. The operations for inserting and\nremoving items from a stack are known as push and pop,r e s p e c t i v e l y .A n\noperating system often uses a stack when invoking function calls. Parameters,\nlocal variables, and the return address are pushed onto the stack when a\nfunction is called; returning from the function call pops those items off the\nstack.\nA queue,i nc o n t r a s t ,i sas e q u e n t i a l l yo r d e r e dd a t as t r u c t u r et h a tu s e st h e\n\ufb01rst in, \ufb01rst out (FIFO) principle: items are removed from a queue in the order\nin which they were inserted. There are many everyday examples of queues,\nincluding shoppers waiting in a checkout line at a store and cars waiting in line\nat a traf\ufb01c signal. Queues are also quite common in operating systems\u2014jobs\nthat are sent to a printer are typically printed in the order in which they were\nsubmitted, for example. As we shall see in Chapter 6, tasks that are waiting to\nbe run on an available CPU are often organized in queues.\ndata null nulldata data data\n\u2022\u2022 \u2022\nFigure 1.14 Doubly linked list.1.10 Kernel Data Structures 33\ndata data data data\n\u2022\u2022 \u2022\nFigure 1.15 Circularly linked list.\n1.10.2 Trees\nA tree is a data structure that can be used to represent data hierarchically. Data\nvalues in a tree structure are linked through parent\u2013child relationships. In a\ngeneral tree, a parent may have an unlimited number of children. In a binary\ntree,ap a r e n tm a yh a v ea tm o s tt w oc h i l d r e n ,w h i c hw et e r mt h eleft child\nand the right child .A binary search tree additionally requires an ordering\nbetween the parent\u2019s two children in whichlef t\n child <= right\n child .F i g u r e\n1.16 provides an example of a binary search tree. When we search for an item in\nab i n a r ys e a r c ht r e e ,t h ew o r s t - c a s ep e r f o r m a n c ei sO(n)( c o n s i d e rh o wt h i sc a n\noccur). To remedy this situation, we can use an algorithm to create abalanced\nbinary search tree. Here, a tree containing n items has at most lg nlevels, thus\nensuring worst-case performance of O(lg n). We shall see in Section 6.7.1 that\nLinux uses a balanced binary search tree as part itsCPU-scheduling algorithm.\n1.10.3 Hash Functions and Maps\nA hash function takes data as its input, performs a numeric operation on this\ndata, and returns a numeric value. This numeric value can then be used as an\nindex into a table (typically an array) to quickly retrieve the data. Whereas\nsearching for a data item through a list of size n can require up to O(n)\ncomparisons in the worst case, using a hash function for retrieving data from\ntable can be as good as O(1) in the worst case, depending on implementation\ndetails. Because of this performance, has h functions are used extensively in\noperating systems.\n17\n35\n40\n42\n12\n146\nFigure 1.16 Binary search tree.34 Chapter 1 Introduction\n01 .. n\nvalue\nhash map\nhash_function(key)\nFigure 1.17 Hash map.\nOne potential dif\ufb01culty with hash functions is that two inputs can result\nin the same output value\u2014that is, they can link to the same table location.\nWe can accommodate this hash collision by having a linked list at that table\nlocation that contains all of the items with the same hash value. Of course, the\nmore collisions there are, the less ef\ufb01cient the hash function is.\nOne use of a hash function is to implement a hash map,w h i c ha s s o c i a t e s\n(or maps) [key:value] pairs using a hash function. For example, we can map\nthe key operating to the value system.O n c et h em a p p i n gi se s t a b l i s h e d ,w ec a n\napply the hash function to the key to obtain the value from the hash map\n(Figure 1.17). For example, suppose that a user name is mapped to a password.\nPassword authentication then proceeds as follows: a user enters his user name\nand password. The hash function is applied to the user name, which is then\nused to retrieve the password. The retrieved password is then compared with\nthe password entered by the user for authentication.\n1.10.4 Bitmaps\nA bitmap is a string ofn binary digits that can be used to represent the status of\nn items. For example, suppose we have several resources, and the availability\nof each resource is indicated by the value of a binary digit: 0 means that the\nresource is available, while 1 indicates that it is unavailable (or vice-versa). The\nvalue of the ith position in the bitmap is associated with theith resource. As an\nexample, consider the bitmap shown below:\n001011101\nResources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available.\nThe power of bitmaps becomes apparen t when we consider their space\nef\ufb01ciency. If we were to use an eight-bit Boolean value instead of a single bit,\nthe resulting data structure would be eight times larger. Thus, bitmaps are\ncommonly used when there is a need to represent the availability of a large\nnumber of resources. Disk drives provide a nice illustration. A medium-sized\ndisk drive might be divided into several thousand individual units, calleddisk\nblocks.Ab i t m a pc a nb eu s e dt oi n d i c a t et h ea v a i l a b i l i t yo fe a c hd i s kb l o c k .\nData structures are pervasive in operating system implementations. Thus,\nwe will see the structures discussed here, along with others, throughout this\ntext as we explore kernel algorithms and their implementations.", "1.11 Computing Environments 35\nLINUX KERNEL DATA STRUCTURES\nThe data structures used in the Linux kernel are available in the kernel source\ncode. The include \ufb01le <linux/list.h> provides details of the linked-list\ndata structure used throughout the kernel. A queue in Linux is known as\na kfifo,a n di t si m p l e m e n t a t i o nc a nb ef o u n di nt h ekfifo.c \ufb01le in the\nkernel directory of the source code. Linux also provides a balanced binary\nsearch tree implementation using red-black trees.D e t a i l sc a nb ef o u n di nt h e\ninclude \ufb01le <linux/rbtree.h>.\n1.11 Computing Environments\nSo far, we have brie\ufb02y described several aspects of computer systems and the\noperating systems that manage them. We turn now to a discussion of how\noperating systems are used in a variety of computing environments.\n1.11.1 Traditional Computing\nAs computing has matured, the lines separating many of the traditional com-\nputing environments have blurred. Consider the\u201ctypical of\ufb01ce environment.\u201d\nJust a few years ago, this environment consisted ofPCsc o n n e c t e dt oan e t w o r k ,\nwith servers providing \ufb01le and print services. Remote access was awkward,\nand portability was achieved by use of laptop computers. Terminals attached\nto mainframes were prevalent at many c ompanies as well, with even fewer\nremote access and portability options.\nThe current trend is toward providing more ways to access these computing\nenvironments. Web technologies and increasingWANbandwidth are stretching\nthe boundaries of traditional computing. Companies establish portals,w h i c h\nprovide Web accessibility to their internal servers. Network computers (or\nthin clients )\u2014which are essentially terminals that understand web-based\ncomputing\u2014are used in place of traditional workstations where more security\nor easier maintenance is desired. Mobile computers can synchronize with PCs\nto allow very portable use of company information. Mobile computers can also\nconnect to wireless networks and cellular data networks to use the company\u2019s\nWeb portal (as well as the myriad other Web resources).\nAt home, most users once had a single computer with a slow modem\nconnection to the of\ufb01ce, the Internet, or both. Today, network-connection\nspeeds once available only at great c ost are relatively inexpensive in many\nplaces, giving home users more access to more data. These fast data connections\nare allowing home computers to serve up Web pages and to run networks that\ninclude printers, client PCs, and servers. Many homes use \ufb01rewalls to protect\ntheir networks from security breaches.\nIn the latter half of the 20th century, computing resources were relatively\nscarce. (Before that, they were nonexistent!) For a period of time, systems\nwere either batch or interactive. Batch systems processed jobs in bulk, with\npredetermined input from \ufb01les or other data sources. Interactive systems\nwaited for input from users. To optimize the use of the computing resources,\nmultiple users shared time on these systems. Time-sharing systems used a36 Chapter 1 Introduction\ntimer and scheduling algorithms to cycle processes rapidly through the CPU,\ngiving each user a share of the resources.\nToday, traditional time-sharing systems are uncommon. The same schedul-\ning technique is still in use on desktop computers, laptops, servers, and even\nmobile computers, but frequently all the processes are owned by the same\nuser (or a single user and the operating system). User processes, and system\nprocesses that provide services to the user,a r em a n a g e ds ot h a te a c hf r e q u e n t l y\ngets a slice of computer time. Consider the windows created while a user\nis working on a PC,f o re x a m p l e ,a n dt h ef a c tt h a tt h e ym a yb ep e r f o r m i n g\ndifferent tasks at the same time. Even a web browser can be composed of\nmultiple processes, one for each website currently being visited, with time\nsharing applied to each web browser process.\n1.11.2 Mobile Computing\nMobile computing refers to computing on handheld smartphones and tablet\ncomputers. These devices share the disting uishing physical features of being\nportable and lightweight. Historically, compared with desktop and laptop\ncomputers, mobile systems gave up screen size, memory capacity, and overall\nfunctionality in return for handheld mobile access to services such as e-mail\nand web browsing. Over the past few years, however, features on mobile\ndevices have become so rich that the distinction in functionality between, say,\na consumer laptop and a tablet computer may be dif\ufb01cult to discern. In fact,\nwe might argue that the features of a contemporary mobile device allow it to\nprovide functionality that is either unavailable or impractical on a desktop or\nlaptop computer.\nToday, mobile systems are used not only for e-mail and web browsing but\nalso for playing music and video, reading digital books, taking photos, and\nrecording high-de\ufb01nition video. Accordingly, tremendous growth continues\nin the wide range of applications tha tr u no ns u c hd e v i c e s .M a n yd e v e l o p e r s\nare now designing applications that take advantage of the unique features of\nmobile devices, such as global positioning system (GPS)c h i p s ,a c c e l e r o m e t e r s ,\nand gyroscopes. An embeddedGPS chip allows a mobile device to use satellites\nto determine its precise location on earth. That functionality is especially useful\nin designing applications that provide navigation\u2014for example, telling users\nwhich way to walk or drive or perhaps directing them to nearby services, such\nas restaurants. An accelerometer allows a mobile device to detect its orientation\nwith respect to the ground and to detect certain other forces, such as tilting\nand shaking. In several computer games that employ accelerometers, players\ninterface with the system not by using a mouse or a keyboard but rather by\ntilting, rotating, and shaking the mobile device! Perhaps more a practical use\nof these features is found in augmented-reality applications, which overlay\ninformation on a display of the current environment. It is dif\ufb01cult to imagine\nhow equivalent applications could be developed on traditional laptop or\ndesktop computer systems.\nTo provide access to on-line services, mobile devices typically use either\nIEEE standard 802.11 wireless or cellular data networks. The memory capacity\nand processing speed of mobile devices, however, are more limited than those\nof PCs. Whereas a smartphone or tablet may have 64 GB in storage, it is not\nuncommon to \ufb01nd 1 TB in storage on a desktop computer. Similarly, because1.11 Computing Environments 37\npower consumption is such a concern, mobile devices often use processors that\nare smaller, are slower, and offer fewer processing cores than processors found\non traditional desktop and laptop computers.\nTwo operating systems currently dominate mobile computing: Apple iOS\nand Google Android .i OS was designed to run on Apple iPhone and iPad\nmobile devices. Android powers smartphones and tablet computers available\nfrom many manufacturers. We examine these two mobile operating systems in\nfurther detail in Chapter 2.\n1.11.3 Distributed Systems\nAd i s t r i b u t e ds y s t e mi sac o l l e c t i o no fp h y s i c a l l ys e p a r a t e ,p o s s i b l yh e t e r o g e -\nneous, computer systems that are networked to provide users with access to\nthe various resources that the system maintains. Access to a shared resource\nincreases computation speed, functionality, data availability, and reliability.\nSome operating systems generalize network access as a form of \ufb01le access, with\nthe details of networking contained in the network interface\u2019s device driver.\nOthers make users speci\ufb01cally invoke network functions. Generally, systems\ncontain a mix of the two modes\u2014for example FTP and NFS.T h ep r o t o c o l s\nthat create a distributed system can greatly affect that system\u2019s utility and\npopularity.\nA network,i nt h es i m p l e s tt e r m s ,i sac o m m u n i c a t i o np a t hb e t w e e n\ntwo or more systems. Distributed systems depend on networking for their\nfunctionality. Networks vary by the pro tocols used, the distances between\nnodes, and the transport media. TCP/IP is the most common network protocol,\nand it provides the fundamental architecture of the Internet. Most operating\nsystems support TCP/IP,i n c l u d i n ga l lg e n e r a l - p u r p o s eo n e s .S o m es y s t e m s\nsupport proprietary protocols to suit their needs. To an operating system, a\nnetwork protocol simply needs an in terface device\u2014a network adapter, for\nexample\u2014with a device driver to manage it, as well as software to handle\ndata. These concepts are discussed throughout this book.\nNetworks are characterized based on th ed i s t a n c e sb e t w e e nt h e i rn o d e s .\nA local-area network (LAN) connects computers within a room, a building,\nor a campus. A wide-area network (WAN) usually links buildings, cities, or\ncountries. A global company may have aWAN to connect its of\ufb01ces worldwide,\nfor example. These networks may run one protocol or several protocols. The\ncontinuing advent of new technologies b rings about new forms of networks.\nFor example, a metropolitan-area network (MAN) could link buildings within\nac i t y .B l u e T o o t ha n d8 0 2 . 1 1d e v i c e su s ew i r e l e s st e c h n o l o g yt oc o m m u n i c a t e\nover a distance of several feet, in essence creating a personal-area network\n(PAN) between a phone and a headset or a smartphone and a desktop computer.\nThe media to carry networks are equally varied. They include copper wires,\n\ufb01ber strands, and wireless transmissions between satellites, microwave dishes,\nand radios. When computing devices are connected to cellular phones, they\ncreate a network. Even very short-range infrared communication can be used\nfor networking. At a rudimentary level, whenever computers communicate,\nthey use or create a network. These n etworks also vary in their performance\nand reliability.\nSome operating systems have taken the concept of networks and dis-\ntributed systems further than the notion of providing network connectivity.", "38 Chapter 1 Introduction\nA network operating system is an operating system that provides features\nsuch as \ufb01le sharing across the network, along with a communication scheme\nthat allows different processes on different computers to exchange messages.\nA computer running a network operating system acts autonomously from all\nother computers on the network, although it is aware of the network and is\nable to communicate with other networked computers. A distributed operating\nsystem provides a less autonomous environment. The different computers\ncommunicate closely enough to provide the illusion that only a single operating\nsystem controls the network. We cover computer networks and distributed\nsystems in Chapter 17.\n1.11.4 Client\u2013Server Computing\nAs PCsh a v eb e c o m ef a s t e r ,m o r ep o w erful, and cheaper, designers have shifted\naway from centralized system architecture. Terminals connected to centralized\nsystems are now being supplanted by PCsa n dm o b i l ed e v i c e s .C o r r e s p o n d -\ningly, user-interface functionality once handled directly by centralized systems\nis increasingly being handled by PCs, quite often through a web interface. As\nar e s u l t ,m a n yo ft o d a y \u2019 ss y s t e m sa c ta sserver systems to satisfy requests\ngenerated byclient systems.T h i sf o r mo fs p e c i a l i z e dd i s t r i b u t e ds y s t e m ,c a l l e d\na client\u2013server system, has the general structure depicted in Figure 1.18.\nServer systems can be broadly categorized as compute servers and \ufb01le\nservers:\n\u2022 The compute-server system provides an interface to which a client can\nsend a request to perform an action (for example, read data). In response,\nthe server executes the action and sends the results to the client. A server\nrunning a database that responds to client requests for data is an example\nof such a system.\n\u2022 The \ufb01le-server system provides a \ufb01le-system interface where clients can\ncreate, update, read, and delete \ufb01les. An example of such a system is a web\nserver that delivers \ufb01les to clients running web browsers.\nServer Network\nclient\ndesktop\nclient\nlaptop\nclient\nsmartphone\nFigure 1.18 General structure of a client\u2013server system.1.11 Computing Environments 39\n1.11.5 Peer-to-Peer Computing\nAnother structure for a distributed system is the peer-to-peer ( P2P)s y s t e m\nmodel. In this model, clients and servers are not distinguished from one\nanother. Instead, all nodes within the system are considered peers, and each\nmay act as either a client or a server, depending on whether it is requesting or\nproviding a service. Peer-to-peer systems offer an advantage over traditional\nclient-server systems. In a client-server system, the server is a bottleneck; but\nin a peer-to-peer system, services can be provided by several nodes distributed\nthroughout the network.\nTo participate in a peer-to-peer system, a node must \ufb01rst join the network\nof peers. Once a node has joined the netw ork, it can begin providing services\nto\u2014and requesting services from\u2014other nodes in the network. Determining\nwhat services are available is accomplished in one of two general ways:\n\u2022 When a node joins a network, it registers its service with a centralized\nlookup service on the network. Any node desiring a speci\ufb01c service \ufb01rst\ncontacts this centralized lookup service to determine which node provides\nthe service. The remainder of the communication takes place between the\nclient and the service provider.\n\u2022 An alternative scheme uses no centralized lookup service. Instead, a peer\nacting as a client must discover what node provides a desired service by\nbroadcasting a request for the service to all other nodes in the network. The\nnode (or nodes) providing that service responds to the peer making the\nrequest. To support this approach, adiscovery protocol must be provided\nthat allows peers to discover servic es provided by other peers in the\nnetwork. Figure 1.19 illustrates such a scenario.\nPeer-to-peer networks gained widespread popularity in the late 1990s with\nseveral \ufb01le-sharing services, such as Napster and Gnutella, that enabled peers\nto exchange \ufb01les with one another. The Napster system used an approach\nsimilar to the \ufb01rst type described ab ove: a centralized server maintained an\nindex of all \ufb01les stored on peer nodes in the Napster network, and the actual\nclient\nclientclient\nclient client\nFigure 1.19 Peer-to-peer system with no centralized service.40 Chapter 1 Introduction\nexchange of \ufb01les took place between the peer nodes. The Gnutella system used\na technique similar to the second type: a client broadcasted \ufb01le requests to\nother nodes in the system, and nodes that could service the request responded\ndirectly to the client. The future of exchanging \ufb01les remains uncertain because\npeer-to-peer networks can be used to exchange copyrighted materials (music,\nfor example) anonymously, and there are laws governing the distribution of\ncopyrighted material. Notably, Napster ran into legal trouble for copyright\ninfringement and its services were shut down in 2001.\nSkype is another example of peer-to-peer computing. It allows clients to\nmake voice calls and video calls and to s end text messages over the Internet\nusing a technology known as voice over IP (VoIP).S k y p eu s e sah y b r i dp e e r -\nto-peer approach. It includes a centralized login server, but it also incorporates\ndecentralized peers and allows two peers to communicate.\n1.11.6 Virtualization\nVirtualization is a technology that allows operating systems to run as appli-\ncations within other operating systems. At \ufb01rst blush, there seems to be\nlittle reason for such functionality. But the virtualization industry is vast and\ngrowing, which is a testament to its utility and importance.\nBroadly speaking, virtualization is one member of a class of software\nthat also includes emulation. Emulation is used when the source CPU type\nis different from the targetCPU type. For example, when Apple switched from\nthe IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers,\nit included an emulation facility called \u201cRosetta,\u201d which allowed applications\ncompiled for the IBM CPU to run on the Intel CPU.T h a ts a m ec o n c e p tc a nb e\nextended to allow an entire operating system written for one platform to run\non another. Emulation comes at a heavy price, however. Every machine-level\ninstruction that runs natively on the source system must be translated to the\nequivalent function on the target system, frequently resulting in several target\ninstructions. If the source and targetCPUsh a v es i m i l a rp e r f o r m a n c el e v e l s ,t h e\nemulated code can run much slower than the native code.\nA common example of emulation occurs when a computer language is\nnot compiled to native code but instead is either executed in its high-level\nform or translated to an intermediate form. This is known as interpretation.\nSome languages, such as BASIC,c a nb ee i t h e rc o m p i l e do ri n t e r p r e t e d .J a v a ,i n\ncontrast, is always interpreted. Interpretation is a form of emulation in that the\nhigh-level language code is translated to native CPU instructions, emulating\nnot anotherCPU but a theoretical virtual machine on which that language could\nrun natively. Thus, we can run Java programs on \u201cJava virtual machines,\u201d but\ntechnically those virtual machines are Java emulators.\nWith virtualization,i nc o n t r a s t ,a no p e r a t i n gs y s t e mt h a ti sn a t i v e l yc o m -\npiled for a particular CPU architecture runs within another operating system\nalso native to that CPU.V i r t u a l i z a t i o n\ufb01 r s tc a m ea b o u to nIBM mainframes\nas a method for multiple users to run tasks concurrently. Running multiple\nvirtual machines allowed (and still allows) many users to run tasks on a system\ndesigned for a single user. Later, in response to problems with running multiple\nMicrosoft Windows XP applications on the Intel x86 CPU, VMware created a\nnew virtualization technology in the form of an application that ran on XP.\nThat application ran one or more guest copies of Windows or other native", "1.11 Computing Environments 41\n(a)\nprocesses\nhardware\nkernel\n(b)\nprogramming\ninterface\nprocesses\nprocesses\nprocesses\nkernelkernel kernel\nVM2VM1 VM3\nmanager\nhardware\nvirtual machine\nFigure 1.20 VMware.\nx86 operating systems, each running its own applications. (See Figure 1.20.)\nWindows was the host operating system, and the VMware application was the\nvirtual machine manager VMM.T h e VMM runs the guest operating systems,\nmanages their resource use, and protects each guest from the others.\nEven though modern operating systems are fully capable of running\nmultiple applications reliably, the use of virtualization continues to grow. On\nlaptops and desktops, a VMM allows the user to install multiple operating\nsystems for exploration or to run applications written for operating systems\nother than the native host. For example, an Apple laptop running Mac OS\nX on the x86 CPU can run a Windows guest to allow execution of Windows\napplications. Companies writing software for multiple operating systems\ncan use virtualization to run all of those operating systems on a single\nphysical server for development, testing, andd e b u g g i n g .W i t h i nd a t ac e n t e r s ,\nvirtualization has become a common method of executing and managing\ncomputing environments. VMMsl i k eVMware, ESX,a n dC i t r i xX e n S e r v e rn o\nlonger run on host operating systems but rather are the hosts. Full details of\nthe features and implementation of virtualization are found in Chapter 16.\n1.11.7 Cloud Computing\nCloud computing is a type of computing that delivers computing, storage,\nand even applications as a service across a network. In some ways, it\u2019s a\nlogical extension of virtualization, because it uses virtualization as a base for\nits functionality. For example, the Amazon Elastic Compute Cloud(EC2) facility\nhas thousands of servers, millions of virtual machines, and petabytes of storage\navailable for use by anyone on the Internet. Users pay per month based on how\nmuch of those resources they use.\nThere are actually many types of cloud computing, including the following:\n\u2022 Public cloud\u2014a cloud available via the Internet to anyone willing to pay\nfor the services42 Chapter 1 Introduction\n\u2022 Private cloud\u2014a cloud run by a company for that company\u2019s own use\n\u2022 Hybrid cloud \u2014a cloud that includes both public and private cloud\ncomponents\n\u2022 Software as a service (SaaS)\u2014one or more applications (such as word\nprocessors or spreadsheets) available via the Internet\n\u2022 Platform as a service (PaaS)\u2014a software stack ready for application use\nvia the Internet (for example, a database server)\n\u2022 Infrastructure as a service (IaaS)\u2014servers or storage available over the\nInternet (for example, storage available for making backup copies of\nproduction data)\nThese cloud-computing types are not discrete, as a cloud computing environ-\nment may provide a combination of several types. For example, an organization\nmay provide both SaaS and IaaS as a publicly available service.\nCertainly, there are traditional operating systems within many of the\ntypes of cloud infrastructure. Beyond those are the VMMst h a tm a n a g et h e\nvirtual machines in which the user processes run. At a higher level, the VMMs\nthemselves are managed by cloud management tools, such as Vware vCloud\nDirector and the open-source Eucalyptus toolset. These tools manage the\nresources within a given cloud and provide interfaces to the cloud components,\nmaking a good argument for considering them a new type of operating system.\nFigure 1.21 illustrates a public cloud providing IaaS. Notice that both the\ncloud services and the cloud user interface are protected by a \ufb01rewall.\nfirewall\ncloud\ncustomer\ninterface\nload balancer\nvirtual\nmachines\nvirtual\nmachines\nservers servers\nstorage\nInternet\ncustomer\nrequests\ncloud\nmanagement\ncommands\ncloud\nmanagment\nservices\nFigure 1.21 Cloud computing.1.12 Open-Source Operating Systems 43\n1.11.8 Real-Time Embedded Systems\nEmbedded computers are the most prevalent form of computers in existence.\nThese devices are found everywhere, f rom car engines and manufacturing\nrobots to DVDsa n dm i c r o w a v eo v e n s .T h e yt e n dt oh a v ev e r ys p e c i \ufb01 ct a s k s .\nThe systems they run on are usually primitive, and so the operating systems\nprovide limited features. Usually, they havelittle or no user interface, preferring\nto spend their time monitoring and managing hardware devices, such as\nautomobile engines and robotic arms.\nThese embedded systems vary considerably. Some are general-purpose\ncomputers, running standard operating systems\u2014such as Linux\u2014with\nspecial-purpose applications to implement the functionality. Others are hard-\nware devices with a special-purpose embedded operating system providing\njust the functionality desired. Yet others are hardware devices with application-\nspeci\ufb01c integrated circuits (ASICs)t h a tp e r f o r mt h e i rt a s k sw i t h o u ta no p e r a t -\ning system.\nThe use of embedded systems continues to expand. The power of these\ndevices, both as standalone units and as elements of networks and the web,\nis sure to increase as well. Even now, entire houses can be computerized, so\nthat a central computer\u2014either a general-purpose computer or an embedded\nsystem\u2014can control heating and lighting, alarm systems, and even coffee\nmakers. Web access can enable a home owner to tell the house to heat up\nbefore she arrives home. Someday, the refrigerator can notify the grocery store\nwhen it notices the milk is gone.\nEmbedded systems almost always run real-time operating systems .A\nreal-time system is used when rigid time requirements have been placed on\nthe operation of a processor or the \ufb02ow of data; thus, it is often used as a\ncontrol device in a dedicated application. Sensors bring data to the computer.\nThe computer must analyze the data and possibly adjust controls to modify\nthe sensor inputs. Systems that control scienti\ufb01c experiments, medical imaging\nsystems, industrial control systems, and certain display systems are real-\ntime systems. Some automobile-engine fuel-injection systems, home-appliance\ncontrollers, and weapon systems are also real-time systems.\nAr e a l - t i m es y s t e mh a sw e l l - d e \ufb01 n e d ,\ufb01 x e dt i m ec o n s t r a i n t s .P r o c e s s i n g\nmust be done within the de\ufb01ned constraints, or the system will fail. For\ninstance, it would not do for a robot arm to be instructed to halt after it had\nsmashed into the car it was building. A real-time system functions correctly\nonly if it returns the correct result within its time constraints. Contrast this\nsystem with a time-sharing system, where it is desirable (but not mandatory)\nto respond quickly, or a batch system, which may have no time constraints at\nall.\nIn Chapter 6, we consider the scheduling facility needed to implement\nreal-time functionality in an operating system. In Chapter 9, we describe the\ndesign of memory management for real-time computing. Finally, in Chapters\n18 and 19, we describe the real-time components of the Linux and Windows 7\noperating systems.\n1.12 Open-Source Operating Systems\nWe noted at the beginning of this chapter that the study of operating systems\nhas been made easier by the availability of a vast number of open-source", "44 Chapter 1 Introduction\nreleases. Open-source operating systems are those available in source-code\nformat rather than as compiled binary code. Linux is the most famous open-\nsource operating system, while Microsoft Windows is a well-known example\nof the opposite closed-source approach. Apple\u2019s MacOS X and iOS operating\nsystems comprise a hybrid approach. They contain an open-source kernel\nnamed Darwin yet include proprietary, closed-source components as well.\nStarting with the source code allows the programmer to produce binary\ncode that can be executed on a system. Doing the opposite\u2014 reverse engi-\nneering the source code from the binaries\u2014is quite a lot of work, and useful\nitems such as comments are never recovered. Learning operating systems by\nexamining the source code has other bene\ufb01ts as well. With the source code\nin hand, a student can modify the operating system and then compile and\nrun the code to try out those changes, which is an excellent learning tool.\nThis text includes projects that involve modifying operating-system source\ncode, while also describing algorithms at ah i g hl e v e lt ob es u r ea l li m p o r t a n t\noperating-system topics are covered. Throughout the text, we provide pointers\nto examples of open-source code for deeper study.\nThere are many bene\ufb01ts to open-source operating systems, including a\ncommunity of interested (and usually unpaid) programmers who contribute\nto the code by helping to debug it, analyze it, provide support, and suggest\nchanges. Arguably, open-source code is more secure than closed-source code\nbecause many more eyes are viewing the code. Certainly, open-source code has\nbugs, but open-source advocates argue that bugs tend to be found and \ufb01xed\nfaster owing to the number of people using and viewing the code. Companies\nthat earn revenue from selling their programs often hesitate to open-source\ntheir code, but Red Hat and a myriad of other companies are doing just that\nand showing that commercial companies bene\ufb01t,rather than suffer, when they\nopen-source their code. Revenue can b eg e n e r a t e dt h r o u g hs u p p o r tc o n t r a c t s\nand the sale of hardware on which the software runs, for example.\n1.12.1 History\nIn the early days of modern computing (that is, the 1950s), a great deal of\nsoftware was available in open-source format. The original hackers (computer\nenthusiasts) at MIT\u2019s Tech Model Railroad Club left their programs in drawers\nfor others to work on. \u201cHomebrew\u201d user groups exchanged code during their\nmeetings. Later, company-speci\ufb01c user groups, such as Digital Equipment\nCorporation\u2019sDEC, accepted contributions of source-code programs, collected\nthem onto tapes, and distributed the tapes to interested members.\nComputer and software companies eventually sought to limit the use of\ntheir software to authorized computers and paying customers. Releasing only\nthe binary \ufb01les compiled from the source code, rather than the source code\nitself, helped them to achieve this goal, as well as protecting their code and their\nideas from their competitors. Another issue involved copyrighted material.\nOperating systems and other programs can limit the ability to play back movies\nand music or display electronic books to authorized computers. Such copy\nprotection or digital rights management (DRM) would not be effective if the\nsource code that implemented these limits were published. Laws in many\ncountries, including the U.S. Digital Millennium Copyright Act (DMCA), make\nit illegal to reverse-engineer DRM code or otherwise try to circumvent copy\nprotection.1.12 Open-Source Operating Systems 45\nTo counter the move to limit software use and redistribution, Richard\nStallman in 1983 started the GNU project to create a free, open-source, UNIX-\ncompatible operating system. In 1985, he published theGNU Manifesto, which\nargues that all software should be free and open-sourced. He also formed\nthe Free Software Foundation (FSF) with the goal of encouraging the free\nexchange of software source code and the free use of that software. Rather than\ncopyright its software, the FSF \u201ccopylefts\u201d the software to encourage sharing\nand improvement. TheGNU General Public License(GPL) codi\ufb01es copylefting\nand is a common license under which free software is released. Fundamentally,\nGPL requires that the source code be distributed with any binaries and that any\nchanges made to the source code be released under the same GPL license.\n1.12.2 Linux\nAs an example of an open-source operating system, consider GNU/Linux.\nThe GNU project produced many UNIX-compatible tools, including compilers,\neditors, and utilities, but never released a kernel. In 1991, a student in\nFinland, Linus Torvalds, released a rudimentary UNIX-like kernel using the\nGNU compilers and tools and invited contributions worldwide. The advent of\nthe Internet meant that anyone interested could download the source code,\nmodify it, and submit changes to Torvalds. Releasing updates once a week\nallowed this so-called Linux operating system to grow rapidly, enhanced by\nseveral thousand programmers.\nThe resulting GNU/Linux operating system has spawned hundreds of\nunique distributions,o rc u s t o mb u i l d s ,o ft h es y s t e m .M a j o rd i s t r i b u t i o n s\ninclude RedHat, SUSE,F e d o r a ,D e b i a n ,S l a c k w a r e ,a n dU b u n t u .D i s t r i b u t i o n s\nvary in function, utility, installed applications, hardware support, user inter-\nface, and purpose. For example, RedHat Enterprise Linux is geared to large\ncommercial use. PCLinuxOS is a LiveCD\u2014an operating system that can be\nbooted and run from a CD-ROM without being installed on a system\u2019s hard\ndisk. One variant of PCLinuxOS\u2014called \u201cPCLinuxOS Supergamer DVD\u201d\u2014is a\nLiveDVD that includes graphics drivers and games. A gamer can run it on\nany compatible system simply by booting from the DVD.W h e nt h eg a m e ri s\n\ufb01nished, a reboot of the system resets it to its installed operating system.\nYou can run Linux on a Windows system using the following simple, free\napproach:\n1. Download the free \u201cVMware Player\u201d tool from\nhttp://www.vmware.com/download/player/\nand install it on your system.\n2. Choose a Linux version from among the hundreds of \u201cappliances,\u201d or\nvirtual machine images, available from VMware at\nhttp://www.vmware.com/appliances/\nThese images are preinstalled with operating systems and applications\nand include many \ufb02avors of Linux.46 Chapter 1 Introduction\n3. Boot the virtual machine within VMware Player.\nWith this text, we provide a virtual machine image of Linux running the Debian\nrelease. This image contains the Linux source code as well as tools for software\ndevelopment. We cover examples involving thatL i n u xi m a g et h r o u g h o u tt h i s\ntext, as well as in a detailed case study in Chapter 18.\n1.12.3 BSD UNIX\nBSD UNIX has a longer and more complicated history than Linux. It started in\n1978 as a derivative ofAT&T\u2019sUNIX.R e l e a s e sf r o mt h eU n i v e r s i t yo fC a l i f o r n i a\nat Berkeley ( UCB)c a m ei ns o u r c ea n db i n a r yf o r m ,b u tt h e yw e r en o to p e n -\nsource because a license fromAT&T was required.BSD UNIX\u2019s development was\nslowed by a lawsuit by AT&T, but eventually a fully functional, open-source\nversion, 4.4BSD-lite, was released in 1994.\nJust as with Linux, there are many distributions of BSD UNIX,i n c l u d i n g\nFreeBSD, NetBSD, OpenBSD,a n dD r a g o n \ufb02 yBSD.T oe x p l o r et h es o u r c ec o d e\nof FreeBSD,s i m p l yd o w n l o a dt h ev i r t u a lm a c h i n ei m a g eo ft h ev e r s i o no f\ninterest and boot it within VMware, as described above for Linux. The source\ncode comes with the distribution and is stored in /usr/src/.T h ek e r n e l\nsource code is in /usr/src/sys.F o re x a m p l e ,t oe x a m i n et h ev i r t u a lm e m o r y\nimplementation code in the FreeBSD kernel, see the \ufb01les in /usr/src/sys/vm.\nDarwin, the core kernel component of Mac OS X ,i sb a s e do n BSD\nUNIX and is open-sourced as well. That source code is available from\nhttp://www.opensource.apple.com/.E v e r yM a cOS X release has its open-\nsource components posted at that site. The name of the package that contains\nthe kernel begins with \u201cxnu.\u201d Apple also provides extensive developer tools,\ndocumentation, and support at http://connect.apple.com.F o rm o r ei n f o r m a -\ntion, see Appendix A.\n1.12.4 Solaris\nSolaris is the commercial UNIX-based operating system of Sun Microsystems.\nOriginally, Sun\u2019sSunOS operating system was based onBSD UNIX.S u nm o v e d\nto AT&T\u2019s System VUNIX as its base in 1991. In 2005, Sun open-sourced most\nof the Solaris code as the OpenSolaris project. The purchase of Sun by Oracle\nin 2009, however, left the state of this project unclear. The source code as it\nwas in 2005 is still available via a source code browser and for download at\nhttp://src.opensolaris.org/source.\nSeveral groups interested in using OpenSolaris have started from that base\nand expanded its features. Their working set is Project Illumos, which has\nexpanded from the OpenSolaris base to include more features and to be the\nbasis for several products. Illumos is available at http://wiki.illumos.org.\n1.12.5 Open-Source Systems as Learning Tools\nThe free software movement is driving legions of programmers to create\nthousands of open-source projects, including operating systems. Sites like\nhttp://freshmeat.net/ and http://distrowatch.com/ provide portals to many\nof these projects. As we stated earlier, open-source projects enable students to\nuse source code as a learning tool. They can modify programs and test them,", "1.13 Summary 47\nhelp \ufb01nd and \ufb01x bugs, and otherwise explore mature, full-featured operating\nsystems, compilers, tools, user interfaces, and other types of programs. The\navailability of source code for historic projects, such as Multics, can help\nstudents to understand those projects and to build knowledge that will help in\nthe implementation of new projects.\nGNU/Linux and BSD UNIX are all open-source operating systems, but each\nhas its own goals, utility, licensing, and purpose. Sometimes, licenses are not\nmutually exclusive and cross-pollination occurs, allowing rapid improvements\nin operating-system projects. For example, several major components of\nOpenSolaris have been ported to BSD UNIX. The advantages of free software\nand open sourcing are likely to increase the number and quality of open-source\nprojects, leading to an increase in the number of individuals and companies\nthat use these projects.\n1.13 Summary\nAn operating system is software that manages the computer hardware, as well\nas providing an environment for application programs to run. Perhaps the\nmost visible aspect of an operating system is the interface to the computer\nsystem it provides to the human user.\nFor a computer to do its job of executing programs, the programs must be in\nmain memory. Main memory is the only large storage area that the processor\ncan access directly. It is an array of bytes, ranging in size from millions to\nbillions. Each byte in memory has its own address. The main memory is usually\na volatile storage device that loses its contents when power is turned off or\nlost. Most computer systems provide secondary storage as an extension of\nmain memory. Secondary storage provides a form of nonvolatile storage that\nis capable of holding large quantities of data permanently. The most common\nsecondary-storage device is a magnetic disk, which provides storage of both\nprograms and data.\nThe wide variety of storage systems in a computer system can be organized\nin a hierarchy according to speed and cost. The higher levels are expensive,\nbut they are fast. As we move down the hierarchy, the cost per bit generally\ndecreases, whereas the access time generally increases.\nThere are several different strategies for designing a computer system.\nSingle-processor systems have only one processor, while multiprocessor\nsystems contain two or more processors that share physical memory and\nperipheral devices. The most common multiprocessor design is symmetric\nmultiprocessing (or SMP), where all processors are considered peers and run\nindependently of one another. Clustered systems are a specialized form of\nmultiprocessor systems and consist of multiple computer systems connected\nby a local-area network.\nTo best utilize the CPU,m o d e r no p e r a t i n gs y s t e m se m p l o ym u l t i p r o g r a m -\nming, which allows several jobs to be in memory at the same time, thus ensuring\nthat the CPU always has a job to execute. Time-sharing systems are an exten-\nsion of multiprogramming whereinCPU scheduling algorithms rapidly switch\nbetween jobs, thus providing the illusion that each job is running concurrently.\nThe operating system must ensure correct operation of the computer\nsystem. To prevent user programs from interfering with the proper operation of48 Chapter 1 Introduction\nTHE STUDY OF OPERATING SYSTEMS\nThere has never been a more interesting time to study operating systems, and\nit has never been easier. The open-source movement has overtaken operating\nsystems, causing many of them to be made available in both source and binary\n(executable) format. The list of operating systems available in both formats\nincludes Linux, BSD UNIX ,S o l a r i s ,a n dp a r to fM a cOS X .T h ea v a i l a b i l i t y\nof source code allows us to study operating systems from the inside out.\nQuestions that we could once answer only by looking at documentation or\nthe behavior of an operating system we can now answer by examining the\ncode itself.\nOperating systems that are no longer commercially viable have been\nopen-sourced as well, enabling us to study how systems operated in a\ntime of fewer CPU,m e m o r y ,a n ds t o r a g er e s o u r c e s .A ne x t e n s i v eb u t\nincomplete list of open-source operating-system projects is available from\nhttp://dmoz.org/Computers/Software/Operating\n Systems/Open\n Source/.\nIn addition, the rise of virtualization as a mainstream (and frequently free)\ncomputer function makes it possible to run many operating systems on top of\none core system. For example,VMware (http://www.vmware.com)p r o v i d e s\naf r e e\u201cplayer\u201d for Windows on which hundreds of free \u201cvirtual appliances\u201d\ncan run. Virtualbox ( http://www.virtualbox.com)p r o v i d e saf r e e ,o p e n -\nsource virtual machine manager on many operating systems. Using such\ntools, students can try out hundreds of operating systems without dedicated\nhardware.\nIn some cases, simulators of speci\ufb01c hardware are also available, allowing\nthe operating system to run on \u201cnative\u201d hardware, all within the con\ufb01nes\nof a modern computer and modern operating system. For example, a\nDECSYSTEM-20 simulator running on Mac OS X can boot TOPS-20,l o a dt h e\nsource tapes, and modify and compile a new TOPS-20 kernel. An interested\nstudent can search the Internet to \ufb01nd the original papers that describe the\noperating system, as well as the original manuals.\nThe advent of open-source operating systems has also made it easier to\nmake the move from student to operating-system developer. With some\nknowledge, some effort, and an Internet connection, a student can even create\na new operating-system distribution. Just a few years ago, it was dif\ufb01cult or\nimpossible to get access to source code. Now, such access is limited only by\nhow much interest, time, and disk space a student has.\nthe system, the hardware has two modes: user mode and kernel mode. Various\ninstructions (such as I/O instructions and halt instructions) are privileged and\ncan be executed only in kernel mode. The memory in which the operating\nsystem resides must also be protected from modi\ufb01cation by the user. A timer\nprevents in\ufb01nite loops. These facilities (dual mode, privileged instructions,\nmemory protection, and timer interrupt) are basic building blocks used by\noperating systems to achieve correct operation.\nAp r o c e s s( o rj o b )i st h ef u n d a m e n t a lu n i to fw o r ki na no p e r a t i n gs y s t e m .\nProcess management includes creating and deleting processes and providing\nmechanisms for processes to communicate and synchronize with each other.Practice Exercises 49\nAn operating system manages memory by keeping track of what parts of\nmemory are being used and by whom. The operating system is also responsible\nfor dynamically allocating and freeing memory space. Storage space is also\nmanaged by the operating system; this includes providing \ufb01le systems for\nrepresenting \ufb01les and directoriesand managing space on mass-storage devices.\nOperating systems must also be concerned with protecting and securing the\noperating system and users. Protection measures control the access of processes\nor users to the resources made available by the computer system. Security\nmeasures are responsible for defending a computer system from external or\ninternal attacks.\nSeveral data structures that are fundamental to computer science are widely\nused in operating systems, including lists, stacks, queues, trees, hash functions,\nmaps, and bitmaps.\nComputing takes place in a variety of environments. Traditional computing\ninvolves desktop and laptop PCs, usually connected to a computer network.\nMobile computing refers to computing on handheld smartphones and tablet\ncomputers, which offer several unique features. Distributed systems allow\nusers to share resources on geographically dispersed hosts connected via\nac o m p u t e rn e t w o r k .S e r v i c e sm a yb eprovided through either the client\u2013\nserver model or the peer-to-peer model. Virtualization involves abstracting\nac o m p u t e r \u2019 sh a r d w a r ei n t os e v e r a ld i f f e r e n te x e c u t i o ne n v i r o n m e n t s .C l o u d\ncomputing uses a distributed system to abstract services into a\u201ccloud,\u201d where\nusers may access the services from remote locations. Real-time operating\nsystems are designed for embedded environments, such as consumer devices,\nautomobiles, and robotics.\nThe free software movement has created thousands of open-source projects,\nincluding operating systems. Because of these projects, students are able to use\nsource code as a learning tool. They can modify programs and test them,\nhelp \ufb01nd and \ufb01x bugs, and otherwise explore mature, full-featured operating\nsystems, compilers, tools, user interfaces, and other types of programs.\nGNU/Linux and BSD UNIX are open-source operating systems. The advan-\ntages of free software and open sourcing are likely to increase the number\nand quality of open-source projects, leading to an increase in the number of\nindividuals and companies that use these projects.\nPractice Exercises\n1.1 What are the three main purposes of an operating system?\n1.2 We have stressed the need for an operating system to make ef\ufb01cient use\nof the computing hardware. When is it appropriate for the operating\nsystem to forsake this principle and to \u201cwaste\u201d resources? Why is such\nas y s t e mn o tr e a l l yw a s t e f u l ?\n1.3 What is the main dif\ufb01culty that a programmer must overcome in writing\nan operating system for a real-time environment?\n1.4 Keeping in mind the various de\ufb01nitions of operating system, consider\nwhether the operating system should include applications such as web\nbrowsers and mail programs. Argue both that it should and that it should\nnot, and support your answers.", "50 Chapter 1 Introduction\n1.5 How does the distinction betweenkernel mode and user mode function\nas a rudimentary form of protection (security) system?\n1.6 Which of the following instructions should be privileged?\na. Set value of timer.\nb. Read the clock.\nc. Clear memory.\nd. Issue a trap instruction.\ne. Turn off interrupts.\nf. Modify entries in device-status table.\ng. Switch from user to kernel mode.\nh. Access I/O device.\n1.7 Some early computers protected the operating system by placing it in\nam e m o r yp a r t i t i o nt h a tc o u l dn o tb em o d i \ufb01 e db ye i t h e rt h eu s e rj o b\nor the operating system itself. Describe two dif\ufb01culties that you think\ncould arise with such a scheme.\n1.8 Some CPUsp r o v i d ef o rm o r et h a nt w om o d e so fo p e r a t i o n .W h a ta r e\ntwo possible uses of these multiple modes?\n1.9 Timers could be used to compute the current time. Provide a short\ndescription of how this could be accomplished.\n1.10 Give two reasons why caches are useful. What problems do they solve?\nWhat problems do they cause? If a cache can be made as large as the\ndevice for which it is caching (for instance, a cache as large as a disk),\nwhy not make it that large and eliminate the device?\n1.11 Distinguish between the client\u2013 server and peer-to-peer models of\ndistributed systems.\nExercises\n1.12 In a multiprogramming and time-sharing environment, several users\nshare the system simultaneously. This situation can result in various\nsecurity problems.\na. What are two such problems?\nb. Can we ensure the same degree of security in a time-shared\nmachine as in a dedicated machine? Explain your answer.\n1.13 The issue of resource utilization shows up in different forms in different\ntypes of operating systems. List what resources must be managed\ncarefully in the following settings:\na. Mainframe or minicomputer systems\nb. Workstations connected to servers\nc. Mobile computersExercises 51\n1.14 Under what circumstances would a user be better off using a time-\nsharing system than a PC or a single-user workstation?\n1.15 Describe the differences between symmetric and asymmetric multipro-\ncessing. What are three advantages and one disadvantage of multipro-\ncessor systems?\n1.16 How do clustered systems differ from multiprocessor systems? What is\nrequired for two machines belonging to a cluster to cooperate to provide\nah i g h l ya v a i l a b l es e r v i c e ?\n1.17 Consider a computing cluster consisting of two nodes running a\ndatabase. Describe two ways in which the cluster software can manage\naccess to the data on the disk. Discuss the bene\ufb01ts and disadvantages of\neach.\n1.18 How are network computers different from traditional personal com-\nputers? Describe some usage scenarios in which it is advantageous to\nuse network computers.\n1.19 What is the purpose of interrupts? How does an interrupt differ from a\ntrap? Can traps be generated intentionally by a user program? If so, for\nwhat purpose?\n1.20 Direct memory access is used for high-speed I/O devices in order to\navoid increasing the CPU\u2019s execution load.\na. How does the CPU interface with the device to coordinate the\ntransfer?\nb. How does the CPU know when the memory operations are com-\nplete?\nc. The CPU is allowed to execute other programs while the DMA\ncontroller is transferring data. Does this process interfere with\nthe execution of the user programs? If so, describe what forms\nof interference are caused.\n1.21 Some computer systems do not provide a privileged mode of operation\nin hardware. Is it possible to construct a secure operating system for\nthese computer systems? Give arguments both that it is and that it is not\npossible.\n1.22 Many SMP systems have different levels of caches; one level is local to\neach processing core, and another level is shared among all processing\ncores. Why are caching systems designed this way?\n1.23 Consider an SMP system similar to the one shown in Figure 1.6. Illustrate\nwith an example how data residing in memory could in fact have a\ndifferent value in each of the local caches.\n1.24 Discuss, with examples, how the problem of maintaining coherence of\ncached data manifests itself in the followingp r o c e s s i n ge n v i r o n m e n t s :\na. Single-processor systems\nb. Multiprocessor systems\nc. Distributed systems52 Chapter 1 Introduction\n1.25 Describe a mechanism for enforcing memory protection in order to\nprevent a program from modifying the memory associated with other\nprograms.\n1.26 Which network con\ufb01guration\u2014 LAN or WAN\u2014would best suit the\nfollowing environments?\na. A campus student union\nb. Several campus locations across a statewide university system\nc. A neighborhood\n1.27 Describe some of the challenges of designing operating systems for\nmobile devices compared with designing operating systems for tradi-\ntional PCs.\n1.28 What are some advantages of peer-to-peer systems over client-server\nsystems?\n1.29 Describe some distributed applications that would be appropriate for a\npeer-to-peer system.\n1.30 Identify several advantages and several disadvantages of open-source\noperating systems. Include the types of people who would \ufb01nd each\naspect to be an advantage or a disadvantage.\nBibliographical Notes\n[Brookshear (2012)] provides an overview of computer science in general.\nThorough coverage of data structures can be found in [Cormen et al. (2009)].\n[Russinovich and Solomon (2009)] give an overview of Microsoft Windows\nand covers considerable technical detail about the system internals and\ncomponents. [McDougall and Mauro (2007)] cover the internals of the Solaris\noperating system. Mac OS X internals are discussed in [Singh (2007)]. [Love\n(2010)] provides an overview of the Linux operating system and great detail\nabout data structures used in the Linux kernel.\nMany general textbooks cover operating systems, including [Stallings\n(2011)], [Deitel et al. (2004)], and [Tanenbaum (2007)]. [Kurose and Ross (2013)]\nprovides a general overview of computer networks, including a discussion\nof client-server and peer-to-peer systems. [Tarkoma and Lagerspetz (2011)]\nexamines several different mobile operating systems, including Android and\niOS.\n[Hennessy and Patterson (2012)] provide coverage ofI/O systems and buses\nand of system architecture in general. [Bryant and O\u2019Hallaron (2010)] provide\nat h o r o u g ho v e r v i e wo fac o m p u t e rs y s t e mf r o mt h ep e r s p e c t i v eo fac o m p u t e r\nprogrammer. Details of the Intel 64 instruction set and privilege modes can be\nfound in [Intel (2011)].\nThe history of open sourcing and its bene\ufb01ts and challenges appears in\n[Raymond (1999)]. The Free Software Foundation has published its philosophy\nin http://www.gnu.org/philosophy/free-software-for-freedom.html.T h eo p e n\nsource of Mac OS X are available from http://www.apple.com/opensource/.", "Bibliography 53\nWikipedia has an informative entry about the contributions of Richard\nStallman at http://en.wikipedia.org/wiki/Richard\n Stallman.\nThe source code of Multics is available athttp://web.mit.edu/multics-history\n/source/Multics\n Internet\n Server/Multics\n sources.html.\nBibliography\n[Brookshear (2012)] J. G. Brookshear, Computer Science: An Overview,Eleventh\nEdition, Addison-Wesley (2012).\n[Bryant and O\u2019Hallaron (2010)] R. Bryant and D. O\u2019Hallaron,Computer Systems:\nAP r o g r a m m e r sP e r s p e c t i v e ,Second Edition, Addison-Wesley (2010).\n[Cormen et al. (2009)] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein,\nIntroduction to Algorithms,Third Edition, MIT Press (2009).\n[Deitel et al. (2004)] H. Deitel, P . Deitel, and D. Choffnes, Operating Systems,\nThird Edition, Prentice Hall (2004).\n[Hennessy and Patterson (2012)] J. Hennessy and D. Patterson,Computer Archi-\ntecture: A Quantitative Approach,Fifth Edition, Morgan Kaufmann (2012).\n[Intel (2011)] Intel 64 and IA-32 Architectures Software Developer\u2019s Manual, Com-\nbined Volumes: 1, 2A, 2B, 3A and 3B.I n t e lC o r p o r a t i o n ( 2 0 1 1 ) .\n[Kurose and Ross (2013)] J. Kurose and K. Ross, Computer Networking\u2014A Top\u2013\nDown Approach,Sixth Edition, Addison-Wesley (2013).\n[Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developer\u2019s\nLibrary (2010).\n[McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[Raymond (1999)] E. S. Raymond, The Cathedral and the Bazaar,O \u2019 R e i l l y&\nAssociates (1999).\n[Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon,Win-\ndows Internals: Including Windows Server 2008 and Windows Vista,Fifth Edition,\nMicrosoft Press (2009).\n[Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach,A d d i s o n -\nWesley (2007).\n[Stallings (2011)] W. Stallings,Operating Systems,Seventh Edition, Prentice Hall\n(2011).\n[Tanenbaum (2007)] A. S. Tanenbaum,Modern Operating Systems,Third Edition,\nPrentice Hall (2007).\n[Tarkoma and Lagerspetz (2011)] S. Tarkoma and E. Lagerspetz,\u201cArching over\nthe Mobile Computing Chasm: Platforms and Runtimes \u201d, IEEE Computer,\nVolume 44, (2011), pages 22\u201328.2\nCHAPTEROperating -\nSystem\nStructures\nAn operating system provides the environment within which programs are\nexecuted. Internally, operating systems vary greatly in their makeup, since\nthey are organized along many different lines. The design of a new operating\nsystem is a major task. It is important that the goals of the system be well\nde\ufb01ned before the design begins. These goals form the basis for choices among\nvarious algorithms and strategies.\nWe can view an operating system from several vantage points. One view\nfocuses on the services that the system provides; another, on the interface that\nit makes available to users and programmers; a third, on its components and\ntheir interconnections. In this chapter, weexplore all three aspects of operating\nsystems, showing the viewpoints of users, programmers, and operating system\ndesigners. We consider what services an operating system provides, how they\nare provided, how they are debugged, and what the various methodologies\nare for designing such systems. Finally, we describe how operating systems\nare created and how a computer starts its operating system.\nCHAPTER OBJECTIVES\n\u2022 To describe the services an operating system provides to users, processes,\nand other systems.\n\u2022 To discuss the various ways of structuring an operating system.\n\u2022 To explain how operating systems are installed and customized and how\nthey boot.\n2.1 Operating-System Services\nAn operating system provides an environment for the execution of programs.\nIt provides certain services to programs and to the users of those programs.\nThe speci\ufb01c services provided, of course, differ from one operating system to\nanother, but we can identify common classes. These operating system services\nare provided for the convenience of the programmer, to make the programming\n55", "56 Chapter 2 Operating-System Structures\nuser and other system programs\nservices\noperating system\nhardware\nsystem calls\nGUI batch\nuser interfaces\ncommand line\nprogram\nexecution\nI/O\noperations\nfile\nsystems communication resource\nallocation accounting\nprotection\nand\nsecurity\nerror\ndetection\nFigure 2.1 Av i e wo fo p e r a t i n gs y s t e ms e r v i c e s .\ntask easier. Figure 2.1 shows one view of the various operating-system services\nand how they interrelate.\nOne set of operating system services provides functions that are helpful to\nthe user.\n\u2022 User interface.A l m o s ta l lo p e r a t i n gs y s t e m sh a v eauser interface (UI).\nThis interface can take several forms. One is a command-line interface\n(CLI),w h i c hu s e st e x tc o m m a n d sa n dam e t h o df o re n t e r i n gt h e m( s a y ,\nak e y b o a r df o rt y p i n gi nc o m m a n d si nas p e c i \ufb01 cf o r m a tw i t hs p e c i \ufb01 c\noptions). Another is a batch interface,i nw h i c hc o m m a n d sa n dd i r e c t i v e s\nto control those commands are entered into \ufb01les, and those \ufb01les are\nexecuted. Most commonly, a graphical user interface (GUI) is used. Here,\nthe interface is a window system with a pointing device to direct I/O,\nchoose from menus, and make selections and a keyboard to enter text.\nSome systems provide two or all three of these variations.\n\u2022 Program execution .T h es y s t e mm u s tb ea b l et ol o a dap r o g r a mi n t o\nmemory and to run that program. The program must be able to end its\nexecution, either normally or abnormally (indicating error).\n\u2022 I/O operations.Ar u n n i n gp r o g r a mm a yr e q u i r eI/O,w h i c hm a yi n v o l v ea\n\ufb01le or an I/O device. For speci\ufb01c devices, special functions may be desired\n(such as recording to a CD or DVD drive or blanking a display screen). For\nef\ufb01ciency and protection, users usually cannot controlI/O devices directly.\nTherefore, the operating system must provide a means to do I/O.\n\u2022 File-system manipulation.T h e\ufb01 l es y s t e mi so fp a r t i c u l a ri n t e r e s t .O b v i -\nously, programs need to read and write \ufb01les and directories. They also\nneed to create and delete them by name, search for a given \ufb01le, and\nlist \ufb01le information. Finally, some operating systems include permissions\nmanagement to allow or deny access to \ufb01les or directories based on \ufb01le\nownership. Many operating systems provide a variety of \ufb01le systems,\nsometimes to allow personal choice and sometimes to provide speci\ufb01c\nfeatures or performance characteristics.2.1 Operating-System Services 57\n\u2022 Communications.T h e r ea r em a n yc i r c u m s t a n c e si nw h i c ho n ep r o c e s s\nneeds to exchange information with another process. Such communication\nmay occur between processes that are executing on the same computer or\nbetween processes that are executing on different computer systems tied\ntogether by a computer network. Communications may be implemented\nvia shared memory,i nw h i c ht w oo rm o r ep r o c e s s e sr e a da n dw r i t et o\nas h a r e ds e c t i o no fm e m o r y ,o rmessage passing ,i nw h i c hp a c k e t so f\ninformation in prede\ufb01ned formats are moved between processes by the\noperating system.\n\u2022 Error detection.T h eo p e r a t i n gs y s t e mn e e d st ob ed e t e c t i n ga n dc o r r e c t i n g\nerrors constantly. Errors may occur in theCPU and memory hardware (such\nas a memory error or a power failure), inI/O devices (such as a parity error\non disk, a connection failure on a network, or lack of paper in the printer),\nand in the user program (such as an arithmetic over\ufb02ow, an attempt to\naccess an illegal memory location, or a too-great use of CPU time). For\neach type of error, the operating system should take the appropriate action\nto ensure correct and consistent computing. Sometimes, it has no choice\nbut to halt the system. At other times, it might terminate an error-causing\nprocess or return an error code to a process for the process to detect and\npossibly correct.\nAnother set of operating system functions exists not for helping the user\nbut rather for ensuring the ef\ufb01cient operation of the system itself. Systems with\nmultiple users can gain ef\ufb01ciency by sharing the computer resources among\nthe users.\n\u2022 Resource allocation .W h e nt h e r ea r em u l t i p l eu s e r so rm u l t i p l ej o b s\nrunning at the same time, resources must be allocated to each of them. The\noperating system manages many different types of resources. Some (such\nas CPU cycles, main memory, and \ufb01le storage) may have special allocation\ncode, whereas others (such as I/O devices) may have much more general\nrequest and release code. For instance, in determining how best to use\nthe CPU,o p e r a t i n gs y s t e m sh a v eCPU-scheduling routines that take into\naccount the speed of theCPU,t h ej o b st h a tm u s tb ee x e c u t e d ,t h en u m b e ro f\nregisters available, and other factors. There may also be routines to allocate\nprinters, USB storage drives, and other peripheral devices.\n\u2022 Accounting.W ew a n tt ok e e pt r a c ko fw h i c hu s e r su s eh o wm u c ha n d\nwhat kinds of computer resources. This record keeping may be used for\naccounting (so that users can be billed) or simply for accumulating usage\nstatistics. Usage statistics may be a valuable tool for researchers who wish\nto recon\ufb01gure the system to improve computing services.\n\u2022 Protection and security. The owners of information stored in a multiuser or\nnetworked computer system may want to control use of that information.\nWhen several separate processes execute concurrently, it should not be\npossible for one process to interfere with the others or with the operating\nsystem itself. Protection involves ensuring that all access to system\nresources is controlled. Security of the system from outsiders is also\nimportant. Such security starts with requiring each user to authenticate58 Chapter 2 Operating-System Structures\nhimself or herself to the system, usually by means of a password, to gain\naccess to system resources. It extends to defending external I/O devices,\nincluding network adapters, from invalid access attempts and to recording\nall such connections for detection of break-ins. If a system is to be protected\nand secure, precautions must be instituted throughout it. A chain is only\nas strong as its weakest link.\n2.2 User and Operating-System Interface\nWe mentioned earlier that there are several ways for users to interface with\nthe operating system. Here, we discuss two fundamental approaches. One\nprovides a command-line interface, orcommand interpreter,t h a ta l l o w su s e r s\nto directly enter commands to be performed by the operating system. The\nother allows users to interface with the operating system via a graphical user\ninterface, or GUI.\n2.2.1 Command Interpreters\nSome operating systems include the command interpreter in the kernel. Others,\nsuch as Windows andUNIX, treat the command interpreter as a special program\nthat is running when a job is initiated or when a user \ufb01rst logs on (on interactive\nsystems). On systems with multiple command interpreters to choose from, the\ninterpreters are known as shells.F o re x a m p l e ,o nUNIX and Linux systems, a\nuser may choose among several different shells, including the Bourne shell, C\nshell, Bourne-Again shell, Korn shell,a n do t h e r s .T h i r d - p a r t ys h e l l sa n df r e e\nuser-written shells are also available. Most shells provide similar functionality,\nand a user\u2019s choice of which shell to use is generally based on personal\npreference. Figure 2.2 shows the Bourne shell command interpreter being used\non Solaris 10.\nThe main function of the command interpreter is to get and execute the next\nuser-speci\ufb01ed command. Many of thecommands given at this level manipulate\n\ufb01les: create, delete, list, print, copy, execute, and so on. The MS-DOS and UNIX\nshells operate in this way. These commands can be implemented in two general\nways.\nIn one approach, the command interpreter itself contains the code to\nexecute the command. For example, a command to delete a \ufb01le may cause\nthe command interpreter to jump to a section of its code that sets up the\nparameters and makes the appropriate system call. In this case, the number of\ncommands that can be given determines the size of the command interpreter,\nsince each command requires its own implementing code.\nAn alternative approach\u2014used by UNIX, among other operating systems\n\u2014implements most commands through system programs. In this case, the\ncommand interpreter does not understand the command in any way; it merely\nuses the command to identify a \ufb01le to be loaded into memory and executed.\nThus, the UNIX command to delete a \ufb01le\nrm file.txt\nwould search for a \ufb01le called rm,l o a dt h e\ufb01 l ei n t om e m o r y ,a n de x e c u t ei tw i t h\nthe parameter file.txt.T h ef u n c t i o na s s o c i a t e dw i t ht h erm command would", "2.2 User and Operating-System Interface 59\nFigure 2.2 The Bourne shell command interpreter in Solrais 10.\nbe de\ufb01ned completely by the code in the \ufb01le rm.I nt h i sw a y ,p r o g r a m m e r sc a n\nadd new commands to the system easily by creating new \ufb01les with the proper\nnames. The command-interpreter program, which can be small, does not have\nto be changed for new commands to be added.\n2.2.2 Graphical User Interfaces\nAs e c o n ds t r a t e g yf o ri n t e r f a c i n gw i t ht h eo p e r a t i n gs y s t e mi st h r o u g hau s e r -\nfriendly graphical user interface, orGUI.H e r e ,r a t h e rt h a ne n t e r i n gc o m m a n d s\ndirectly via a command-line interface, users employ a mouse-based window-\nand-menu system characterized by a desktop metaphor. The user moves the\nmouse to position its pointer on images, or icons,o nt h es c r e e n( t h ed e s k t o p )\nthat represent programs, \ufb01les, directories, and system functions. Depending\non the mouse pointer\u2019s location, clicking a button on the mouse can invoke a\nprogram, select a \ufb01le or directory\u2014known as a folder\u2014or pull down a menu\nthat contains commands.\nGraphical user interfaces \ufb01rst appeared due in part to research taking place\nin the early 1970s at Xerox PARC research facility. The \ufb01rst GUI appeared on\nthe Xerox Alto computer in 1973. However, graphical interfaces became more\nwidespread with the advent of Apple Macintosh computers in the 1980s. The\nuser interface for the Macintosh operating system (Mac OS)h a su n d e r g o n e\nvarious changes over the years, the most signi\ufb01cant being the adoption of\nthe Aqua interface that appeared with Mac OS X.M i c r o s o f t \u2019 s\ufb01 r s tv e r s i o no f\nWindows\u2014Version 1.0\u2014was based on the addition of a GUI interface to the\nMS-DOS operating system. Later versions of Windows have made cosmetic60 Chapter 2 Operating-System Structures\nchanges in the appearance of the GUI along with several enhancements in its\nfunctionality.\nBecause a mouse is impractical for most mobile systems, smartphones and\nhandheld tablet computers typically use a touchscreen interface. Here, users\ninteract by making gestures on the touchscreen\u2014for example, pressing and\nswiping \ufb01ngers across the screen. Figure 2.3 illustrates the touchscreen of the\nApple iPad. Whereas earlier smartphones included a physical keyboard, most\nsmartphones now simulate a keyboard on the touchscreen.\nTraditionally,UNIX systems have been dominated by command-line inter-\nfaces. VariousGUI interfaces are available, however.T h e s ei n c l u d et h eC o m m o n\nDesktop Environment ( CDE)a n dX - W i n d o w ss y s t e m s ,w h i c ha r ec o m m o n\non commercial versions of UNIX,s u c ha sS o l a r i sa n dI B M \u2019 sAIX system. In\naddition, there has been signi\ufb01cant development in GUI designs from various\nopen-source projects, such asKD e s k t o pE n v i r o n m e n t(or KDE)a n dt h eGNOME\ndesktop by the GNU project. Both the KDE and GNOME desktops run on Linux\nand various UNIX systems and are available under open-source licenses, which\nmeans their source code is readily available for reading and for modi\ufb01cation\nunder speci\ufb01c license terms.\nFigure 2.3 The iPad touchscreen.2.2 User and Operating-System Interface 61\n2.2.3 Choice of Interface\nThe choice of whether to use a command-line or GUI interface is mostly\none of personal preference. System administrators who manage computers\nand power users who have deep knowledge of a system frequently use the\ncommand-line interface. For them, it is more ef\ufb01cient, giving them faster\naccess to the activities they need to perform. Indeed, on some systems, only a\nsubset of system functions is available via the GUI, leaving the less common\ntasks to those who are command-line knowledgeable. Further, command-\nline interfaces usually make repetitive tasks easier, in part because they have\ntheir own programmability. For example, if a frequent task requires a set of\ncommand-line steps, those steps can be recorded into a \ufb01le, and that \ufb01le can\nbe run just like a program. The program is not compiled into executable code\nbut rather is interpreted bythe command-line interface. Theseshell scripts are\nvery common on systems that are command-line oriented, such as UNIX and\nLinux.\nIn contrast, most Windows users are happy to use the Windows GUI\nenvironment and almost never use the MS-DOS shell interface. The various\nchanges undergone by the Macintosh operating systems provide a nice study\nin contrast. Historically, Mac OS has not provided a command-line interface,\nalways requiring its users to interface with the operating system using itsGUI.\nHowever, with the release of Mac OS X (which is in part implemented using a\nUNIX kernel), the operating system now provides both a Aqua interface and a\ncommand-line interface. Figure 2.4 is a screenshot of the Mac OS X GUI.\nFigure 2.4 The Mac OS X GUI.", "62 Chapter 2 Operating-System Structures\nThe user interface can vary from system to system and even from user\nto user within a system. It typically is substantially removed from the actual\nsystem structure. The design of a useful and friendly user interface is therefore\nnot a direct function of the operating system. In this book, we concentrate on\nthe fundamental problems of providing adequate service to user programs.\nFrom the point of view of the operating system, we do not distinguish between\nuser programs and system programs.\n2.3 System Calls\nSystem callsprovide an interface to the services made available by an operating\nsystem. These calls are generally available as routines written in C and\nC++, although certain low-level tasks (for example, tasks where hardware\nmust be accessed directly) may have to be written using assembly-language\ninstructions.\nBefore we discuss how an operating system makes system calls available,\nlet\u2019s \ufb01rst use an example to illustrate how system calls are used: writing a\nsimple program to read data from one \ufb01le and copy them to another \ufb01le. The\n\ufb01rst input that the program will need is the names of the two \ufb01les: the input \ufb01le\nand the output \ufb01le. These names can bespeci\ufb01ed in many ways, depending on\nthe operating-system design. One approach is for the program to ask the user\nfor the names. In an interactive system, this approach will require a sequence of\nsystem calls, \ufb01rst to write a prompting message on the screen and then to read\nfrom the keyboard the characters that de\ufb01ne the two \ufb01les. On mouse-based and\nicon-based systems, a menu of \ufb01le names is usually displayed in a window.\nThe user can then use the mouse to select the source name, and a window\ncan be opened for the destination name to be speci\ufb01ed. This sequence requires\nmany I/O system calls.\nOnce the two \ufb01le names have been ob tained, the program must open the\ninput \ufb01le and create the output \ufb01le. Each of these operations requires another\nsystem call. Possible error conditions for each operation can require additional\nsystem calls. When the program tries to open the input \ufb01le, for example, it may\n\ufb01nd that there is no \ufb01le of that name or that the \ufb01le is protected against access.\nIn these cases, the program should print a message on the console (another\nsequence of system calls) and then terminate abnormally (another system call).\nIf the input \ufb01le exists, then we must create a new output \ufb01le. We may \ufb01nd that\nthere is already an output \ufb01le with the same name. This situation may cause\nthe program to abort (a system call), or we may delete the existing \ufb01le (another\nsystem call) and create a new one (yet another system call). Another option,\nin an interactive system, is to ask the user (via a sequence of system calls to\noutput the prompting message and to read the response from the terminal)\nwhether to replace the existing \ufb01le or to abort the program.\nWhen both \ufb01les are set up, we enter a loop that reads from the input \ufb01le\n(a system call) and writes to the output \ufb01le (another system call). Each read\nand write must return status information regarding various possible error\nconditions. On input, the program may \ufb01nd that the end of the \ufb01le has been\nreached or that there was a hardware failure in the read (such as a parity error).\nThe write operation may encounter various errors, depending on the output\ndevice (for example, no more disk space).2.3 System Calls 63\nFinally, after the entire \ufb01le is copied, the program may close both \ufb01les\n(another system call), write a message to the console or window (more system\ncalls), and \ufb01nally terminate normally (the \ufb01nal system call). This system-call\nsequence is shown in Figure 2.5.\nAs you can see, even simple programs may make heavy use of the\noperating system. Frequently, systems execute thousands of system calls\nper second. Most programmers never see this level of detail, however.\nTypically, application developers design programs according to anapplication\nprogramming interface (API).T h e API speci\ufb01es a set of functions that are\navailable to an application programmer, including the parameters that are\npassed to each function and the return values the programmer can expect.\nThree of the most common APIsa v a i l a b l et oa p p l i c a t i o np r o g r a m m e r sa r e\nthe Windows API for Windows systems, thePOSIX API for POSIX-based systems\n(which include virtually all versions ofUNIX, Linux, and MacOS X), and the Java\nAPI for programs that run on the Java virtual machine. A programmer accesses\nan API via a library of code provided by the operating system. In the case of\nUNIX and Linux for programs written in the C language, the library is called\nlibc. Note that\u2014unless speci\ufb01ed\u2014the system-call names used throughout\nthis text are generic examples. Each operating system has its own name for\neach system call.\nBehind the scenes, the functions that make up an API typically invoke the\nactual system calls on behalf of the application programmer. For example, the\nWindows function CreateProcess() (which unsurprisingly is used to create\nan e wp r o c e s s )a c t u a l l yi n v o k e st h eNTCreateProcess() system call in the\nWindows kernel.\nWhy would an application programmer prefer programming according to\nan API rather than invoking actual system calls? There are several reasons for\ndoing so. One bene\ufb01t concerns program portability. An application program-\nsource file destination  file\nExample System Call Sequence\nAcquire input file name \n  Write prompt to screen \n  Accept input\nAcquire output file name\n  Write prompt to screen \n  Accept input \nOpen the input file \n  if file doesn't exist, abort \nCreate output file \n  if file exists, abort \nLoop \n  Read from input file \n  Write to output file \nUntil read fails \nClose output file\nWrite completion message to screen \nTerminate normally\nFigure 2.5 Example of how system calls are used.64 Chapter 2 Operating-System Structures\nEXAMPLE OF STANDARD API\nAs an example of a standard API,c o n s i d e rt h eread() function that is\navailable in UNIX and Linux systems. The API for this function is obtained\nfrom the man page by invoking the command\nman read\non the command line. A description of this API appears below:\n#include <unistd.h>\nssize_t read(int fd, void *buf, size_t count)\nreturn\nvalue\nfunction\nname\nparameters\nAp r o g r a mt h a tu s e st h eread() function must include theunistd.h header\n\ufb01le, as this \ufb01le de\ufb01nes the ssize\n t and size\n t data types (among other\nthings). The parameters passed to read() are as follows:\n\u2022 int fd\u2014the \ufb01le descriptor to be read\n\u2022 void *buf\u2014a buffer where the data will be read into\n\u2022 size\n tc o u n t\u2014the maximum number of bytes to be read into the\nbuffer\nOn a successful read, the number of bytes read is returned. A return value of\n0i n d i c a t e se n do f\ufb01 l e .I fa ne r r o ro c c u r s ,read() returns \u22121.\nmer designing a program using anAPI can expect her program to compile and\nrun on any system that supports the sameAPI (although, in reality, architectural\ndifferences often make this more dif\ufb01cult than it may appear). Furthermore,\nactual system calls can often be more detailed and dif\ufb01cult to work with than\nthe API available to an application programmer. Nevertheless, there often exists\nas t r o n gc o r r e l a t i o nb e t w e e naf u n c t i o ni nt h eAPI and its associated system call\nwithin the kernel. In fact, many of the POSIX and Windows APIsa r es i m i l a rt o\nthe native system calls provided by the UNIX,L i n u x ,a n dW i n d o w so p e r a t i n g\nsystems.\nFor most programming languages, the run-time support system (a set of\nfunctions built into libraries included with a compiler) provides a system-\ncall interface that serves as the link to system calls made available by the\noperating system. The system-call interface intercepts function calls in the API\nand invokes the necessary system calls within the operating system. Typically,\na number is associated with each system call, and the system-call interface\nmaintains a table indexed according to these numbers. The system call interface", "2.3 System Calls 65\nImplementation\nof open ( )\nsystem call\nopen ( )\nuser \nmode\nreturn\nuser application\nsystem call interface\nkernel\nmode\ni\nopen ( )\nFigure 2.6 The handling of a user application invoking theopen() system call.\nthen invokes the intended system call in the operating-system kernel and\nreturns the status of the system call and any return values.\nThe caller need know nothing about how the system call is implemented\nor what it does during execution. Rather,the caller need only obey theAPI and\nunderstand what the operating system will do as a result of the execution of\nthat system call. Thus, most of the details of the operating-system interface\nare hidden from the programmer by the API and are managed by the run-time\nsupport library. The relationship between an API, the system-call interface,\nand the operating system is shown in Figure 2.6, which illustrates how the\noperating system handles a user application invoking the open() system call.\nSystem calls occur in different ways, depending on the computer in use.\nOften, more information is required than simply the identity of the desired\nsystem call. The exact type and amount of information vary according to the\nparticular operating system and call. For example, to get input, we may need\nto specify the \ufb01le or device to use as the source, as well as the address and\nlength of the memory buffer into which the input should be read. Of course,\nthe device or \ufb01le and length may be implicit in the call.\nThree general methods are used to pass parameters to the operating system.\nThe simplest approach is to pass the parameters in registers. In some cases,\nhowever, there may be more parameters than registers. In these cases, the\nparameters are generally stored in a block, or table, in memory, and the\naddress of the block is passed as a parameter in a register (Figure 2.7). This\nis the approach taken by Linux and Solaris. Parameters also can be placed,\nor pushed,o n t ot h estack by the program and popped off the stack by the\noperating system. Some operating systems prefer the block or stack method\nbecause those approaches do not limit the number or length of parameters\nbeing passed.66 Chapter 2 Operating-System Structures\ncode for \nsystem \ncall 13\noperating system\nuser program\nuse parameters\nfrom table X\nregister\nX\nX: parameters\nfor call\nload address X\nsystem call 13\nFigure 2.7 Passing of parameters as a table.\n2.4 Types of System Calls\nSystem calls can be grouped roughly into six major categories: process\ncontrol, \ufb01le manipulation, device manipulation, information maintenance,\ncommunications,a n d protection.I nS e c t i o n s2 . 4 . 1t h r o u g h2 . 4 . 6 ,w eb r i e \ufb02 y\ndiscuss the types of system calls that may be provided by an operating system.\nMost of these system calls support, or are supported by, concepts and functions\nthat are discussed in later chapters. Figure 2.8 summarizes the types of system\ncalls normally provided by an operating system. As mentioned, in this text,\nwe normally refer to the system calls by generic names. Throughout the text,\nhowever, we provide examples of the a ctual counterparts to the system calls\nfor Windows, UNIX,a n dL i n u xs y s t e m s .\n2.4.1 Process Control\nAr u n n i n gp r o g r a mn e e d st ob ea b l et oh a l ti t se x e c u t i o ne i t h e rn o r m a l l y\n(end())o ra b n o r m a l l y(abort()). If a system call is made to terminate the\ncurrently running program abnormally, or if the program runs into a problem\nand causes an error trap, a dump of memory is sometimes taken and an error\nmessage generated. The dump is written to disk and may be examined by\na debugger\u2014a system program designed to aid the programmer in \ufb01nding\nand correcting errors, or bugs\u2014to determine the cause of the problem. Under\neither normal or abnormal circumstances, the operating system must transfer\ncontrol to the invoking command interpreter. The command interpreter then\nreads the next command. In an interactive system, the command interpreter\nsimply continues with the next command; it is assumed that the user will\nissue an appropriate command to respond to any error. In a GUI system, a\npop-up window might alert the user to the error and ask for guidance. In a\nbatch system, the command interpreter usually terminates the entire job and\ncontinues with the next job. Some systems may allow for special recovery\nactions in case an error occurs. If the program discovers an error in its input\nand wants to terminate abnormally, it may also want to de\ufb01ne an error level.\nMore severe errors can be indicated by a higher-level error parameter. It is then2.4 Types of System Calls 67\n\u2022 Process control\n\u25e6 end, abort\n\u25e6 load, execute\n\u25e6 create process, terminate process\n\u25e6 get process attributes, set process attributes\n\u25e6 wait for time\n\u25e6 wait event, signal event\n\u25e6 allocate and free memory\n\u2022 File management\n\u25e6 create \ufb01le, delete \ufb01le\n\u25e6 open, close\n\u25e6 read, write, reposition\n\u25e6 get \ufb01le attributes, set \ufb01le attributes\n\u2022 Device management\n\u25e6 request device, release device\n\u25e6 read, write, reposition\n\u25e6 get device attributes, set device attributes\n\u25e6 logically attach or detach devices\n\u2022 Information maintenance\n\u25e6 get time or date, set time or date\n\u25e6 get system data, set system data\n\u25e6 get process, \ufb01le, or device attributes\n\u25e6 set process, \ufb01le, or device attributes\n\u2022 Communications\n\u25e6 create, delete communication connection\n\u25e6 send, receive messages\n\u25e6 transfer status information\n\u25e6 attach or detach remote devices\nFigure 2.8 Types of system calls.\npossible to combine normal and abnormal termination by de\ufb01ning a normal\ntermination as an error at level 0. The command interpreter or a following\nprogram can use this error level to determine the next action automatically.\nAp r o c e s so rj o be x e c u t i n go n ep r o g r a mm a yw a n tt oload() and\nexecute() another program. This feature allows the command interpreter to\nexecute a program as directed by, for example, a user command, the click of a", "68 Chapter 2 Operating-System Structures\nEXAMPLES OF WINDOWS AND UNIX SYSTEM CALLS\nWindows Unix\nProcess CreateProcess() fork()\nControl ExitProcess() exit()\nWaitForSingleObject() wait()\nFile CreateFile() open()\nManipulation ReadFile() read()\nWriteFile() write()\nCloseHandle() close()\nDevice SetConsoleMode() ioctl()\nManipulation ReadConsole() read()\nWriteConsole() write()\nInformation GetCurrentProcessID() getpid()\nMaintenance SetTimer() alarm()\nSleep() sleep()\nCommunication CreatePipe() pipe()\nCreateFileMapping() shm\n open()\nMapViewOfFile() mmap()\nProtection SetFileSecurity() chmod()\nInitlializeSecurityDescriptor() umask()\nSetSecurityDescriptorGroup() chown()\nmouse, or a batch command. An interesting question is where to return control\nwhen the loaded program terminates. This question is related to whether the\nexisting program is lost, saved, or allowed to continue execution concurrently\nwith the new program.\nIf control returns to the existing program when the new program termi-\nnates, we must save the memory image of the existing program; thus, we have\neffectively created a mechanism for one program to call another program. If\nboth programs continue concurrently, we have created a new job or process to\nbe multiprogrammed. Often, there is a system call speci\ufb01cally for this purpose\n(create\n process() or submit\n job()).\nIf we create a new job or process, or perhaps even a set of jobs or\nprocesses, we should be able to control its execution. This control requires\nthe ability to determine and reset the attributes of a job or process, includ-\ning the job\u2019s priority, its maximum allowable execution time, and so on\n(get\n process\n attributes() and set\n process\n attributes()). We may also\nwant to terminate a job or process that we created ( terminate\n process())i f\nwe \ufb01nd that it is incorrect or is no longer needed.2.4 Types of System Calls 69\nEXAMPLE OF STANDARD C LIBRARY\nThe standard C library provides a portion of the system-call interface for\nmany versions of UNIX and Linux. As an example, let\u2019s assume a C program\ninvokes the printf() statement. The C library intercepts this call and\ninvokes the necessary system call (or calls) in the operating system\u2014in this\ninstance, the write() system call. The C library takes the value returned by\nwrite() and passes it back to the user program. This is shown below:\nwrite ( )\nsystem call\nuser\nmode\nkernel\nmode\n#include <stdio.h>\nint main ( )\n{\n   \u2022   \u2022   \u2022\n   printf (\"Greetings\");\n   \u2022   \u2022   \u2022\n   return 0;\n}\nstandard C library\nwrite ( )\nHaving created new jobs or processes, we may need to wait for them to\n\ufb01nish their execution. We may want to wait for a certain amount of time to\npass (wait\n time()). More probably, we will want to wait for a speci\ufb01c event\nto occur (wait\n event()). The jobs or processes should then signal when that\nevent has occurred (signal\n event()).\nQuite often, two or more processes may share data. To ensure the integrity\nof the data being shared, operating systems often provide system calls allowing\nap r o c e s st olock shared data. Then, no other process can access the data until\nthe lock is released. Typically, such system calls include acquire\n lock() and\nrelease\n lock().S y s t e mc a l l so ft h e s et y p e s ,d e a l i n gw i t ht h ec o o r d i n a t i o no f\nconcurrent processes, are discussed in great detail in Chapter 5.\nThere are so many facets of and variations in process and job control that\nwe next use two examples\u2014one involving a single-tasking system and the\nother a multitasking system\u2014to clarify these concepts. The MS-DOS operating\nsystem is an example of a single-tasking system. It has a command interpreter\nthat is invoked when the computer is started (Figure 2.9(a)). Because MS-DOS\nis single-tasking, it uses a simple method to run a program and does not create\nan e wp r o c e s s .I tl o a d st h ep r o g r a mi n t om e m o r y ,w r i t i n go v e rm o s to fi t s e l ft o70 Chapter 2 Operating-System Structures\n(a) (b)\nfree memory\ncommand \ninterpreter\nkernel\nprocess\nfree memory\ncommand \ninterpreter\nkernel\nFigure 2.9 MS-DOS execution. (a) At system startup. (b) Running a program.\ngive the program as much memory as possible (Figure 2.9(b)). Next, it sets the\ninstruction pointer to the \ufb01rst instruction of the program. The program then\nruns, and either an error causes a trap, or the program executes a system call\nto terminate. In either case, the error code is saved in the system memory for\nlater use. Following this action, the small portion of the command interpreter\nthat was not overwritten resumes execution. Its \ufb01rst task is to reload the rest\nof the command interpreter from disk. Then the command interpreter makes\nthe previous error code available to the user or to the next program.\nFreeBSD (derived from Berkeley UNIX)i sa ne x a m p l eo fam u l t i t a s k i n g\nsystem. When a user logs on to the system, the shell of the user\u2019s choice\nis run. This shell is similar to the MS-DOS shell in that it accepts commands\nand executes programs that the user requests. However, since FreeBSD is a\nmultitasking system, the command interpreter may continue running while\nanother program is executed (Figure 2.10). To start a new process, the shell\nfree memory\ninterpreter\nkernel\nprocess D\nprocess C\nprocess B\nFigure 2.10 FreeBSD running multiple programs.", "2.4 Types of System Calls 71\nexecutes a fork() system call. Then, the selected program is loaded into\nmemory via an exec() system call, and the program is executed. Depending\non the way the command was issued, the shell then either waits for the process\nto \ufb01nish or runs the process \u201cin the background.\u201d In the latter case, the shell\nimmediately requests another command. When a process is running in the\nbackground, it cannot receive input dire ctly from the keyboard, because the\nshell is using this resource.I/O is therefore done through \ufb01les or through aGUI\ninterface. Meanwhile, the user is free to ask the shell to run other programs, to\nmonitor the progress of the running process, to change that program\u2019s priority,\nand so on. When the process is done, it executes an exit() system call to\nterminate, returning to the invoking process a status code of 0 or a nonzero\nerror code. This status or error code is th en available to the shell or other\nprograms. Processes are discussed in Chapter 3 with a program example using\nthe fork() and exec() system calls.\n2.4.2 File Management\nThe \ufb01le system is discussed in more detail in Chapters 11 and 12. We can,\nhowever, identify several common system calls dealing with \ufb01les.\nWe \ufb01rst need to be able tocreate() and delete() \ufb01les. Either system call\nrequires the name of the \ufb01le and perhaps some of the \ufb01le\u2019s attributes. Once\nthe \ufb01le is created, we need to open() it and to use it. We may also read(),\nwrite(),o r reposition() (rewind or skip to the end of the \ufb01le, for example).\nFinally, we need to close() the \ufb01le, indicating that we are no longer using it.\nWe may need these same sets of operations for directories if we have a\ndirectory structure for organizing \ufb01les in the \ufb01le system. In addition, for either\n\ufb01les or directories, we need to be able to determine the values of various\nattributes and perhaps to reset them ifnecessary. File attributes include the \ufb01le\nname, \ufb01le type, protection codes, accounting information, and so on. At least\ntwo system calls, get\n file\n attributes() and set\n file\n attributes(),a r e\nrequired for this function. Some operating systems provide many more calls,\nsuch as calls for \ufb01le move() and copy().O t h e r sm i g h tp r o v i d ea nAPI that\nperforms those operations using code and other system calls, and others might\nprovide system programs to perform those tasks. If the system programs are\ncallable by other programs, then each can be considered anAPI by other system\nprograms.\n2.4.3 Device Management\nAp r o c e s sm a yn e e ds e v e r a lr e s o u r c e st oe x e c u t e \u2014 m a i nm e m o r y ,d i s kd r i v e s ,\naccess to \ufb01les, and so on. If the resour ces are available, they can be granted,\nand control can be returned to the user process. Otherwise, the process will\nhave to wait until suf\ufb01cient resources are available.\nThe various resources controlled by the operating system can be thought\nof as devices. Some of these devices are physical devices (for example, disk\ndrives), while others can be thought of as abstract or virtual devices (for\nexample, \ufb01les). A system with multiple users may require us to \ufb01rstrequest()\nad e v i c e ,t oe n s u r ee x c l u s i v eu s eo fi t .A f t e rwe are \ufb01nished with the device, we\nrelease() it. These functions are similar to the open() and close() system\ncalls for \ufb01les. Other operating systems allow unmanaged access to devices.72 Chapter 2 Operating-System Structures\nThe hazard then is the potential for device contention and perhaps deadlock,\nwhich are described in Chapter 7.\nOnce the device has been requested (and allocated to us), we can read(),\nwrite(),a n d( p o s s i b l y )reposition() the device, just as we can with \ufb01les. In\nfact, the similarity betweenI/O devices and \ufb01les is so great that many operating\nsystems, including UNIX,m e r g et h et w oi n t oac o m b i n e d\ufb01 l e\u2013d e v i c es t r u c t u r e .\nIn this case, a set of system calls is used on both \ufb01les and devices. Sometimes,\nI/O devices are identi\ufb01ed by special \ufb01l en a m e s ,d i r e c t o r yp l a c e m e n t ,o r\ufb01 l e\nattributes.\nThe user interface can also make \ufb01les and devices appear to be similar, even\nthough the underlying system calls are dissimilar. This is another example of\nthe many design decisions that go into building an operating system and user\ninterface.\n2.4.4 Information Maintenance\nMany system calls exist simply for the purpose of transferring information\nbetween the user program and the operating system. For example, most\nsystems have a system call to return the current time() and date().O t h e r\nsystem calls may return information about the system, such as the number of\ncurrent users, the version number of the operating system, the amount of free\nmemory or disk space, and so on.\nAnother set of system calls is helpful in debugging a program. Many\nsystems provide system calls to dump() memory. This provision is useful for\ndebugging. A program trace lists each system call as it is executed. Even\nmicroprocessors provide a CPU mode known as single step,i nw h i c hat r a p\nis executed by the CPU after every instruction. The trap is usually caught by a\ndebugger.\nMany operating systems provide a time pro\ufb01le of a program to indicate\nthe amount of time that the program executes at a particular location or set\nof locations. A time pro\ufb01le requires either a tracing facility or regular timer\ninterrupts. At every occurrence of thetimer interrupt, the value of the program\ncounter is recorded. With suf\ufb01ciently frequent timer interrupts, a statistical\npicture of the time spent on various parts of the program can be obtained.\nIn addition, the operating system keeps information about all its processes,\nand system calls are used to access this information. Generally, calls are\nalso used to reset the process information ( get\n process\n attributes() and\nset\n process\n attributes()). In Section 3.1.3, we discuss what information is\nnormally kept.\n2.4.5 Communication\nThere are two common models of interpr ocess communication: the message-\npassing model and the shared-memory model. In themessage-passing model,\nthe communicating processes exchange messages with one another to transfer\ninformation. Messages can be exchanged between the processes either directly\nor indirectly through a common mailbox. Before communication can take\nplace, a connection must be opened. The name of the other communicator\nmust be known, be it another process on the same system or a process on\nanother computer connected by a communications network. Each computer in\nan e t w o r kh a sahost name by which it is commonly known. A host also has a2.4 Types of System Calls 73\nnetwork identi\ufb01er, such as an IP address. Similarly, each process has aprocess\nname, and this name is translated into an identi\ufb01er by which the operating\nsystem can refer to the process. The get\n hostid() and get\n processid()\nsystem calls do this translation. The identi\ufb01ers are then passed to the general-\npurpose open() and close() calls provided by the \ufb01le system or to speci\ufb01c\nopen\n connection() and close\n connection() system calls, depending on the\nsystem\u2019s model of communication. The recipient process usually must give its\npermission for communication to take place with an accept\n connection()\ncall. Most processes that will be receiving connections are special-purpose\ndaemons,w h i c ha r es y s t e mp r o g r a m sp r o v i d e df o rt h a tp u r p o s e .T h e ye x e c u t e\na wait\n for\n connection() call and are awakened when a connection is made.\nThe source of the communication, known as the client, and the receiving\ndaemon, known as aserver,then exchange messages by usingread\n message()\nand write\n message() system calls. The close\n connection() call terminates\nthe communication.\nIn the shared-memory model ,p r o c e s s e su s eshared\n memory\n create()\nand shared\n memory\n attach() system calls to create and gain access to regions\nof memory owned by other processes. Recall that, normally, the operating\nsystem tries to prevent one process from accessing another process\u2019s memory.\nShared memory requires that two or more processes agree to remove this\nrestriction. They can then exchange information by reading and writing data\nin the shared areas. The form of the data is determined by the processes and is\nnot under the operating system\u2019s control. The processes are also responsible for\nensuring that they are not writing to the same location simultaneously. Such\nmechanisms are discussed in Chapter 5. In Chapter 4, we look at a variation of\nthe process scheme\u2014threads\u2014in which memory is shared by default.\nBoth of the models just discussed are common in operating systems,\nand most systems implement both. Message passing is useful for exchanging\nsmaller amounts of data, because no con\ufb02icts need be avoided. It is also easier to\nimplement than is shared memory for intercomputer communication. Shared\nmemory allows maximum speed and convenience of communication, since it\ncan be done at memory transfer speedswhen it takes place within a computer.\nProblems exist, however, in the areas of protection and synchronization\nbetween the processes sharing memory.\n2.4.6 Protection\nProtection provides a mechanism for controlling access to the resources\nprovided by a computer system. Historically, protection was a concern only on\nmultiprogrammed computer systems with several users. However, with the\nadvent of networking and the Internet, all computer systems, from servers to\nmobile handheld devices, must be concerned with protection.\nTypically, system calls providing protection include set\n permission()\nand get\n permission(),w h i c hm a n i p u l a t et h ep e r m i s s i o ns e t t i n g so f\nresources such as \ufb01les and disks. The allow\n user() and deny\n user() system\ncalls specify whether particular users can\u2014or cannot\u2014be allowed access to\ncertain resources.\nWe cover protection in Chapter 14 and the much larger issue of security in\nChapter 15.", "74 Chapter 2 Operating-System Structures\n2.5 System Programs\nAnother aspect of a modern system is its collection of system programs. Recall\nFigure 1.1, which depicted the logical computer hierarchy. At the lowest level is\nhardware. Next is the operating system, then the system programs, and \ufb01nally\nthe application programs. System programs,a l s ok n o w na ssystem utilities,\nprovide a convenient environment fo rp r o g r a md e v e l o p m e n ta n de x e c u t i o n .\nSome of them are simply user interfaces to system calls. Others are considerably\nmore complex. They can be divided into these categories:\n\u2022 File management .T h e s ep r o g r a m sc r e a t e ,d e l e t e ,c o p y ,r e n a m e ,p r i n t ,\ndump, list, and generally manipulate \ufb01les and directories.\n\u2022 Status information.S o m ep r o g r a m ss i m p l ya s kt h es y s t e mf o rt h ed a t e ,\ntime, amount of available memory or disk space, number of users, or\nsimilar status information. Others are more complex, providing detailed\nperformance, logging, and debugging information. Typically, these pro-\ngrams format and print the output to the terminal or other output devices\nor \ufb01les or display it in a window of the GUI.S o m es y s t e m sa l s os u p p o r ta\nregistry,w h i c hi su s e dt os t o r ea n dr e t r i e v ec o n \ufb01 g u r a t i o ni n f o r m a t i o n .\n\u2022 File modi\ufb01cation .S e v e r a lt e x te d i t o r sm a yb ea v a i l a b l et oc r e a t ea n d\nmodify the content of \ufb01les stored on disk or other storage devices. There\nmay also be special commands to search contents of \ufb01les or perform\ntransformations of the text.\n\u2022 Programming-language support.C o m p i l e r s ,a s s e m b l e r s ,d e b u g g e r s ,a n d\ninterpreters for common programming languages (such as C, C++, Java,\nand PERL)a r eo f t e np r o v i d e dw i t ht h eo p e r a t i n gs y s t e mo ra v a i l a b l ea sa\nseparate download.\n\u2022 Program loading and execution . Once a program is assembled or com-\npiled, it must be loaded into memory to be executed. The system may\nprovide absolute loaders, relocatable loaders, linkage editors, and overlay\nloaders. Debugging systems for either higher-level languages or machine\nlanguage are needed as well.\n\u2022 Communications.T h e s ep r o g r a m sp r o v i d et h em e c h a n i s mf o rc r e a t i n g\nvirtual connections among processes, users, and computer systems. They\nallow users to send messages to one another\u2019s screens, to browse Web\npages, to send e-mail messages, to log in remotely, or to transfer \ufb01les from\none machine to another.\n\u2022 Background services .A l lg e n e r a l - p u r p o s es y s t e m sh a v em e t h o d sf o r\nlaunching certain system-program processes at boot time. Some of these\nprocesses terminate after completing their tasks, while others continue\nto run until the system is halted. Constantly running system-program\nprocesses are known asservices, subsystems,o rd a e m o n s .O n ee x a m p l ei s\nthe network daemon discussed in Section 2.4.5. In that example, a system\nneeded a service to listen for network connections in order to connect\nthose requests to the correct processes. Other examples include process\nschedulers that start processes according to a speci\ufb01ed schedule, system\nerror monitoring services, and print servers. Typical systems have dozens2.6 Operating-System Design and Implementation 75\nof daemons. In addition, operating systems that run important activities\nin user context rather than in kernel context may use daemons to run these\nactivities.\nAlong with system programs, most operating systems are supplied with\nprograms that are useful in solving common problems or performing common\noperations. Such application programs include Web browsers, word proces-\nsors and text formatters, spreadsheets, database systems, compilers, plotting\nand statistical-analysis packages, and games.\nThe view of the operating system seen by most users is de\ufb01ned by the\napplication and system programs, rather than by the actual system calls.\nConsider a user\u2019s PC.W h e nau s e r \u2019 sc o m p u t e ri sr u n n i n gt h eM a cOS X\noperating system, the user might see theGUI, featuring a mouse-and-windows\ninterface. Alternatively, or even in one of the windows, the user might have a\ncommand-line UNIX shell. Both use the same set of system calls, but the system\ncalls look different and act in different ways. Further confusing the user view,\nconsider the user dual-booting from Mac OS X into Windows. Now the same\nuser on the same hardware has two entirely different interfaces and two sets of\napplications using the same physical resources. On the same hardware, then,\nau s e rc a nb ee x p o s e dt om u l t i p l eu s e rinterfaces sequentially or concurrently.\n2.6 Operating-System Design and Implementation\nIn this section, we discuss problems we face in designing and implementing an\noperating system. There are, of course, no complete solutions to such problems,\nbut there are approaches that have proved successful.\n2.6.1 Design Goals\nThe \ufb01rst problem in designing a system is to de\ufb01ne goals and speci\ufb01cations.\nAt the highest level, the design of the system will be affected by the choice of\nhardware and the type of system: batch, time sharing, single user, multiuser,\ndistributed, real time, or general purpose.\nBeyond this highest design level, the requirements may be much harder\nto specify. The requirements can, however, be divided into two basic groups:\nuser goals and system goals.\nUsers want certain obvious properties in a system. The system should be\nconvenient to use, easy to learn and t ou s e ,r e l i a b l e ,s a f e ,a n df a s t .O fc o u r s e ,\nthese speci\ufb01cations are not particularly useful in the system design, since there\nis no general agreement on how to achieve them.\nAs i m i l a rs e to fr e q u i r e m e n t sc a nb ed e \ufb01 n e db yt h o s ep e o p l ew h om u s t\ndesign, create, maintain, and operate the system. The system should be easy to\ndesign, implement, and maintain; and it should be \ufb02exible, reliable, error free,\nand ef\ufb01cient. Again, these require ments are vague and may be interpreted in\nvarious ways.\nThere is, in short, no unique solution to the problem of de\ufb01ning the\nrequirements for an operating system. The wide range of systems in existence\nshows that different requirements can result in a large variety of solutions for\ndifferent environments. For example, the requirements for VxWorks, a real-76 Chapter 2 Operating-System Structures\ntime operating system for embedded systems, must have been substantially\ndifferent from those for MVS,al a r g em u l t i u s e r ,m u l t i a c c e s so p e r a t i n gs y s t e m\nfor IBM mainframes.\nSpecifying and designing an operating system is a highly creative task.\nAlthough no textbook can tell you how to do it, general principles have\nbeen developed in the \ufb01eld of software engineering ,a n dw et u r nn o wt o\na discussion of some of these principles.\n2.6.2 Mechanisms and Policies\nOne important principle is the separation of policy from mechanism.M e c h a -\nnisms determine how to do something; policies determine what will be done.\nFor example, the timer construct (see Section 1.5.2) is a mechanism for ensuring\nCPU protection, but deciding how long the timer is to be set for a particular\nuser is a policy decision.\nThe separation of policy and mechanism is important for \ufb02exibility. Policies\nare likely to change across places or over time. In the worst case, each change\nin policy would require a change in the underlying mechanism. A general\nmechanism insensitive to changes in policy would be more desirable. A change\nin policy would then require rede\ufb01nition of only certain parameters of the\nsystem. For instance, consider a mechanism for giving priority to certain types\nof programs over others. If the mechanism is properly separated from policy,\nit can be used either to support a policy decision that I/O-intensive programs\nshould have priority overCPU-intensive ones or to support the opposite policy.\nMicrokernel-based operating systems (Section 2.7.3) take the separation of\nmechanism and policy to one extreme by implementing a basic set of primitive\nbuilding blocks. These blocks are almost policy free, allowing more advanced\nmechanisms and policies to be added via user-created kernel modules or user\nprograms themselves. As an example, consider the history of UNIX.A t\ufb01 r s t ,\nit had a time-sharing scheduler. In the latest version of Solaris, scheduling\nis controlled by loadable tables. Depending on the table currently loaded,\nthe system can be time sharing, batch processing, real time, fair share, or\nany combination. Making the scheduling mechanism general purpose allows\nvast policy changes to be made with a single load-new-table command. At\nthe other extreme is a system such as Windows, in which both mechanism\nand policy are encoded in the system to enforce a global look and feel. All\napplications have similar interfaces, because the interface itself is built into\nthe kernel and system libraries. The Mac OS X operating system has similar\nfunctionality.\nPolicy decisions are important for all resource allocation. Whenever it is\nnecessary to decide whether or not to allocate a resource, a policy decision must\nbe made. Whenever the question is how rather than what,i ti sam e c h a n i s m\nthat must be determined.\n2.6.3 Implementation\nOnce an operating system is designed, it must be implemented. Because\noperating systems are collections of many programs, written by many people\nover a long period of time, it is dif\ufb01cult to make general statements about how\nthey are implemented.", "2.6 Operating-System Design and Implementation 77\nEarly operating systems were written in assembly language. Now, although\nsome operating systems are still written in assembly language, most are written\nin a higher-level language such as C or an even higher-level language such as\nC++. Actually, an operating system can be written in more than one language.\nThe lowest levels of the kernel might be assembly language. Higher-level\nroutines might be in C, and system programs might be in C or C++, in\ninterpreted scripting languages like PERL or Python, or in shell scripts. In\nfact, a given Linux distribution probably includes programs written in all of\nthose languages.\nThe \ufb01rst system that was not written in assembly language was probably\nthe Master Control Program (MCP) for Burroughs computers.MCP was written\nin a variant of ALGOL. MULTICS,d e v e l o p e da tMIT,w a sw r i t t e nm a i n l yi n\nthe system programming language PL/1.T h eL i n u xa n dW i n d o w so p e r a t i n g\nsystem kernels are written mostly in C, although there are some small sections\nof assembly code for device drivers and for saving and restoring the state of\nregisters.\nThe advantages of using a higher-level language, or at least a systems-\nimplementation language, for implementing operating systems are the same\nas those gained when the language is used for application programs: the code\ncan be written faster, is more compact, and is easier to understand and debug.\nIn addition, improvements in compiler technology will improve the generated\ncode for the entire operating system by simple recompilation. Finally, an\noperating system is far easier to port\u2014to move to some other hardware\u2014\nif it is written in a higher-level language. For example, MS-DOS was written in\nIntel 8088 assembly language. Consequently, it runs natively only on the Intel\nX86 family of CPUs. (Note that although MS-DOS runs natively only on Intel\nX86, emulators of the X86 instruction set allow the operating system to run on\nother CPUs\u2014but moreslowly ,andwithhigherresourceuse.Aswementioned\nin Chapter 1, emulators are programs that duplicate the functionality of one\nsystem on another system.) The Linux operating system, in contrast, is written\nmostly in C and is available natively on a number of different CPUs, including\nIntel X86, Oracle SPARC,a n dIBMPowerPC.\nThe only possible disadvantages of implementing an operating system in a\nhigher-level language are reduced speed and increased storage requirements.\nThis, however, is no longer a major issue in today\u2019s systems. Although an\nexpert assembly-language programmer can produce ef\ufb01cient small routines,\nfor large programs a modern compiler can perform complex analysis and apply\nsophisticated optimizations that produce excellent code. Modern processors\nhave deep pipelining and multiple functional units that can handle the details\nof complex dependencies much more easily than can the human mind.\nAs is true in other systems, major performance improvements in oper-\nating systems are more likely to be the result of better data structures and\nalgorithms than of excellent assembly-language code. In addition, although\noperating systems are large, only a small amount of the code is critical to high\nperformance; the interrupt handler, I/O manager, memory manager, and CPU\nscheduler are probably the most critical routines. After the system is written\nand is working correctly, bottleneck routines can be identi\ufb01ed and can be\nreplaced with assembly-language equivalents.78 Chapter 2 Operating-System Structures\n2.7 Operating-System Structure\nAs y s t e ma sl a r g ea n dc o m p l e xa sam o d e r no p e r a t i n gs y s t e mm u s tb e\nengineered carefully if it is to function properly and be modi\ufb01ed easily. A\ncommon approach is to partition the task into small components, or modules,\nrather than have one monolithic system. Each of these modules should be\naw e l l - d e \ufb01 n e dp o r t i o no ft h es y s t e m ,w i t hc a r e f u l l yd e \ufb01 n e di n p u t s ,o u t p u t s ,\nand functions. We have already discussed brie\ufb02y in Chapter 1 the common\ncomponents of operating systems. In this section, we discuss how these\ncomponents are interconnected and melded into a kernel.\n2.7.1 Simple Structure\nMany operating systems do not have well-de\ufb01ned structures. Frequently, such\nsystems started as small, simple, and limited systems and then grew beyond\ntheir original scope. MS-DOS is an example of such a system. It was originally\ndesigned and implemented by a few people who had no idea that it would\nbecome so popular. It was written to provide the most functionality in the\nleast space, so it was not carefully divided into modules. Figure 2.11 shows its\nstructure.\nIn MS-DOS,t h ei n t e r f a c e sa n dl e v e l so ff u n c t i o n a l i t ya r en o tw e l ls e p a r a t e d .\nFor instance, application programs are able to access the basic I/O routines\nto write directly to the display and disk drives. Such freedom leaves MS-DOS\nvulnerable to errant (or malicious) programs, causing entire system crashes\nwhen user programs fail. Of course, MS-DOS was also limited by the hardware\nof its era. Because the Intel 8088 for which it was written provides no dual\nmode and no hardware protection, the designers of MS-DOS had no choice but\nto leave the base hardware accessible.\nAnother example of limited structuring is the original UNIX operating\nsystem. Like MS-DOS, UNIX initially was limited by hardware functionality. It\nconsists of two separable parts: the kernel and the system programs. The kernel\nROM BIOS device drivers\napplication program\nMS-DOS device drivers\nresident system program\nFigure 2.11 MS-DOS layer structure.2.7 Operating-System Structure 79\nkernel\n(the users)\nshells and commands\ncompilers and interpreters\nsystem libraries\nsystem-call interface to the kernel\nsignals terminal\nhandling\ncharacter I/O system\nterminal drivers\nfile system\nswapping block I/O\nsystem\ndisk and tape drivers\nCPU scheduling\npage replacement\ndemand paging\nvirtual memory\nkernel interface to the hardware\nterminal controllers\nterminals\ndevice controllers\ndisks and tapes\nmemory controllers\nphysical memory\nFigure 2.12 Traditional UNIX system structure.\nis further separated into a series of in terfaces and device drivers, which have\nbeen added and expanded over the years asUNIX has evolved. We can view the\ntraditional UNIX operating system as being layered to some extent, as shown in\nFigure 2.12. Everything below the system-call interface and above the physical\nhardware is the kernel. The kernel provides the \ufb01le system, CPU scheduling,\nmemory management, and other operating-system functions through system\ncalls. Taken in sum, that is an enormous amount of functionality to be combined\ninto one level. This monolithic structure was dif\ufb01cult to implement and\nmaintain. It had a distinct performance advantage, however: there is very little\noverhead in the system call interface or in communication within the kernel.\nWe still see evidence of this simple, monolithic structure in the UNIX,L i n u x ,\nand Windows operating systems.\n2.7.2 Layered Approach\nWith proper hardware support, operating systems can be broken into pieces\nthat are smaller and more appropriate than those allowed by the original\nMS-DOS and UNIX systems. The operating system can then retain much greater\ncontrol over the computer and over the applications that make use of that\ncomputer. Implementers have more freedom in changing the inner workings\nof the system and in creating modular operating systems. Under a top-\ndown approach, the overall functionality and features are determined and\nare separated into components. Information hiding is also important, because\nit leaves programmers free to implement the low-level routines as they see \ufb01t,\nprovided that the external inte rface of the routine stays unchanged and that\nthe routine itself performs the advertised task.\nAs y s t e mc a nb em a d em o d u l a ri nm a n yw a y s .O n em e t h o di st h elayered\napproach,i nw h i c ht h eo p e r a t i n gs y s t e mi sb r o k e ni n t oan u m b e ro fl a y e r s\n(levels). The bottom layer (layer 0) is the hardware; the highest (layerN)i st h e\nuser interface. This layering structure is depicted in Figure 2.13.", "80 Chapter 2 Operating-System Structures\nlayer N\nuser interface\n\u2022\u2022\u2022\nlayer 1\nlayer 0\nhardware\nFigure 2.13 Al a y e r e do p e r a t i n gs y s t e m .\nAn operating-system layer is an implementation of an abstract object made\nup of data and the operations that can manipulate those data. A typical\noperating-system layer\u2014say, layer M\u2014consists of data structures and a set\nof routines that can be invoked by higher-level layers. Layer M, in turn, can\ninvoke operations on lower-level layers.\nThe main advantage of the layered approach is simplicity of construction\nand debugging. The layers are selected so that each uses functions (operations)\nand services of only lower-level layers. Th is approach simpli\ufb01es debugging\nand system veri\ufb01cation. The \ufb01rst layer can be debugged without any concern\nfor the rest of the system, because, by de\ufb01nition, it uses only the basic hardware\n(which is assumed correct) to implement its functions. Once the \ufb01rst layer is\ndebugged, its correct functioning can be assumed while the second layer is\ndebugged, and so on. If an error is found during the debugging of a particular\nlayer, the error must be on that layer, because the layers below it are already\ndebugged. Thus, the design and implementation of the system are simpli\ufb01ed.\nEach layer is implemented only with operations provided by lower-level\nlayers. A layer does not need to know how these operations are implemented;\nit needs to know only what these operations do. Hence, each layer hides the\nexistence of certain data structures, operations,and hardware from higher-level\nlayers.\nThe major dif\ufb01culty with the layered approach involves appropriately\nde\ufb01ning the various layers. Because a layer can use only lower-level layers,\ncareful planning is necessary. For example, the device driver for the backing\nstore (disk space used by virtual-memory algorithms) must be at a lower\nlevel than the memory-management ro utines, because memory management\nrequires the ability to use the backing store.\nOther requirements may not be so obvious. The backing-store driver would\nnormally be above the CPU scheduler, because the driver may need to wait for\nI/O and the CPU can be rescheduled during this time. However, on a large2.7 Operating-System Structure 81\nsystem, the CPU scheduler may have more information about all the active\nprocesses than can \ufb01t in memory. Therefore, this information may need to be\nswapped in and out of memory, requiring the backing-store driver routine to\nbe below the CPU scheduler.\nA\ufb01 n a lp r o b l e mw i t hl a y e r e di m p l e m e n t a t i o n si st h a tt h e yt e n dt ob el e s s\nef\ufb01cient than other types. For instance, when a user program executes an I/O\noperation, it executes a system call that is trapped to the I/O layer, which calls\nthe memory-management layer, which in turn calls the CPU-scheduling layer,\nwhich is then passed to the hardware. At each layer, the parameters may be\nmodi\ufb01ed, data may need to be passed, and so on. Each layer adds overhead to\nthe system call. The net result is a system call that takes longer than does one\non a nonlayered system.\nThese limitations have caused a small backlash against layering in recent\nyears. Fewer layers with more functionality are being designed, providing\nmost of the advantages of modularized code while avoiding the problems of\nlayer de\ufb01nition and interaction.\n2.7.3 Microkernels\nWe have already seen that as UNIX expanded, the kernel became large\nand dif\ufb01cult to manage. In the mid-1980s, researchers at Carnegie Mellon\nUniversity developed an operating system called Mach that modularized\nthe kernel using the microkernel approach. This method structures the\noperating system by removing all nonessential components from the kernel and\nimplementing them as system and user-level programs. The result is a smaller\nkernel. There is little consensus regarding which services should remain in the\nkernel and which should be implemented in user space. Typically, however,\nmicrokernels provide minimal process and memory management, in addition\nto a communication facility. Figure 2.14 illustrates the architecture of a typical\nmicrokernel.\nThe main function of the microkernel is to provide communication between\nthe client program and the various services that are also running in user space.\nCommunication is provided through message passing, which was described\nin Section 2.4.5. For example, if the client program wishes to access a \ufb01le, it\nApplication\nProgram\nFile\nSystem\nDevice\nDriver\nInterprocess\nCommunication\nmemory\nmanagment\nCPU\nscheduling\nmessagesmessages\nmicrokernel\nhardware\nuser\nmode\nkernel\nmode\nFigure 2.14 Architecture of a typical microkernel.82 Chapter 2 Operating-System Structures\nmust interact with the \ufb01le server. The client program and service never interact\ndirectly. Rather, they communicate indirectly by exchanging messages with the\nmicrokernel.\nOne bene\ufb01t of the microkernel a pproach is that it makes extending\nthe operating system easier. All new services are added to user space and\nconsequently do not require modi\ufb01cation of the kernel. When the kernel does\nhave to be modi\ufb01ed, the changes tend to be fewer, because the microkernel is\nas m a l l e rk e r n e l .T h er e s u l t i n go p e r a t i n gs y s t e mi se a s i e rt op o r tf r o mo n e\nhardware design to another. The microkernel also provides more security\nand reliability, since most services are running as user\u2014rather than kernel\u2014\nprocesses. If a service fails, the rest of the operating system remains untouched.\nSome contemporary operating systems have used the microkernel\napproach. Tru64UNIX (formerly Digital UNIX)p r o v i d e saUNIX interface to the\nuser, but it is implemented with a Mac hk e r n e l .T h eM a c hk e r n e lm a p sUNIX\nsystem calls into messages to the appropriate user-level services. The MacOS X\nkernel (also known as Darwin)i sa l s op a r t l yb a s e do nt h eM a c hm i c r o k e r n e l .\nAnother example is QNX,ar e a l - t i m eo p e r a t i n gs y s t e mf o re m b e d d e d\nsystems. The QNX Neutrino microkernel provides services for message passing\nand process scheduling. It also handles low-level network communication\nand hardware interrupts. All other services in QNX are provided by standard\nprocesses that run outside the kernel in user mode.\nUnfortunately, the performance of microkernels can suffer due to increased\nsystem-function overhead. Consider the history of Windows NT.T h e\ufb01 r s t\nrelease had a layered microkernel organization. This version\u2019s performance\nwas low compared with that of Windows 95. Windows NT 4.0 partially\ncorrected the performance problem by moving layers from user space to\nkernel space and integrating them more closely. By the time Windows XP\nwas designed, Windows architecture had become more monolithic than\nmicrokernel.\n2.7.4 Modules\nPerhaps the best current m ethodology for operating-system design involves\nusing loadable kernel modules.H e r e ,t h ek e r n e lh a sas e to fc o r ec o m p o n e n t s\nand links in additional services via modules, either at boot time or during run\ntime. This type of design is common in modern implementations ofUNIX,s u c h\nas Solaris, Linux, and Mac OS X,a sw e l la sW i n d o w s .\nThe idea of the design is for the kernel to provide core services while\nother services are implemented dynamically, as the kernel is running. Linking\nservices dynamically is preferable to adding new features directly to the kernel,\nwhich would require recompiling the kernel every time a change was made.\nThus, for example, we might build CPU scheduling and memory management\nalgorithms directly into the kernel and then add support for different \ufb01le\nsystems by way of loadable modules.\nThe overall result resembles a layered system in that each kernel section\nhas de\ufb01ned, protected interfaces; but it is more \ufb02exible than a layered system,\nbecause any module can call any other module. The approach is also similar to\nthe microkernel approach in that the primary module has only core functions\nand knowledge of how to load and communicate with other modules; but it", "2.7 Operating-System Structure 83\ncore Solaris\nkernel\nfile systems\nloadable\nsystem calls\nexecutable\nformats\nSTREAMS\nmodules\nmiscellaneous\nmodules\ndevice and\nbus drivers\nscheduling\nclasses\nFigure 2.15 Solaris loadable modules.\nis more ef\ufb01cient, because modules do not need to invoke message passing in\norder to communicate.\nThe Solaris operating system structure, shown in Figure 2.15, is organized\naround a core kernel with seven types of loadable kernel modules:\n1. Scheduling classes\n2. File systems\n3. Loadable system calls\n4. Executable formats\n5. STREAMS modules\n6. Miscellaneous\n7. Device and bus drivers\nLinux also uses loadable kernel modules, primarily for supporting device\ndrivers and \ufb01le systems. We cover creating loadable kernel modules in Linux\nas a programming exercise at the end of this chapter.\n2.7.5 Hybrid Systems\nIn practice, very few operating systems adopt a single, strictly de\ufb01ned\nstructure. Instead, they combine dif ferent structures, resulting in hybrid\nsystems that address performance, security, and usability issues. For example,\nboth Linux and Solaris are monolithic, because having the operating system\nin a single address space provides very ef\ufb01cient performance. However,\nthey are also modular, so that new functionality can be dynamically added\nto the kernel. Windows is largely monolithic as well (again primarily for\nperformance reasons), but it retains some behavior typical of microkernel\nsystems, including providing support for separate subsystems (known as\noperating-system personalities) that run as user-mode processes. Windows\nsystems also provide support for dynamically loadable kernel modules. We\nprovide case studies of Linux and Windows 7 in in Chapters 18 and 19,\nrespectively. In the remainder of this section, we explore the structure of84 Chapter 2 Operating-System Structures\nthree hybrid systems: the Apple Mac OS X operating system and the two most\nprominent mobile operating systems\u2014i OS and Android.\n2.7.5.1 Mac OS X\nThe Apple Mac OS X operating system uses a hybrid structure. As shown in\nFigure 2.16, it is a layered system. The top layers include theAqua user interface\n(Figure 2.4) and a set of application environments and services. Notably,\nthe Cocoa environment speci\ufb01es an API for the Objective-C programming\nlanguage, which is used for writing Mac OS X applications. Below these\nlayers is the kernel environment ,w h i c hc o n s i s t sp r i m a r i l yo ft h eM a c h\nmicrokernel and the BSD UNIX kernel. Mach provides memory management;\nsupport for remote procedure calls ( RPCs) and interprocess communication\n(IPC) facilities, including message passing; and thread scheduling. The BSD\ncomponent provides a BSD command-line interface, support for networking\nand \ufb01le systems, and an implementation of POSIX API s, including Pthreads.\nIn addition to Mach and BSD,t h ek e r n e le n v i r o n m e n tp r o v i d e sa nI/O kit\nfor development of device drivers and dynamically loadable modules (which\nMac OS X refers to as kernel extensions). As shown in Figure 2.16, the BSD\napplication environment can make use of BSD facilities directly.\n2.7.5.2 iOS\niOS is a mobile operating system designed by Apple to run its smartphone, the\niPhone,a sw e l la si t st a b l e tc o m p u t e r ,t h eiPad.i OS is structured on the Mac\nOS X operating system, with added functionality pertinent to mobile devices,\nbut does not directly run Mac OS X applications. The structure of iOS appears\nin Figure 2.17.\nCocoa Touchis anAPI for Objective-C that provides several frameworks for\ndeveloping applications that run on i OS devices. The fundamental difference\nbetween Cocoa, mentioned earlier, and Cocoa Touch is that the latter provides\nsupport for hardware features unique to mobile devices, such as touch screens.\nThe media services layer provides services for graphics, audio, and video.\ngraphical user interface Aqua\napplication environments and services\nkernel environment\nJava Cocoa Quicktime BSD\nMach\nI/O kit kernel extensions\nBSD\nFigure 2.16 The Mac OS X structure.2.7 Operating-System Structure 85\nCocoa Touch\nMedia Services\nCore Services\nCore OS\nFigure 2.17 Architecture of Apple\u2019s iOS.\nThe core services layer provides a variety of features, including support for\ncloud computing and databases. The bottom layer represents the core operating\nsystem, which is based on the kernel environment shown in Figure 2.16.\n2.7.5.3 Android\nThe Android operating system was designed by the Open Handset Alliance\n(led primarily by Google) and was developed for Android smartphones and\ntablet computers. Whereas i OS is designed to run on Apple mobile devices\nand is close-sourced, Android runs on a variety of mobile platforms and is\nopen-sourced, partly explaining its rapid rise in popularity. The structure of\nAndroid appears in Figure 2.18.\nAndroid is similar to i OS in that it is a layered stack of software that\nprovides a rich set of frameworks for d eveloping mobile applications. At the\nbottom of this software stack is the Linux kernel, although it has been modi\ufb01ed\nby Google and is currently outside the normal distribution of Linux releases.\nApplications\nApplication Framework\nAndroid runtime\nCore Libraries\nDalvik\nvirtual machine\nLibraries\nLinux kernel\nSQLite openGL\nsurface\nmanager\nwebkit libc\nmedia\nframework\nFigure 2.18 Architecture of Google\u2019s Android.", "86 Chapter 2 Operating-System Structures\nLinux is used primarily for process, memory, and device-driver support for\nhardware and has been expanded to include power management. The Android\nruntime environment includes a core set of libraries as well as the Dalvik virtual\nmachine. Software designers for Android devices develop applications in the\nJava language. However, rather than using the standard Java API,G o o g l eh a s\ndesigned a separate Android API for Java development. The Java class \ufb01les are\n\ufb01rst compiled to Java bytecode and thent r a n s l a t e di n t oa ne x e c u t a b l e\ufb01 l et h a t\nruns on the Dalvik virtual machine. The Dalvik virtual machine was designed\nfor Android and is optimized for mobile devices with limited memory and\nCPU processing capabilities.\nThe set of libraries available for Android applications includes frameworks\nfor developing web browsers (webkit), database support (SQLite), and multi-\nmedia. The libc library is similar to the standard C library but is much smaller\nand has been designed for the slower CPUst h a tc h a r a c t e r i z em o b i l ed e v i c e s .\n2.8 Operating-System Debugging\nWe have mentioned debugging frequently in this chapter. Here, we take a closer\nlook. Broadly,debugging is the activity of \ufb01nding and \ufb01xing errors in a system,\nboth in hardware and in software. Performance problems are considered bugs,\nso debugging can also include performance tuning,w h i c hs e e k st oi m p r o v e\nperformance by removing processing bottlenecks. In this section, we explore\ndebugging process and kernel errors and performance problems. Hardware\ndebugging is outside the scope of this text.\n2.8.1 Failure Analysis\nIf a process fails, most operating systems write the error information to a log\n\ufb01le to alert system operators or users that the problem occurred. The operating\nsystem can also take a core dump\u2014a capture of the memory of the process\u2014\nand store it in a \ufb01le for later analysis. (Memory was referred to as the \u201ccore\u201d\nin the early days of computing.) Running programs and core dumps can be\nprobed by a debugger, which allows a programmer to explore the code and\nmemory of a process.\nDebugging user-level process code is a challenge. Operating-system kernel\ndebugging is even more complex bec ause of the size and complexity of the\nkernel, its control of the hardware, and the lack of user-level debugging tools.\nAf a i l u r ei nt h ek e r n e li sc a l l e dacrash.W h e nac r a s ho c c u r s ,e r r o ri n f o r m a t i o n\nis saved to a log \ufb01le, and the memory state is saved to a crash dump.\nOperating-system debugging and process debugging frequently use dif-\nferent tools and techniques due to the very different nature of these two tasks.\nConsider that a kernel failure in the \ufb01le-system code would make it risky for\nthe kernel to try to save its state to a \ufb01le on the \ufb01le system before rebooting.\nAc o m m o nt e c h n i q u ei st os a v et h ek e r n e l \u2019 sm e m o r ys t a t et oas e c t i o no fd i s k\nset aside for this purpose that contains no \ufb01le system. If the kernel detects\nan unrecoverable error, it writes the entire contents of memory, or at least the\nkernel-owned parts of the system memory, to the disk area. When the system\nreboots, a process runs to gather the data from that area and write it to a crash2.8 Operating-System Debugging 87\nKernighan\u2019s Law\n\u201cDebugging is twice as hard as writing the code in the \ufb01rst place. Therefore,\nif you write the code as cleverly as possible, you are, by de\ufb01nition, not smart\nenough to debug it.\u201d\ndump \ufb01le within a \ufb01le system for analysis. Obviously, such strategies would\nbe unnecessary for debugging ordinary user-level processes.\n2.8.2 Performance Tuning\nWe mentioned earlier that performance tuning seeks to improve performance\nby removing processing bottlenecks. To identify bottlenecks, we must be able\nto monitor system performance. Thus, the operating system must have some\nmeans of computing and displaying measures of system behavior. In a number\nof systems, the operating system does this by producingtrace listingsof system\nbehavior. All interesting events ar el o g g e dw i t ht h e i rt i m ea n di m p o r t a n t\nparameters and are written to a \ufb01le. Later, an analysis program can process\nthe log \ufb01le to determine system performance and to identify bottlenecks and\ninef\ufb01ciencies. These same traces can be run as input for a simulation of a\nsuggested improved system. Traces also can help people to \ufb01nd errors in\noperating-system behavior.\nAnother approach to performance tuning uses single-purpose, interactive\ntools that allow users and administrators to question the state of various system\ncomponents to look for bottlenecks. One such tool employs theUNIX command\ntop to display the resources used on the system, as well as a sorted list of\nthe \u201ctop\u201d resource-using processes. Other tools display the state of disk I/O,\nmemory allocation, and network traf\ufb01c.\nThe Windows Task Manager is a similar tool for Windows systems. The\ntask manager includes information for current applications as well as processes,\nCPU and memory usage, and networking statistics. A screen shot of the task\nmanager appears in Figure 2.19.\nMaking operating systems easier to understand, debug, and tune as they\nrun is an active area of research and implementation. A new generation of\nkernel-enabled performance analysis tools has made signi\ufb01cant improvements\nin how this goal can be achieved. Next, we discuss a leading example of such\nat o o l :t h eS o l a r i s1 0DTrace dynamic tracing facility.\n2.8.3 DTrace\nDTrace is a facility that dynamically adds probes to a running system, both\nin user processes and in the kernel. These probes can be queried via the D\nprogramming language to determine an astonishing amount about the kernel,\nthe system state, and process activities. For example, Figure 2.20 follows an\napplication as it executes a system call ( ioctl())a n ds h o w st h ef u n c t i o n a l\ncalls within the kernel as they execute to perform the system call. Lines ending\nwith \u201cU\u201d are executed in user mode, and lines ending in \u201cK\u201d in kernel mode.88 Chapter 2 Operating-System Structures\nFigure 2.19 The Windows task manager.\nDebugging the interactions between user-level and kernel code is nearly\nimpossible without a toolset that understands both sets of code and can\ninstrument the interactions. For that toolset to be truly useful, it must be able\nto debug any area of a system, including areas that were not written with\ndebugging in mind, and do so without affecting system reliability. This tool\nmust also have a minimum performance impact\u2014ideally it should have no\nimpact when not in use and a proportional impact during use. TheDTrace tool\nmeets these requirements and providesa dynamic, safe, low-impact debugging\nenvironment.\nUntil the DTrace framework and tools became available with Solaris 10,\nkernel debugging was usually shrouded in mystery and accomplished via\nhappenstance and archaic code and tools. For example,CPUsh a v eab r e a k p o i n t\nfeature that will halt execution and allow a debugger to examine the state of the\nsystem. Then execution can continue until the next breakpoint or termination.\nThis method cannot be used in a multiuser operating-system kernel without\nnegatively affecting all of the users on the system.Pro\ufb01ling,w h i c hp e r i o d i c a l l y\nsamples the instruction pointer to determine which code is being executed, can\nshow statistical trends but not individual activities. Code can be included in\nthe kernel to emit speci\ufb01c data under speci\ufb01c circumstances, but that code\nslows down the kernel and tends not to be included in the part of the kernel\nwhere the speci\ufb01c problem being debugged is occurring.", "2.8 Operating-System Debugging 89\n# ./all.d \u2018pgrep xclock\u2018 XEventsQueued\ndtrace: script \u2019./all.d\u2019 matched 52377 probes\nCPU FUNCTION\n  0 \u2013> XEventsQueued    U\n  0   \u2013> _XEventsQueued   U\n  0     \u2013> _X11TransBytesReadable  U\n  0     <\u2013 _X11TransBytesReadable  U\n  0     \u2013> _X11TransSocketBytesReadable U\n  0     <\u2013 _X11TransSocketBytesreadable U\n  0     \u2013> ioctl    U\n  0       \u2013> ioctl    K\n  0         \u2013> getf    K\n  0           \u2013> set_active_fd  K\n  0           <\u2013 set_active_fd  K\n  0         <\u2013 getf    K\n  0         \u2013> get_udatamodel  K\n  0         <\u2013 get_udatamodel  K\n...\n  0         \u2013> releasef   K\n  0           \u2013> clear_active_fd  K\n  0           <\u2013 clear_active_fd  K\n  0           \u2013> cv_broadcast  K\n  0           <\u2013 cv_broadcast  K\n  0         <\u2013 releasef   K\n  0       <\u2013 ioctl    K\n  0     <\u2013 ioctl    U\n  0   <\u2013 _XEventsQueued   U\n  0 <\u2013 XEventsQueued    U\nFigure 2.20 Solaris 10dtrace follows a system call within the kernel.\nIn contrast, DTrace runs on production systems\u2014systems that are running\nimportant or critical applications\u2014and causes no harm to the system. It\nslows activities while enabled, but after execution it resets the system to its\npre-debugging state. It is also a broad and deep tool. It can broadly debug\neverything happening in the system (both at the user and kernel levels and\nbetween the user and kernel layers). It cana l s od e l v ed e e pi n t oc o d e ,s h o w i n g\nindividual CPU instructions or kernel subroutine activities.\nDTrace is composed of a compiler, a framework, providers of probes\nwritten within that framework, and consumers of those probes. DTrace\nproviders create probes. Kernel structures exist to keep track of all probes that\nthe providers have created. The probes are stored in a hash-table data structure\nthat is hashed by name and indexed according to unique probe identi\ufb01ers.\nWhen a probe is enabled, a bit of code in the area to be probed is rewritten\nto call dtrace\n probe(probe identifier) and then continue with the code\u2019s\noriginal operation. Different providers create different kinds of probes. For\nexample, a kernel system-call probe works differently from a user-process\nprobe, and that is different from an I/O probe.\nDTrace features a compiler that generates a byte code that is run in the\nkernel. This code is assured to be\u201csafe\u201d by the compiler. For example, no loops\nare allowed, and only speci\ufb01c kernel state modi\ufb01cations are allowed when\nspeci\ufb01cally requested. Only users with DTrace \u201cprivileges\u201d (or \u201croot\u201d users)90 Chapter 2 Operating-System Structures\nare allowed to use DTrace, as it can retrieve private kernel data (and modify\ndata if requested). The generated code r uns in the kernel and enables probes.\nIt also enables consumers in user mode ande n a b l e sc o m m u n i c a t i o n sb e t w e e n\nthe two.\nA DTrace consumer is code that is interested in a probe and its results.\nAc o n s u m e rr e q u e s t st h a tt h ep r o v i d e rc r e a t eo n eo rm o r ep r o b e s .W h e na\nprobe \ufb01res, it emits data that are managed by the kernel. Within the kernel,\nactions called enabling control blocks ,o r ECBs, are performed when probes\n\ufb01re. One probe can cause multiple ECBst oe x e c u t ei fm o r et h a no n ec o n s u m e r\nis interested in that probe. Each ECB contains a predicate (\u201cif statement\u201d)t h a t\ncan \ufb01lter out that ECB.O t h e r w i s e ,t h el i s to fa c t i o n si nt h eECB is executed. The\nmost common action is to capture some bit of data, such as a variable\u2019s value at\nthat point of the probe execution. By gathering such data, a complete picture of\nau s e ro rk e r n e la c t i o nc a nb eb u i l t .F u r t h e r ,p r o b e s\ufb01 r i n gf r o mb o t hu s e rs p a c e\nand the kernel can show how a user-level action caused kernel-level reactions.\nSuch data are invaluable for performance monitoring and code optimization.\nOnce the probe consumer terminates, its ECBsa r er e m o v e d .I ft h e r ea r en o\nECBs consuming a probe, the probe is removed. That involves rewriting the\ncode to remove thedtrace\n probe() call and put back the original code. Thus,\nbefore a probe is created and after it is destroyed, the system is exactly the\nsame, as if no probing occurred.\nDTrace takes care to assure that probes do not use too much memory or\nCPU capacity, which could harm the running system. The buffers used to hold\nthe probe results are monitored for exceeding default and maximum limits.\nCPU time for probe execution is monitored as well. If limits are exceeded, the\nconsumer is terminated, along with the offending probes. Buffers are allocated\nper CPU to avoid contention and data loss.\nAn example of D code and its output shows some of its utility. The following\nprogram shows the DTrace code to enable scheduler probes and record the\namount of CPU time of each process running with user ID 101 while those\nprobes are enabled (that is, while the program runs):\nsched:::on-cpu\nuid == 101\n{\nself->ts = timestamp;\n}\nsched:::off-cpu\nself->ts\n{\n@time[execname] = sum(timestamp - self->ts);\nself->ts = 0;\n}\nThe output of the program, showing the processes and how much time (in\nnanoseconds) they spend running on the CPUs, is shown in Figure 2.21.\nBecause DTrace is part of the open-source OpenSolaris version of the Solaris\n10 operating system, it has been added to other operating systems when those2.9 Operating-System Generation 91\n#d t r a c e- ss c h e d . d\ndtrace: script \u2019sched.d\u2019 matched 6 probes\n\u02c6C\ngnome-settings-d 142354\ngnome-vfs-daemon 158243\ndsdm 189804\nwnck-applet 200030\ngnome-panel 277864\nclock-applet 374916\nmapping-daemon 385475\nxscreensaver 514177\nmetacity 539281\nXorg 2579646\ngnome-terminal 5007269\nmixer\n applet2 7388447\njava 10769137\nFigure 2.21 Output of the D code.\nsystems do not have con\ufb02icting license agreements. For example, DTrace has\nbeen added to Mac OS X and FreeBSD and will likely spread further due to its\nunique capabilities. Other operating systems, especially the Linux derivatives,\nare adding kernel-tracing functionality as well. Still other operating systems\nare beginning to include performance and tracing tools fostered by research at\nvarious institutions, including the Paradyn project.\n2.9 Operating-System Generation\nIt is possible to design, code, and implement an operating system speci\ufb01cally\nfor one machine at one site. More commonly, however, operating systems\nare designed to run on any of a class of machines at a variety of sites with\na variety of peripheral con\ufb01gurations. The system must then be con\ufb01gured\nor generated for each speci\ufb01c computer site, a process sometimes known as\nsystem generation SYSGEN.\nThe operating system is normally distributed on disk, on CD-ROM or\nDVD-ROM,o ra sa n \u201cISO\u201d image, which is a \ufb01le in the format of a CD-ROM\nor DVD-ROM.T og e n e r a t eas y s t e m ,w eu s eas p e c i a lp r o g r a m .T h i sSYSGEN\nprogram reads from a given \ufb01le, or asks the operator of the system for\ninformation concerning the speci\ufb01c con\ufb01guration of the hardware system, or\nprobes the hardware directly to d etermine what components are there. The\nfollowing kinds of information must be determined.\n\u2022 What CPU is to be used? What options (extended instruction sets, \ufb02oating-\npoint arithmetic, and so on) are installed? For multiple CPU systems, each\nCPU may be described.\n\u2022 How will the boot disk be formatted? How many sections, or\u201cpartitions,\u201d\nwill it be separated into, and what will go into each partition?", "92 Chapter 2 Operating-System Structures\n\u2022 How much memory is available? Some systems will determine this value\nthemselves by referencing memory location after memory location until an\n\u201cillegal address\u201d fault is generated. This procedure de\ufb01nes the \ufb01nal legal\naddress and hence the amount of available memory.\n\u2022 What devices are available? The system will need to know how to address\neach device (the device number), the devicei n t e r r u p tn u m b e r ,t h ed e v i c e \u2019 s\ntype and model, and any special device characteristics.\n\u2022 What operating-system options are desired, or what parameter values are\nto be used? These options or values might include how many buffers of\nwhich sizes should be used, what type of CPU-scheduling algorithm is\ndesired, what the maximum number of processes to be supported is, and\nso on.\nOnce this information is determined, it can be used in several ways. At one\nextreme, a system administrator can use it to modify a copy of the source code of\nthe operating system. The operating system then is completely compiled. Data\ndeclarations, initializations, and constants, along with conditional compilation,\nproduce an output-object version of the operating system that is tailored to the\nsystem described.\nAt a slightly less tailored level, the system description can lead to the\ncreation of tables and the selection of modules from a precompiled library.\nThese modules are linked together to form the generated operating system.\nSelection allows the library to contain the device drivers for all supported I/O\ndevices, but only those needed are linked into the operating system. Because\nthe system is not recompiled, system generation is faster, but the resulting\nsystem may be overly general.\nAt the other extreme, it is possible to construct a system that is completely\ntable driven. All the code is always part of the system, and selection occurs at\nexecution time, rather than at compileor link time. System generation involves\nsimply creating the appropriate tables to describe the system.\nThe major differences among these approaches are the size and generality\nof the generated system and the ease of modifying it as the hardware\ncon\ufb01guration changes. Consider the cost of modifying the system to support a\nnewly acquired graphics terminal or another disk drive. Balanced against that\ncost, of course, is the frequency (or infrequency) of such changes.\n2.10 System Boot\nAfter an operating system is generated, it must be made available for use by\nthe hardware. But how does the hardware know where the kernel is or how to\nload that kernel? The procedure of starting a computer by loading the kernel\nis known as booting the system. On most computer systems, a small piece of\ncode known as the bootstrap program or bootstrap loader locates the kernel,\nloads it into main memory, and starts its execution. Some computer systems,\nsuch as PCs, use a two-step process in which a simple bootstrap loader fetches\nam o r ec o m p l e xb o o tp r o g r a mf r o md i s k ,w h i c hi nt u r nl o a d st h ek e r n e l .\nWhen a CPU receives a reset event\u2014for instance, when it is powered up\nor rebooted\u2014the instruction register is loaded with a prede\ufb01ned memory2.11 Summary 93\nlocation, and execution starts there. At that location is the initial bootstrap\nprogram. This program is in the form of read-only memory ( ROM), because\nthe RAM is in an unknown state at system startup. ROM is convenient because\nit needs no initialization and cannot easily be infected by a computer virus.\nThe bootstrap program can perform a variety of tasks. Usually, one task\nis to run diagnostics to determine the state of the machine. If the diagnostics\npass, the program can continue with the booting steps. It can also initialize all\naspects of the system, fromCPU registers to device controllers and the contents\nof main memory. Sooner or later, it starts the operating system.\nSome systems\u2014such as cellular phones, tablets, and game consoles\u2014store\nthe entire operating system in ROM.S t o r i n gt h eo p e r a t i n gs y s t e mi nROM is\nsuitable for small operating systems, simple supporting hardware, and rugged\noperation. A problem with this approach is that changing the bootstrap code\nrequires changing theROM hardware chips. Some systems resolve this problem\nby using erasable programmable read-only memory (EPROM),w h i c hi sr e a d -\nonly except when explicitly given a command to become writable. All forms\nof ROM are also known as \ufb01rmware,s i n c et h e i rc h a r a c t e r i s t i c sf a l ls o m e w h e r e\nbetween those of hardware and those of software. A problem with \ufb01rmware\nin general is that executing code the re is slower than executing code in RAM.\nSome systems store the operating system in \ufb01rmware and copy it to RAM for\nfast execution. A \ufb01nal issue with \ufb01rmware is that it is relatively expensive, so\nusually only small amounts are available.\nFor large operating systems (including most general-purpose operating\nsystems like Windows, Mac OS X ,a n d UNIX)o rf o rs y s t e m st h a tc h a n g e\nfrequently, the bootstrap loader is stored in \ufb01rmware, and the operating system\nis on disk. In this case, the bootstrap runs diagnostics and has a bit of code\nthat can read a single block at a \ufb01xed location (say block zero) from disk into\nmemory and execute the code from thatboot block. The program stored in the\nboot block may be sophisticated enough to load the entire operating system\ninto memory and begin its execution. More typically, it is simple code (as it \ufb01ts\nin a single disk block) and knows only the address on disk and length of the\nremainder of the bootstrap program. GRUB is an example of an open-source\nbootstrap program for Linux systems. All of the disk-bound bootstrap, and the\noperating system itself, can be easily changed by writing new versions to disk.\nA disk that has a boot partition (more on that in Section 10.5.1) is called aboot\ndisk or system disk.\nNow that the full bootstrap program has been loaded, it can traverse the\n\ufb01le system to \ufb01nd the operating system kernel, load it into memory, and start\nits execution. It is only at this point that the system is said to be running.\n2.11 Summary\nOperating systems provide a number of services. At the lowest level, system\ncalls allow a running program to make requests from the operating system\ndirectly. At a higher level, the comman di n t e r p r e t e ro rs h e l lp r o v i d e sa\nmechanism for a user to issue a request without writing a program. Commands\nmay come from \ufb01les during batch-mode execution or directly from a terminal\nor desktop GUI when in an interactive or time-shared mode. System programs\nare provided to satisfy many common user requests.94 Chapter 2 Operating-System Structures\nThe types of requests vary according to level. The system-call level must\nprovide the basic functions, such as process control and \ufb01le and device\nmanipulation. Higher-level requests, satis\ufb01 ed by the command interpreter or\nsystem programs, are translated into a sequence of system calls. System services\ncan be classi\ufb01ed into several categories: program control, status requests, and\nI/O requests. Program errors can be considered implicit requests for service.\nThe design of a new operating system is a major task. It is important that\nthe goals of the system be well de\ufb01ned before the design begins. The type of\nsystem desired is the foundation for choices among various algorithms and\nstrategies that will be needed.\nThroughout the entire design cycle, we must be careful to separate policy\ndecisions from implementation details (mechanisms). This separation allows\nmaximum \ufb02exibility if policy decisions are to be changed later.\nOnce an operating system is designed, it must be implemented. Oper-\nating systems today are almost always written in a systems-implementation\nlanguage or in a higher-level language. This feature improves their implemen-\ntation, maintenance, and portability.\nAs y s t e ma sl a r g ea n dc o m p l e xa sam o d e r no p e r a t i n gs y s t e mm u s t\nbe engineered carefully. Modularity is important. Designing a system as a\nsequence of layers or using a microkernel is considered a good technique. Many\noperating systems now support dynamically loaded modules, which allow\nadding functionality to an operating system while it is executing. Generally,\noperating systems adopt a hybrid approach that combines several different\ntypes of structures.\nDebugging process and kernel failures can be accomplished through the\nuse of debuggers and other tools that analyze core dumps. Tools such asDTrace\nanalyze production systems to \ufb01nd bottlenecks and understand other system\nbehavior.\nTo create an operating system for a particular machine con\ufb01guration, we\nmust perform system generation. For the computer system to begin running,\nthe CPU must initialize and start executing the bootstrap program in \ufb01rmware.\nThe bootstrap can execute the operating system directly if the operating system\nis also in the \ufb01rmware, or it can complete a sequence in which it loads\nprogressively smarter programs from \ufb01rmware and disk until the operating\nsystem itself is loaded into memory and executed.\nPractice Exercises\n2.1 What is the purpose of system calls?\n2.2 What are the \ufb01ve major activities of an operating system with regard to\nprocess management?\n2.3 What are the three major activities of an operating system with regard\nto memory management?\n2.4 What are the three major activities of an operating system with regard\nto secondary-storage management?\n2.5 What is the purpose of the command interpreter? Why is it usually\nseparate from the kernel?", "Exercises 95\n2.6 What system calls have to be executed by a command interpreter or shell\nin order to start a new process?\n2.7 What is the purpose of system programs?\n2.8 What is the main advantage of the layered approach to system design?\nWhat are the disadvantages of the layered approach?\n2.9 List \ufb01ve services provided by an operating system, and explain how each\ncreates convenience for users. In which cases would it be impossible for\nuser-level programs to provide these services? Explain your answer.\n2.10 Why do some systems store the operating system in \ufb01rmware, while\nothers store it on disk?\n2.11 How could a system be designed to allow a choice of operating systems\nfrom which to boot? What would the bootstrap program need to do?\nExercises\n2.12 The services and functions provided by an operating system can be\ndivided into two main categories. Brie\ufb02y describe the two categories,\nand discuss how they differ.\n2.13 Describe three general methods for passing parameters to the operating\nsystem.\n2.14 Describe how you could obtain a statistical pro\ufb01le of the amount of time\nspent by a program executing different sections of its code. Discuss the\nimportance of obtaining such a statistical pro\ufb01le.\n2.15 What are the \ufb01ve major activities of an operating system with regard to\n\ufb01le management?\n2.16 What are the advantages and disadvantages of using the same system-\ncall interface for manipulating both \ufb01les and devices?\n2.17 Would it be possible for the user to develop a new command interpreter\nusing the system-call interface provided by the operating system?\n2.18 What are the two models of interprocess communication? What are the\nstrengths and weaknesses of the two approaches?\n2.19 Why is the separation of mechanism and policy desirable?\n2.20 It is sometimes dif\ufb01cult to achieve alayered approach if two components\nof the operating system are dependent on each other. Identify a scenario\nin which it is unclear how to layer two system components that require\ntight coupling of their functionalities.\n2.21 What is the main advantage of the microkernel approach to system\ndesign? How do user programs and system services interact in a\nmicrokernel architecture? What are the disadvantages of using the\nmicrokernel approach?\n2.22 What are the advantages of using loadable kernel modules?96 Chapter 2 Operating-System Structures\n2.23 How are iOS and Android similar? How are they different?\n2.24 Explain why Java programs running on Android systems do not use the\nstandard Java API and virtual machine.\n2.25 The experimental Synthesis operating s ystem has an assembler incor-\nporated in the kernel. To optimize system-call performance, the kernel\nassembles routines within kernel space to minimize the path that the\nsystem call must take through the kernel. This approach is the antithesis\nof the layered approach, in which thepath through the kernel is extended\nto make building the operating system easier. Discuss the pros and cons\nof the Synthesis approach to kernel design and system-performance\noptimization.\nProgramming Problems\n2.26 In Section 2.3, we described a program that copies the contents of one \ufb01le\nto a destination \ufb01le. This program works by \ufb01rst prompting the user for\nthe name of the source and destination \ufb01les. Write this program using\neither the Windows or POSIX API. Be sure to include all necessary error\nchecking, including ensuring that the source \ufb01le exists.\nOnce you have correctly designed and tested the program, if you\nused a system that supports it, run the program using a utility that traces\nsystem calls. Linux systems provide the strace utility, and Solaris and\nMac OS X systems use the dtrace command. As Windows systems do\nnot provide such features, you will have to trace through the Windows\nversion of this program using a debugger.\nProgramming Projects\nLinux Kernel Modules\nIn this project, you will learn how to create a kernel module and load it into the\nLinux kernel. The project can be completed using the Linux virtual machine\nthat is available with this text. Although you may use an editor to write these\nCp r o g r a m s ,y o uw i l lh a v et ou s et h eterminal application to compile the\nprograms, and you will have to enter commands on the command line to\nmanage the modules in the kernel.\nAs you\u2019ll discover, the advantage of developing kernel modules is that it\nis a relatively easy method of interacting witht h ek e r n e l ,t h u sa l l o w i n gy o ut o\nwrite programs that directly invoke kernel functions. It is important for you\nto keep in mind that you are indeed writing kernel code that directly interacts\nwith the kernel. That normally means that any errors in the code could crash\nthe system! However, since you will be using a virtual machine, any failures\nwill at worst only require rebooting the system.\nPart I\u2014Creating Kernel Modules\nThe \ufb01rst part of this project involves following a series of steps for creating and\ninserting a module into the Linux kernel.Programming Projects 97\nYou can list all kernel modules that are currently loaded by entering the\ncommand\nlsmod\nThis command will list the current kernel modules in three columns: name,\nsize, and where the module is being used.\nThe following program (named simple.c and available with the source\ncode for this text) illustrates a very basic kernel module that prints appropriate\nmessages when the kernel module is loaded and unloaded.\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n/* This function is called when the module is loaded. */\nint simple\n init(void)\n{\nprintk(KERN\n INFO \"Loading Module\\n\");\nreturn 0;\n}\n/* This function is called when the module is removed. */\nvoid simple\n exit(void)\n{\nprintk(KERN\n INFO \"Removing Module\\n\");\n}\n/* Macros for registering module entry and exit points. */\nmodule\n init(simple\n init);\nmodule\n exit(simple\n exit);\nMODULE\n LICENSE(\"GPL\");\nMODULE\n DESCRIPTION(\"Simple Module\");\nMODULE\n AUTHOR(\"SGG\");\nThe function simple\n init() is the module entry point,w h i c hr e p r e s e n t s\nthe function that is invoked when the mo dule is loaded into the kernel.\nSimilarly, thesimple\n exit() function is the module exit point\u2014the function\nthat is called when the module is removed from the kernel.\nThe module entry point function m ust return an integer value, with 0\nrepresenting success and any other value representing failure. The module exit\npoint function returns void.N e i t h e rt h em o d u l ee n t r yp o i n tn o rt h em o d u l e\nexit point is passed any parameters. The two following macros are used for\nregistering the module entry and exit points with the kernel:\nmodule\n init()\nmodule\n exit()", "98 Chapter 2 Operating-System Structures\nNotice how both the module entry and exit point functions make calls\nto the printk() function. printk() is the kernel equivalent of printf(),\nyet its output is sent to a kernel log buffer whose contents can be read by\nthe dmesg command. One difference between printf() and printk() is that\nprintk() allows us to specify a priority \ufb02ag whose values are given in the\n<linux/printk.h> include \ufb01le. In this instance, the priority is KERN\n INFO,\nwhich is de\ufb01ned as an informational message.\nThe \ufb01nal lines\u2014 MODULE\n LICENSE(), MODULE\n DESCRIPTION(),a n d MOD-\nULE\n AUTHOR()\u2014represent details regarding the software license, description\nof the module, and author. For our purposes, we do not depend on this\ninformation, but we include it because it is standard practice in developing\nkernel modules.\nThis kernel module simple.c is compiled using the Makefile accom-\npanying the source code with this project. To compile the module, enter the\nfollowing on the command line:\nmake\nThe compilation produces several \ufb01les. The \ufb01le simple.ko represents the\ncompiled kernel module. The following step illustrates inserting this module\ninto the Linux kernel.\nLoading and Removing Kernel Modules\nKernel modules are loaded using theinsmod command, which is run as follows:\nsudo insmod simple.ko\nTo check whether the module has loaded, enter thelsmod command and search\nfor the module simple. Recall that the module entry point is invoked when\nthe module is inserted into the kernel. To check the contents of this message in\nthe kernel log buffer, enter the command\ndmesg\nYou should see the message \"Loading Module.\"\nRemoving the kernel module involves invoking the rmmod command\n(notice that the .ko suf\ufb01x is unnecessary):\nsudo rmmod simple\nBe sure to check with the dmesg command to ensure the module has been\nremoved.\nBecause the kernel log buffer can \ufb01ll up quickly, it often makes sense to\nclear the buffer periodically. This can be accomplished as follows:\nsudo dmesg -cProgramming Projects 99\nPart I Assignment\nProceed through the steps described above to create the kernel module and to\nload and unload the module. Be sure to che ck the contents of the kernel log\nbuffer using dmesg to ensure you have properly followed the steps.\nPart II\u2014Kernel Data Structures\nThe second part of this projec t involves modifying the kernel module so that\nit uses the kernel linked-list data structure.\nIn Section 1.10, we covered various data structures that are common in\noperating systems. The Linux kernel provides several of these structures. Here,\nwe explore using the circular, doubly linked list that is available to kernel\ndevelopers. Much of what we discuss is available in the Linux source code\u2014\nin this instance, the include \ufb01le <linux/list.h>\u2014and we recommend that\nyou examine this \ufb01le as you proceed through the following steps.\nInitially, you must de\ufb01ne a struct containing the elements that are to be\ninserted in the linked list. The following C struct de\ufb01nes birthdays:\nstruct birthday {\nint day;\nint month;\nint year;\nstruct list\n head list;\n}\nNotice the member struct list\n head list.T h e list\n head structure is\nde\ufb01ned in the include \ufb01le <linux/types.h>.I t si n t e n t i o ni st oe m b e dt h e\nlinked list within the nodes that comprise the list. This list\n head structure is\nquite simple\u2014it merely holds two members, next and prev,t h a tp o i n tt ot h e\nnext and previous entries in the list. By embedding the linked list within the\nstructure, Linux makes it possible to manage the data structure with a series of\nmacro functions.\nInserting Elements into the Linked List\nWe can declare a list\n head object, which we use as a reference to the head of\nthe list by using the LIST\n HEAD() macro\nstatic LIST\n HEAD(birthday\n list);\nThis macro de\ufb01nes and initializes the variablebirthday\n list,w h i c hi so ft y p e\nstruct list\n head.100 Chapter 2 Operating-System Structures\nWe create and initialize instances of struct birthday as follows:\nstruct birthday *person;\nperson = kmalloc(sizeof(*person), GFP\n KERNEL);\nperson->day = 2;\nperson->month= 8;\nperson->year = 1995;\nINIT\n LIST\n HEAD(&person->list);\nThe kmalloc() function is the kernel equivalent of the user-level malloc()\nfunction for allocating memory, except that kernel memory is being allocated.\n(The GFP\n KERNEL \ufb02ag indicates routine kernel memory allocation.) The macro\nINIT\n LIST\n HEAD() initializes the list member in struct birthday.W ec a n\nthen add this instance to the end of the linked list using the list\n add\n tail()\nmacro:\nlist\n add\n tail(&person->list, &birthday\n list);\nTraversing the Linked List\nTraversing the list involves using the list\n for\n each\n entry() Macro, which\naccepts three parameters:\n\u2022 Ap o i n t e rt ot h es t r u c t u r eb e i n gi t e r a t e do v e r\n\u2022 Ap o i n t e rt ot h eh e a do ft h el i s tb e i n gi t e r a t e do v e r\n\u2022 The name of the variable containing the list\n head structure\nThe following code illustrates this macro:\nstruct birthday *ptr;\nlist\n for\n each\n entry(ptr, &birthday\n list, list) {\n/* on each iteration ptr points */\n/* to the next birthday struct */\n}\nRemoving Elements from the Linked List\nRemoving elements from the list involves using thelist\n del() macro, which\nis passed a pointer to struct list\n head\nlist\n del(struct list\n head *element)\nThis removes element from the list while maintaining the structure of the\nremainder of the list.\nPerhaps the simplest approach for removing all elements from a\nlinked list is to remove each element as you traverse the list. The macro\nlist\n for\n each\n entry\n safe() behaves much like list\n for\n each\n entry()", "Bibliographical Notes 101\nexcept that it is passed an additional argument that maintains the value of the\nnext pointer of the item being deleted. (This is necessary for preserving the\nstructure of the list.) The following code example illustrates this macro:\nstruct birthday *ptr, *next\nlist\n for\n each\n entry\n safe(ptr,next,&birthday\n list,list) {\n/* on each iteration ptr points */\n/* to the next birthday struct */\nlist\n del(&ptr->list);\nkfree(ptr);\n}\nNotice that after deleting each element, we return memory that was previously\nallocated with kmalloc() back to the kernel with the call to kfree(). Careful\nmemory management\u2014which includes releasing memory to prevent memory\nleaks\u2014is crucial when developing kernel-level code.\nPart II Assignment\nIn the module entry point, create a linked list containing \ufb01vestruct birthday\nelements. Traverse the linked list and output its contents to the kernel log buffer.\nInvoke the dmesg command to ensure the list is properly constructed once the\nkernel module has been loaded.\nIn the module exit point, delete the elements from the linked list and return\nthe free memory back to the kernel. Again, invoke thedmesg command to check\nthat the list has been removed once the kernel module has been unloaded.\nBibliographical Notes\n[Dijkstra (1968)] advocated the layered approach to operating-system design.\n[Brinch-Hansen (1970)] was an early proponent of constructing an operating\nsystem as a kernel (or nucleus) on which more complete systems could be\nbuilt. [Tarkoma and Lagerspetz (2011)] provide an overview of various mobile\noperating systems, including Android and iOS.\nMS-DOS, Version 3.1, is described in [Microsoft (1986)]. Windows NT\nand Windows 2000 are described by [Solomon (1998)] and [Solomon and\nRussinovich (2000)]. Windows XP internals are described in [Russinovich\nand Solomon (2009)]. [Hart (2005)] covers Windows systems programming\nin detail. BSD UNIX is described in [McKusick et al. (1996)]. [Love (2010)] and\n[Mauerer (2008)] thoroughly discuss the Linux kernel. In particular, [Love\n(2010)] covers Linux kernel modules as well as kernel data structures. Several\nUNIX systems\u2014including Mach\u2014are treated in detail in [Vahalia (1996)]. Mac\nOS X is presented at http://www.apple.com/macosx and in [Singh (2007)].\nSolaris is fully described in [McDougall and Mauro (2007)].\nDTrace is discussed in [Gregg and Mauro (2011)]. The DTrace source code\nis available at http://src.opensolaris.org/source/.102 Chapter 2 Operating-System Structures\nBibliography\n[Brinch-Hansen (1970)] P. B r i n c h - H a n s e n ,\u201cThe Nucleus of a Multiprogram-\nming System\u201d, Communications of the ACM,V o l u m e1 3 ,N u m b e r4( 1 9 7 0 ) ,p a g e s\n238\u2013241 and 250.\n[Dijkstra (1968)] E. W. Dijkstra, \u201cThe Structure of the THE Multiprogramming\nSystem\u201d, Communications of the ACM,V o l u m e1 1 ,N u m b e r5( 1 9 6 8 ) ,p a g e s\n341\u2013346.\n[Gregg and Mauro (2011)] B. Gregg and J. Mauro, DTrace\u2014Dynamic Tracing in\nOracle Solaris, Mac OS X, and FreeBSD,P r e n t i c eH a l l( 2 0 1 1 ) .\n[Hart (2005)] J. M. Hart, Windows System Programming,Third Edition, Addison-\nWesley (2005).\n[Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developer\u2019s\nLibrary (2010).\n[Mauerer (2008)] W. Mauerer, Professional Linux Kernel Architecture,J o h nW i l e y\nand Sons (2008).\n[McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[McKusick et al. (1996)] M. K. McKusick, K. Bostic, and M. J. Karels,The Design\nand Implementation of the 4.4 BSD UNIX Operating System,J o h nW i l e ya n dS o n s\n(1996).\n[Microsoft (1986)] Microsoft MS-DOS User\u2019s Reference and Microsoft MS-DOS\nProgrammer\u2019s Reference.M i c r o s o f tP r e s s( 1 9 8 6 ) .\n[Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon,Win-\ndows Internals: Including Windows Server 2008 and Windows Vista,Fifth Edition,\nMicrosoft Press (2009).\n[Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach,A d d i s o n -\nWesley (2007).\n[Solomon (1998)] D. A. Solomon, Inside Windows NT,Second Edition, Microsoft\nPress (1998).\n[Solomon and Russinovich (2000)] D. A. Solomon and M. E. Russinovich,Inside\nMicrosoft Windows 2000,Third Edition, Microsoft Press (2000).\n[Tarkoma and Lagerspetz (2011)] S. Tarkoma and E. Lagerspetz,\u201cArching over\nthe Mobile Computing Chasm: Platforms and Runtimes \u201d, IEEE Computer,\nVolume 44, (2011), pages 22\u201328.\n[Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers,P r e n t i c eH a l l\n(1996).Part Two\nProcess\nManagement\nA process can be thought of as a program in execution. A process will\nneed certain resources\u2014such asCPU time, memory, \ufb01les, andI/O devices\n\u2014to accomplish its task. These resources are allocated to the process\neither when it is created or while it is executing.\nAp r o c e s si st h eu n i to fw o r ki nm o s ts y s t e m s .S y s t e m sc o n s i s to f\nac o l l e c t i o no fp r o c e s s e s :o p e r a t i n g - s y s t e mp r o c e s s e se x e c u t es y s t e m\ncode, and user processes execute usercode. All these processes may\nexecute concurrently.\nAlthough traditionally a process contained only a singlethread of\ncontrol as it ran, most modern operating systems now support processes\nthat have multiple threads.\nThe operating system is responsible for several important aspects of\nprocess and thread management: the creation and deletion of both user\nand system processes; the scheduling of processes; and the provision of\nmechanisms for synchronization, communication, and deadlock handling\nfor processes.", "3\nCHAPTER\nProcesses\nEarly computers allowed only one program to be executed at a time. This\nprogram had complete control of the system and had access to all the system\u2019s\nresources. In contrast, contemporary computer systems allow multiple pro-\ngrams to be loaded into memory and executed concurrently. This evolution\nrequired \ufb01rmer control and more compartmentalization of the various pro-\ngrams; and these needs resulted in the notion of aprocess,w h i c hi sap r o g r a m\nin execution. A process is the unit of work in a modern time-sharing system.\nThe more complex the operating system is, the more it is expected to do on\nbehalf of its users. Although its main concern is the execution of user programs,\nit also needs to take care of various system tasks that are better left outside the\nkernel itself. A system therefore consists of a collection of processes: operating-\nsystem processes executing system code and user processes executing user\ncode. Potentially, all these processes cane x e c u t ec o n c u r r e n t l y ,w i t ht h eCPU (or\nCPUs) multiplexed among them. By switching the CPU between processes, the\noperating system can make the computer more productive. In this chapter, you\nwill read about what processes are and how they work.\nCHAPTER OBJECTIVES\n\u2022 To introduce the notion of a process \u2014 a program in execution, which forms\nthe basis of all computation.\n\u2022 To describe the various features of processes, including scheduling,\ncreation, and termination.\n\u2022 To explore interprocess communication using shared memory and mes-\nsage passing.\n\u2022 To describe communication in client \u2013 server systems.\n3.1 Process Concept\nAq u e s t i o nt h a ta r i s e si nd i s c u s s i n go p e r a t i n gs y s t e m si n v o l v e sw h a tt oc a l l\nall the CPU activities. A batch system executes jobs,w h e r e a sat i m e - s h a r e d\n105106 Chapter 3 Processes\nsystem has user programs,o r tasks.E v e no nas i n g l e - u s e rs y s t e m ,au s e rm a y\nbe able to run several programs at one time: a word processor, a Web browser,\nand an e-mail package. And even if a user can execute only one program at a\ntime, such as on an embedded device that does not support multitasking, the\noperating system may need to support its own internal programmed activities,\nsuch as memory management. In many respects, all these activities are similar,\nso we call all of them processes.\nThe terms job and process are used almost interchangeably in this text.\nAlthough we personally prefer the term process,m u c ho fo p e r a t i n g - s y s t e m\ntheory and terminology was developed during a time when the major activity\nof operating systems was job processing. It would be misleading to avoid\nthe use of commonly accepted terms that include the word job (such as job\nscheduling)s i m p l yb e c a u s eprocess has superseded job.\n3.1.1 The Process\nInformally, as mentioned earlier, a process is a program in execution. A process\nis more than the program code, which is sometimes known as thetext section.\nIt also includes the current activity, as represented by the value of theprogram\ncounter and the contents of the processor\u2019s registers. A process generally also\nincludes the process stack,w h i c hc o n t a i n st e m p o r a r yd a t a( s u c ha sf u n c t i o n\nparameters, return addresses, and local variables), and a data section,w h i c h\ncontains global variables. A process may also include aheap,w h i c hi sm e m o r y\nthat is dynamically allocated during process run time. The structure of a process\nin memory is shown in Figure 3.1.\nWe emphasize that a program by itself is not a process. A program is a\npassive entity, such as a \ufb01le containing a list of instructions stored on disk\n(often called an executable \ufb01le ). In contrast, a process is an active entity,\nwith a program counter specifying the next instruction to execute and a set\nof associated resources. A program becomes a process when an executable \ufb01le\nis loaded into memory. Two common tec hniques for loading executable \ufb01les\ntext\n0\nmax\ndata\nheap\nstack\nFigure 3.1 Process in memory.", "3.1 Process Concept 107\nare double-clicking an icon representing the executable \ufb01le and entering the\nname of the executable \ufb01le on the command line (as in prog.exe or a.out).\nAlthough two processes may be associated with the same program, they\nare nevertheless considered two separate exec ution sequences. For instance,\nseveral users may be running different copies of the mail program, or the same\nuser may invoke many copies of the web browser program. Each of these is a\nseparate process; and although the text sections are equivalent, the data, heap,\nand stack sections vary. It is also common to have a process that spawns many\nprocesses as it runs. We discuss such matters in Section 3.4.\nNote that a process itself can be an execution environment for other\ncode. The Java programming environment provides a good example. In most\ncircumstances, an executable Java program is executed within the Java virtual\nmachine (JVM). The JVM executes as a process that interprets the loaded Java\ncode and takes actions (via native machine instructions) on behalf of that code.\nFor example, to run the compiled Java program Program.class, we would\nenter\njava Program\nThe command java runs the JVM as an ordinary process, which in turns\nexecutes the Java program Programin the virtual machine. The concept is the\nsame as simulation, except that the code, instead of being written for a different\ninstruction set, is written in the Java language.\n3.1.2 Process State\nAs a process executes, it changes state.T h es t a t eo fap r o c e s si sd e \ufb01 n e di np a r t\nby the current activity of that process. A process may be in one of the following\nstates:\n\u2022 New.T h ep r o c e s si sb e i n gc r e a t e d .\n\u2022 Running.I n s t r u c t i o n sa r eb e i n ge x e c u t e d .\n\u2022 Waiting.T h ep r o c e s si sw a i t i n gf o rs o m ee v e n tt oo c c u r( s u c ha sa nI/O\ncompletion or reception of a signal).\n\u2022 Ready. The process is waiting to be assigned to a processor.\n\u2022 Terminated.T h ep r o c e s sh a s\ufb01 n i s h e de x e c u t i o n .\nThese names are arbitrary, and they vary across operating systems. The states\nthat they represent are found on all systems, however. Certain operating\nsystems also more \ufb01nely delineate process states. It is important to realize\nthat only one process can be running on any processor at any instant. Many\nprocesses may beready and waiting, however. The state diagram corresponding\nto these states is presented in Figure 3.2.\n3.1.3 Process Control Block\nEach process is represented in the operating system by aprocess control block\n(PCB)\u2014alsocalleda task control block.A PCB is shown in Figure 3.3. It contains\nmany pieces of information associated with a speci\ufb01c process, including these:108 Chapter 3 Processes\nnew terminated\nrunningready\nadmitted interrupt\nscheduler dispatchI/O or event completion I/O or event wait\nexit\nwaiting\nFigure 3.2 Diagram of process state.\n\u2022 Process state.T h es t a t em a yb en e w ,r e a d y ,r u n n i n g ,w a i t i n g ,h a l t e d ,a n d\nso on.\n\u2022 Program counter.T h ec o u n t e ri n d i c a t e st h ea d d r e s so ft h en e x ti n s t r u c t i o n\nto be executed for this process.\n\u2022 CPU registers.T h er e g i s t e r sv a r yi nn u m b e ra n dt y p e ,d e p e n d i n go n\nthe computer architecture. They include accumulators, index registers,\nstack pointers, and general-purpose registers, plus any condition-code\ninformation. Along with the program counter, this state information must\nbe saved when an interrupt occurs, to allow the process to be continued\ncorrectly afterward (Figure 3.4).\n\u2022 CPU-scheduling information.T h i si n f o r m a t i o ni n c l u d e sap r o c e s sp r i o r i t y ,\npointers to scheduling queues, and any other scheduling parameters.\n(Chapter 6 describes process scheduling.)\n\u2022 Memory-management information.T h i si n f o r m a t i o nm a yi n c l u d es u c h\nitems as the value of the base and limit registers and the page tables, or the\nsegment tables, depending on the memory system used by the operating\nsystem (Chapter 8).\nprocess state\nprocess number\nprogram counter\nmemory limits\nlist of open files\nregisters\n\u2022\n\u2022\n\u2022\nFigure 3.3 Process control block (PCB).3.1 Process Concept 109\nprocess P0 process P1\nsave state into PCB0\nsave state into PCB1\nreload state from PCB1\nreload state from PCB0\noperating system\nidle\nidle\nexecutingidle\nexecuting\nexecuting\ninterrupt or system call\ninterrupt or system call\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nFigure 3.4 Diagram showing CPU switch from process to process.\n\u2022 Accounting information .T h i si n f o r m a t i o ni n c l u d e st h ea m o u n to fCPU\nand real time used, time limits, account numbers, job or process numbers,\nand so on.\n\u2022 I/O status information.T h i si n f o r m a t i o ni n c l u d e st h el i s to fI/O devices\nallocated to the process, a list of open \ufb01les, and so on.\nIn brief, the PCB simply serves as the repository for any information that may\nvary from process to process.\n3.1.4 Threads\nThe process model discussed so far has implied that a process is a program that\nperforms a single thread of execution. For example, when a process is running\naw o r d - p r o c e s s o rp r o g r a m ,as i n g l et h r e a do fi n s t r u c t i o n si sb e i n ge x e c u t e d .\nThis single thread of control allows the process to perform only one task at\nat i m e .T h eu s e rc a n n o ts i m u l t a n e o u s l yt y p ei nc h a r a c t e r sa n dr u nt h es p e l l\nchecker within the same process, for example. Most modern operating systems\nhave extended the process concept toallow a process to have multiple threads\nof execution and thus to perform more than one task at a time. This feature\nis especially bene\ufb01cial on multicore systems, where multiple threads can run\nin parallel. On a system that supports threads, the PCB is expanded to include\ninformation for each thread. Other changes throughout the system are also\nneeded to support threads. Chapter 4 explores threads in detail.", "110 Chapter 3 Processes\nPROCESS REPRESENTATION IN LINUX\nThe process control block in the Linux operating system is represented by\nthe C structure task\n struct,w h i c hi sf o u n di nt h e<linux/sched.h>\ninclude \ufb01le in the kernel source-code directory. This structure contains all the\nnecessary information for representing a process, including the state of the\nprocess, scheduling and memory-management information, list of open \ufb01les,\nand pointers to the process\u2019s parent and a list of its children and siblings. (A\nprocess\u2019sparent is the process that created it; its children are any processes\nthat it creates. Its siblings are children with the same parent process.) Some\nof these \ufb01elds include:\nlong state; /* state of the process */\nstruct sched\n entity se; /* scheduling information */\nstruct task\n struct *parent; /* this process\u2019s parent */\nstruct list\n head children; /* this process\u2019s children */\nstruct files\n struct *files; /* list of open files */\nstruct mm\n struct *mm; /* address space of this process */\nFor example, the state of a process is represented by the \ufb01eld long state\nin this structure. Within the Linux kernel, all active processes are represented\nusing a doubly linked list oftask\n struct.T h ek e r n e lm a i n t a i n sap o i n t e r\u2014\ncurrent\u2014totheprocesscurrentlyexecutingonthesystem,asshownbelow:\nstruct task_struct\nprocess information\n\u2022 \n\u2022 \n\u2022\nstruct task_struct\nprocess information\n\u2022\n\u2022\n\u2022\ncurrent\n(currently executing proccess)\nstruct task_struct\nprocess information\n\u2022\n\u2022\n\u2022\n\u2022  \u2022  \u2022\nAs an illustration of how the kernel might manipulate one of the \ufb01elds in\nthe task\n struct for a speci\ufb01ed process, let\u2019s assume the system would like\nto change the state of the process currently running to the value new\n state.\nIf current is a pointer to the process currently executing, its state is changed\nwith the following:\ncurrent->state = new\n state;\n3.2 Process Scheduling\nThe objective of multiprogramming is to have some process running at all\ntimes, to maximizeCPU utilization. The objective of time sharing is to switch the\nCPU among processes so frequently that users can interact with each program3.2 Process Scheduling 111\nqueue header PCB 7\nPCB3\nPCB5\nPCB14 PCB6\nPCB2\nhead\nhead\nhead\nhead\nhead\nready\nqueue\ndisk \nunit 0\nterminal \nunit 0\nmag\ntape\nunit 0\nmag\ntape\nunit 1\ntail registers registers\ntail\ntail\ntail\ntail\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nFigure 3.5 The ready queue and various I/O device queues.\nwhile it is running. To meet these objectives, the process scheduler selects\nan available process (possibly from a set of several available processes) for\nprogram execution on the CPU.F o ras i n g l e - p r o c e s s o rs y s t e m ,t h e r ew i l ln e v e r\nbe more than one running process. If there are more processes, the rest will\nhave to wait until the CPU is free and can be rescheduled.\n3.2.1 Scheduling Queues\nAs processes enter the system, they are put into a job queue,w h i c hc o n s i s t s\nof all processes in the system. The processes that are residing in main memory\nand are ready and waiting to execute are kept on a list called theready queue.\nThis queue is generally stored as a linked list. A ready-queue header contains\npointers to the \ufb01rst and \ufb01nal PCBsi nt h el i s t .E a c hPCB includes a pointer \ufb01eld\nthat points to the next PCB in the ready queue.\nThe system also includes other queues. When a process is allocated the\nCPU, it executes for a while and eventually quits, is interrupted, or waits for\nthe occurrence of a particular event, such as the completion of an I/O request.\nSuppose the process makes an I/O request to a shared device, such as a disk.\nSince there are many processes in the system, the disk may be busy with the\nI/O request of some other process. The process therefore may have to wait for\nthe disk. The list of processes waiting for a particular I/O device is called a\ndevice queue.E a c hd e v i c eh a si t so w nd e v i c eq u e u e( F i g u r e3 . 5 ) .112 Chapter 3 Processes\nready queue CPU\nI/O I/O queue I/O request\ntime slice\nexpired\nfork a\nchild\nwait for an\ninterrupt\ninterrupt\noccurs\nchild\nexecutes\nFigure 3.6 Queueing-diagram representation of process scheduling.\nAc o m m o nr e p r e s e n t a t i o no fp r o c e s ss c h e d u l i n gi saqueueing diagram,\nsuch as that in Figure 3.6. Each rectangular box represents a queue. Two types\nof queues are present: the ready queue and a set of device queues. The circles\nrepresent the resources that serve the queues, and the arrows indicate the \ufb02ow\nof processes in the system.\nAn e wp r o c e s si si n i t i a l l yp u ti nt h er e a d yq u e u e .I tw a i t st h e r eu n t i li ti s\nselected for execution, or dispatched. Once the process is allocated the CPU\nand is executing, one of several events could occur:\n\u2022 The process could issue anI/O request and then be placed in anI/O queue.\n\u2022 The process could create a new child process and wait for the child\u2019s\ntermination.\n\u2022 The process could be removed forcibly from the CPU,a sar e s u l to fa n\ninterrupt, and be put back in the ready queue.\nIn the \ufb01rst two cases, the processeventually switches from the waiting state\nto the ready state and is then put back in the ready queue. A process continues\nthis cycle until it terminates, at which time it is removed from all queues and\nhas its PCB and resources deallocated.\n3.2.2 Schedulers\nA process migrates among the various scheduling queues throughout its\nlifetime. The operating system must select, for scheduling purposes, processes\nfrom these queues in some fashion. The selection process is carried out by the\nappropriate scheduler.\nOften, in a batch system, more processes are submitted than can be executed\nimmediately. These processes are spooled to a mass-storage device (typically a\ndisk), where they are kept for later execution. Thelong-term scheduler,o r job\nscheduler,s e l e c t sp r o c e s s e sf r o mt h i sp o o la n dl o a d st h e mi n t om e m o r yf o r", "3.2 Process Scheduling 113\nexecution. The short-term scheduler,o r CPU scheduler, selects from among\nthe processes that are ready to execute and allocates the CPU to one of them.\nThe primary distinction between these two schedulers lies in frequency\nof execution. The short-term scheduler must select a new process for the CPU\nfrequently. A process may execute for only a few milliseconds before waiting\nfor an I/O request. Often, the short-term scheduler executes at least once every\n100 milliseconds. Because of the short time between executions, the short-term\nscheduler must be fast. If it takes 10 milliseconds to decide to execute a process\nfor 100 milliseconds, then 10/(100 + 10) = 9 percent of the CPU is being used\n(wasted) simply for scheduling the work.\nThe long-term scheduler executes much less frequently; minutes may sep-\narate the creation of one new process and the next. The long-term scheduler\ncontrols the degree of multiprogramming (the number of processes in mem-\nory). If the degree of multiprogramming is stable, then the average rate of\nprocess creation must be equal to the average departure rate of processes\nleaving the system. Thus, the long-term scheduler may need to be invoked\nonly when a process leaves the system. Because of the longer interval between\nexecutions, the long-term scheduler can a fford to take more time to decide\nwhich process should be selected for execution.\nIt is important that the long-term scheduler make a careful selection. In\ngeneral, most processes can be described as either I/O bound or CPU bound.\nAn I/O-bound process is one that spends more of its time doing I/O than\nit spends doing computations. A CPU-bound process,i nc o n t r a s t ,g e n e r a t e s\nI/O requests infrequently, using more of its time doing computations. It is\nimportant that the long-term scheduler select a goodprocess mix of I/O-bound\nand CPU-bound processes. If all processes areI/O bound, the ready queue will\nalmost always be empty, and the short-term scheduler will have little to do.\nIf all processes are CPU bound, the I/O waiting queue will almost always be\nempty, devices will go unused, and again the system will be unbalanced. The\nsystem with the best performance will thus have a combination of CPU-bound\nand I/O-bound processes.\nOn some systems, the long-term scheduler may be absent or minimal.\nFor example, time-sharing systems such as UNIX and Microsoft Windows\nsystems often have no long-term scheduler but simply put every new process in\nmemory for the short-term scheduler. The stability of these systems depends\neither on a physical limitation (such as the number of available terminals)\nor on the self-adjusting nature of human users. If performance declines to\nunacceptable levels on a multiuser system, some users will simply quit.\nSome operating systems, such as time-sharing systems, may introduce an\nadditional, intermediate level of scheduling. This medium-term scheduler is\ndiagrammed in Figure 3.7. The key idea behind a medium-term scheduler is\nthat sometimes it can be advantageous to remove a process from memory\n(and from active contention for the CPU)a n dt h u sr e d u c et h ed e g r e eo f\nmultiprogramming. Later, the process can be reintroduced into memory, and its\nexecution can be continued where it left off. This scheme is called swapping.\nThe process is swapped out, and is later swapped in, by the medium-term\nscheduler. Swapping may be necessary to improve the process mix or because\na change in memory requirements has overcommitted available memory,\nrequiring memory to be freed up. Swapping is discussed in Chapter 8.114 Chapter 3 Processes\nswap in swap out\nendCPU\nI/O I/O waiting\nqueues\nready queue\npartially executed\nswapped-out processes\nFigure 3.7 Addition of medium-term scheduling to the queueing diagram.\n3.2.3 Context Switch\nAs mentioned in Section 1.2.1, interrupts cause the operating system to change\na CPU from its current task and to run a kernel routine. Such operations happen\nfrequently on general-purpose systems. When an interrupt occurs, the system\nneeds to save the current context of the process running on the CPU so that\nit can restore that context when its processing is done, essentially suspending\nthe process and then resuming it. The context is represented in the PCB of the\nprocess. It includes the value of the CPU registers, the process state (see Figure\n3.2), and memory-management information. Generically, we perform a state\nsave of the current state of the CPU,b ei ti nk e r n e lo ru s e rm o d e ,a n dt h e na\nstate restore to resume operations.\nSwitching the CPU to another process requires performing a state save of\nthe current process and a state restore of a different process. This task is known\nas a context switch.W h e nac o n t e x ts w i t c ho c c u r s ,t h ek e r n e ls a v e st h ec o n t e x t\nof the old process in its PCB and loads the saved context of the new process\nscheduled to run. Context-switch time is pure overhead, because the system\ndoes no useful work while switching. Switching speed varies from machine to\nmachine, depending on the memory speed, the number of registers that must\nbe copied, and the existence of special instructions (such as a single instruction\nto load or store all registers). A typical speed is a few milliseconds.\nContext-switch times are highly dependent on hardware support. For\ninstance, some processors (such as the Sun Ultra SPARC)p r o v i d em u l t i p l es e t s\nof registers. A context switch here simply requires changing the pointer to the\ncurrent register set. Of course, if there are more active processes than there are\nregister sets, the system resorts to copying register data to and from memory,\nas before. Also, the more complex the operating system, the greater the amount\nof work that must be done during a context switch. As we will see in Chapter\n8, advanced memory-management techniques may require that extra data be\nswitched with each context. For instance , the address space of the current\nprocess must be preserved as the space of the next task is prepared for use.\nHow the address space is preserved, and what amount of work is needed\nto preserve it, depend on the memory-management method of the operating\nsystem.3.3 Operations on Processes 115\nMULTITASKING IN MOBILE SYSTEMS\nBecause of the constraints imposed on mobile devices, early versions of i OS\ndid not provide user-application multitasking; only one application runs in\nthe foreground and all other user applications are suspended. Operating-\nsystem tasks were multitasked because they were written by Apple and well\nbehaved. However, beginning with i OS 4, Apple now provides a limited\nform of multitasking for user applications, thus allowing a single foreground\napplication to run concurrently with multiple background applications. (On\nam o b i l ed e v i c e ,t h eforeground application is the application currently\nopen and appearing on the display. The background application remains\nin memory, but does not occupy the display screen.) The iOS 4p r o g r a m m i n g\nAPI provides support for multitasking, thus allowing a process to run in\nthe background without being suspended. However, it is limited and only\navailable for a limited number of application types, including applications\n\u2022 running a single, \ufb01nite-length task (such as completing a download of\ncontent from a network);\n\u2022 receiving noti\ufb01cations of an event occurring (such as a new email\nmessage);\n\u2022 with long-running background tasks (such as an audio player.)\nApple probably limits multitasking due to battery life and memory use\nconcerns. The CPU certainly has the features to support multitasking, but\nApple chooses to not take advantage of some of them in order to better\nmanage resource use.\nAndroid does not place such constraints on the types of applications that\ncan run in the background. If an application requires processing while in\nthe background, the application must use a service,as e p a r a t ea p p l i c a t i o n\ncomponent that runs on behalf of the background process. Consider a\nstreaming audio application: if the application moves to the background, the\nservice continues to send audio \ufb01les to the audio device driver on behalf of\nthe background application. In fact, the service will continue to run even if the\nbackground application is suspended. Services do not have a user interface\nand have a small memory footprint, thus providing an ef\ufb01cient technique for\nmultitasking in a mobile environment.\n3.3 Operations on Processes\nThe processes in most systems can execute concurrently, and they may\nbe created and deleted dynamically. Thus, these systems must provide a\nmechanism for process creation and termination. In this section, we explore\nthe mechanisms involved in creating processes and illustrate process creation\non UNIX and Windows systems.", "116 Chapter 3 Processes\n3.3.1 Process Creation\nDuring the course of execution, a process may create several new processes. As\nmentioned earlier, the creating process is called a parent process, and the new\nprocesses are called the children of that process. Each of these new processes\nmay in turn create other processes, forming a tree of processes.\nMost operating systems (including UNIX,L i n u x ,a n dW i n d o w s )i d e n t i f y\nprocesses according to a unique process identi\ufb01er (or pid), which is typically\nan integer number. The pid provides a unique value for each process in the\nsystem, and it can be used as an index to access various attributes of a process\nwithin the kernel.\nFigure 3.8 illustrates a typical process tree for the Linux operating system,\nshowing the name of each process and its pid. (We use the termprocess rather\nloosely, as Linux prefers the termtask instead.) Theinit process (which always\nhas a pid of 1) serves as the root parent process for all user processes. Once the\nsystem has booted, theinit process can also create various user processes, such\nas a web or print server, an ssh server, and the like. In Figure 3.8, we see two\nchildren of init\u2014 kthreadd and sshd.T h ekthreadd process is responsible\nfor creating additional processes that perform tasks on behalf of the kernel\n(in this situation, khelper and pdflush). The sshd process is responsible for\nmanaging clients that connect to the system by using ssh (which is short for\nsecure shell). Thelogin process is responsible for managing clients that directly\nlog onto the system. In this example, a client has logged on and is using the\nbash shell, which has been assigned pid 8416. Using the bash command-line\ninterface, this user has created the process ps as well as the emacs editor.\nOn UNIX and Linux systems, we can obtain a listing of processes by using\nthe ps command. For example, the command\nps -el\nwill list complete information for all processes currently active in the system.\nIt is easy to construct a process tree similar to the one shown in Figure 3.8 by\nrecursively tracing parent processes all the way to the init process.\ninit\npid = 1\nsshd\npid = 3028\nlogin\npid = 8415\nkthreadd\npid = 2\nsshd\npid = 3610\npdflush\npid = 200\nkhelper\npid = 6\ntcsch\npid = 4005emacs\npid = 9204\nbash\npid = 8416\nps\npid = 9298\nFigure 3.8 At r e eo fp r o c e s s e so nat y p i c a lL i n u xs y s t e m .3.3 Operations on Processes 117\nIn general, when a process creates a child process, that child process will\nneed certain resources ( CPU time, memory, \ufb01les, I/O devices) to accomplish\nits task. A child process may be able to obtain its resources directly from\nthe operating system, or it may be constrained to a subset of the resources\nof the parent process. The parent may have to partition its resources among\nits children, or it may be able to share some resources (such as memory or\n\ufb01les) among several of its children. Restric ting a child process to a subset of\nthe parent\u2019s resources prevents any process from overloading the system by\ncreating too many child processes.\nIn addition to supplying various physical and logical resources, the parent\nprocess may pass along initialization data (input) to the child process. For\nexample, consider a process whose function is to display the contents of a \ufb01le\n\u2014say , image.jpg\u2014on the screen of a terminal. When the process is created,\nit will get, as an input from its parent process, the name of the \ufb01le image.jpg.\nUsing that \ufb01le name, it will open the \ufb01le and write the contents out. It may\nalso get the name of the output device. Alternatively, some operating systems\npass resources to child processes. On such a system, the new process may get\ntwo open \ufb01les, image.jpg and the terminal device, and may simply transfer\nthe datum between the two.\nWhen a process creates a new process, two possibilities for execution exist:\n1. The parent continues to execute concurrently with its children.\n2. The parent waits until some or all of its children have terminated.\nThere are also two address-space possibilities for the new process:\n1. The child process is a duplicate of the parent process (it has the same\nprogram and data as the parent).\n2. The child process has a new program loaded into it.\nTo illustrate these differences, let\u2019s \ufb01rst consider the UNIX operating system.\nIn UNIX,a sw e \u2019 v es e e n ,e a c hp r o c e s si sidenti\ufb01ed by its process identi\ufb01er,\nwhich is a unique integer. A new process is created by the fork() system\ncall. The new process consists of a copy of the address space of the original\nprocess. This mechanism allows the parent process to communicate easily with\nits child process. Both processes (the parent and the child) continue execution\nat the instruction after the fork(),w i t ho n ed i f f e r e n c e :t h er e t u r nc o d ef o r\nthe fork() is zero for the new (child) process, whereas the (nonzero) process\nidenti\ufb01er of the child is returned to the parent.\nAfter a fork() system call, one of the two processes typically uses the\nexec() system call to replace the process\u2019s memory space with a new program.\nThe exec() system call loads a binary \ufb01le into memory (destroying the\nmemory image of the program containing the exec() system call) and starts\nits execution. In this manner, the two processes are able to communicate and\nthen go their separate ways. The parent can then create more children; or, if it\nhas nothing else to do while the child runs, it can issue await() system call to\nmove itself off the ready queue until the termination of the child. Because the118 Chapter 3 Processes\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\npid\n tp i d ;\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0) { /* child process */\nexeclp(\"/bin/ls\",\"ls\",NULL);\n}\nelse { /* parent process */\n/* parent will wait for the child to complete */\nwait(NULL);\nprintf(\"Child Complete\");\n}\nreturn 0;\n}\nFigure 3.9 Creating a separate process using the UNIXfork() system call.\ncall to exec() overlays the process\u2019s address space with a new program, the\ncall to exec() does not return control unless an error occurs.\nThe C program shown in Figure 3.9 illustrates the UNIX system calls\npreviously described. We now have two different processes running copies\nof the same program. The only difference is that the value of pid (the process\nidenti\ufb01er) for the child process is zero, while that for the parent is an integer\nvalue greater than zero (in fact, it is the actual pid of the child process). The\nchild process inherits privileges and scheduling attributes from the parent,\nas well certain resources, such as open \ufb01les. The child process then overlays\nits address space with the UNIX command /bin/ls (used to get a directory\nlisting) using the execlp() system call (execlp() is a version of the exec()\nsystem call). The parent waits for the child process to complete with thewait()\nsystem call. When the child process completes (by either implicitly or explicitly\ninvoking exit()), the parent process resumes from the call towait(), where it\ncompletes using the exit() system call. This is also illustrated in Figure 3.10.\nOf course, there is nothing to prevent the child from not invoking exec()\nand instead continuing to execute as a copy of the parent process. In this\nscenario, the parent and child are concurrent processes running the same code", "3.3 Operations on Processes 119\npid = fork()\nexec()\nparent\nparent (pid > 0)\nchild (pid = 0)\nwait()\nexit()\nparent resumes\nFigure 3.10 Process creation using thefork() system call.\ninstructions. Because the child is a copy of the parent, each process has its own\ncopy of any data.\nAs an alternative example, we next consider process creation in Windows.\nProcesses are created in the Windows API using the CreateProcess() func-\ntion, which is similar to fork() in that a parent creates a new child process.\nHowever, whereas fork() has the child process inheriting the address space\nof its parent, CreateProcess() requires loading a speci\ufb01ed program into the\naddress space of the child process at process creation. Furthermore, whereas\nfork() is passed no parameters, CreateProcess() expects no fewer than ten\nparameters.\nThe C program shown in Figure 3.11 illustrates the CreateProcess()\nfunction, which creates a child process that loads the applicationmspaint.exe.\nWe opt for many of the default values of the ten parameters passed to\nCreateProcess().R e a d e r si n t e r e s t e di np u r s u i n gt h ed e t a i l so fp r o c e s s\ncreation and management in the Windows API are encouraged to consult the\nbibliographical notes at the end of this chapter.\nThe two parameters passed to theCreateProcess() function are instances\nof the STARTUPINFO and PROCESS\n INFORMATION structures. STARTUPINFO\nspeci\ufb01es many properties of the new process, such as window size and\nappearance and handles to standard input and output \ufb01les. The PRO-\nCESS\n INFORMATION structure contains a handle and the identi\ufb01ers to the\nnewly created process and its thread. We invoke the ZeroMemory() func-\ntion to allocate memory for each of these structures before proceeding with\nCreateProcess().\nThe \ufb01rst two parameters passed to CreateProcess() are the application\nname and command-line parameters. If the application name is NULL (as it is\nin this case), the command-line parameter speci\ufb01es the application to load. In\nthis instance, we are loading the Microsoft Windowsmspaint.exe application.\nBeyond these two initial parameters, we use the default parameters for\ninheriting process and thread handles as well as specifying that there will be no\ncreation \ufb02ags. We also use the parent\u2019s existing environment block and starting\ndirectory. Last, we provide two pointers to the STARTUPINFO and PROCESS\n -\nINFORMATION structures created at the beginning of the program. In Figure\n3.9, the parent process waits for the child to complete by invoking the wait()\nsystem call. The equivalent of this in Windows is WaitForSingleObject(),\nwhich is passed a handle of the child process\u2014 pi.hProcess\u2014and waits for\nthis process to complete. Once the child process exits, control returns from the\nWaitForSingleObject() function in the parent process.120 Chapter 3 Processes\n#include <stdio.h>\n#include <windows.h>\nint main(VOID)\n{\nSTARTUPINFO si;\nPROCESS\n INFORMATION pi;\n/* allocate memory */\nZeroMemory(&si, sizeof(si));\nsi.cb = sizeof(si);\nZeroMemory(&pi, sizeof(pi));\n/* create child process */\nif (!CreateProcess(NULL, /* use command line */\n\"C:\\\\WINDOWS\\\\system32\\\\mspaint.exe\", /* command */\nNULL, /* don\u2019t inherit process handle */\nNULL, /* don\u2019t inherit thread handle */\nFALSE, /* disable handle inheritance */\n0, /* no creation flags */\nNULL, /* use parent\u2019s environment block */\nNULL, /* use parent\u2019s existing directory */\n&si,\n&pi))\n{\nfprintf(stderr, \"Create Process Failed\");\nreturn -1;\n}\n/* parent will wait for the child to complete */\nWaitForSingleObject(pi.hProcess, INFINITE);\nprintf(\"Child Complete\");\n/* close handles */\nCloseHandle(pi.hProcess);\nCloseHandle(pi.hThread);\n}\nFigure 3.11 Creating a separate process using the Windows API.\n3.3.2 Process Termination\nAp r o c e s st e r m i n a t e sw h e ni t\ufb01 n i s h e se x e cuting its \ufb01nal statement and asks the\noperating system to delete it by using theexit() system call. At that point, the\nprocess may return a status value (typically an integer) to its parent process\n(via the wait() system call). All the resources of the process\u2014including\nphysical and virtual memory, open \ufb01les, and I/O buffers\u2014are deallocated\nby the operating system.\nTermination can occur in other circumstances as well. A process can cause\nthe termination of another process via an appropriate system call (for example,\nTerminateProcess() in Windows). Usually, such a system call can be invoked3.3 Operations on Processes 121\nonly by the parent of the process that is to be terminated. Otherwise, users could\narbitrarily kill each other\u2019s jobs. Note thata parent needs to know the identities\nof its children if it is to terminate them. Thus, when one process creates a new\nprocess, the identity of the newly created process is passed to the parent.\nAp a r e n tm a yt e r m i n a t et h ee x e c u t i o no fo n eo fi t sc h i l d r e nf o rav a r i e t yo f\nreasons, such as these:\n\u2022 The child has exceeded its usage of some of the resources that it has been\nallocated. (To determine whether this has occurred, the parent must have\nam e c h a n i s mt oi n s p e c tt h es t a t eo fi t sc h i l d r e n . )\n\u2022 The task assigned to the child is no longer required.\n\u2022 The parent is exiting, and the operating system does not allow a child to\ncontinue if its parent terminates.\nSome systems do not allow a child to exist if its parent has terminated. In\nsuch systems, if a process terminates (either normally or abnormally), then\nall its children must also be termina ted. This phenomenon, referred to as\ncascading termination,i sn o r m a l l yi n i t i a t e db yt h eo p e r a t i n gs y s t e m .\nTo illustrate process execution and termination, consider that, in Linux\nand UNIX systems, we can terminate a process by using the exit() system\ncall, providing an exit status as a parameter:\n/* exit with status 1 */\nexit(1);\nIn fact, under normal termination, exit() may be called either directly (as\nshown above) or indirectly (by a return statement in main()).\nAp a r e n tp r o c e s sm a yw a i tf o rt h et e r m i n a t i o no fac h i l dp r o c e s sb yu s i n g\nthe wait() system call. The wait() system call is passed a parameter that\nallows the parent to obtain the exit status of the child. This system call also\nreturns the process identi\ufb01er of the terminated child so that the parent can tell\nwhich of its children has terminated:\npid\n tp i d ;\nint status;\npid = wait(&status);\nWhen a process terminates, its resources are deallocated by the operating\nsystem. However, its entry in the process table must remain there until the\nparent callswait(),b e c a u s et h ep r o c e s st a b l ec o n t a i n st h ep r o c e s s \u2019 se x i ts t a t u s .\nAp r o c e s st h a th a st e r m i n a t e d ,b u tw h o s ep a r e n th a sn o ty e tc a l l e dwait(),i s\nknown as a zombie process. All processes transition to this state when they\nterminate, but generally they exist as zomb ies only brie\ufb02y. Once the parent\ncalls wait(),t h ep r o c e s si d e n t i \ufb01 e ro ft h ez o m b i ep r o c e s sa n di t se n t r yi nt h e\nprocess table are released.\nNow consider what would happen if a parent did not invoke wait() and\ninstead terminated, thereby leaving its child processes as orphans.L i n u xa n d\nUNIX address this scenario by assigning the init process as the new parent to", "122 Chapter 3 Processes\norphan processes. (Recall from Figure 3.8 that theinit process is the root of the\nprocess hierarchy in UNIX and Linux systems.) The init process periodically\ninvokes wait(),t h e r e b ya l l o w i n gt h ee x i ts t a t u so fa n yo r p h a n e dp r o c e s st ob e\ncollected and releasing the orphan\u2019s processidenti\ufb01er and process-table entry.\n3.4 Interprocess Communication\nProcesses executing concurrently in the operating system may be either\nindependent processes or cooperating processes. A process is independent\nif it cannot affect or be affected by the other processes executing in the system.\nAny process that does not share data withany other process is independent. A\nprocess is cooperating if it can affect or be affected by the other processes\nexecuting in the system. Clearly, any process that shares data with other\nprocesses is a cooperating process.\nThere are several reasons for providing an environment that allows process\ncooperation:\n\u2022 Information sharing. Since several users may be interested in the same\npiece of information (for instance, a shared \ufb01le), we must provide an\nenvironment to allow concurrent access to such information.\n\u2022 Computation speedup.I fw ew a n tap a r t i c u l a rt a s kt or u nf a s t e r ,w em u s t\nbreak it into subtasks, each of which will be executing in parallel with the\nothers. Notice that such a speedup ca nb ea c h i e v e do n l yi ft h ec o m p u t e r\nhas multiple processing cores.\n\u2022 Modularity.W em a yw a n tt oc o n s t r u c tt h es y s t e mi nam o d u l a rf a s h i o n ,\ndividing the system functions into separate processes or threads, as we\ndiscussed in Chapter 2.\n\u2022 Convenience.E v e na ni n d i v i d u a lu s e rm a yw o r ko nm a n yt a s k sa tt h e\nsame time. For instance, a user may be editing, listening to music, and\ncompiling in parallel.\nCooperating processes require aninterprocess communication(IPC) mech-\nanism that will allow them to exchange data and information. There are two\nfundamental models of interprocess communication:shared memoryand mes-\nsage passing.I nt h es h a r e d - m e m o r ym o d e l ,ar e g i o no fm e m o r yt h a ti ss h a r e d\nby cooperating processes is established. Processes can then exchange informa-\ntion by reading and writing data to the shared region. In the message-passing\nmodel, communication takes place by means of messages exchanged between\nthe cooperating processes. The two communications models are contrasted in\nFigure 3.12.\nBoth of the models just mentioned are common in operating systems,\nand many systems implement both. Message passing is useful for exchanging\nsmaller amounts of data, because no con\ufb02icts need be avoided. Message\npassing is also easier to implement in a distributed system than shared memory.\n(Although there are systems that provide distributed shared memory, we do not\nconsider them in this text.) Shared memory can be faster than message passing,\nsince message-passing systems are typically implemented using system calls3.4 Interprocess Communication 123\nMULTIPROCESS ARCHITECTURE\u2014CHROME BROWSER\nMany websites contain active content such as JavaScript, Flash, andHTML5 to\nprovide a rich and dynamic web-browsing experience. Unfortunately, these\nweb applications may also contain software bugs, which can result in sluggish\nresponse times and can even cause the web browser to crash. This isn\u2019t a big\nproblem in a web browser that displays content from only one website. But\nmost contemporary web browsers provide tabbed browsing, which allows a\nsingle instance of a web browser application to open several websites at the\nsame time, with each site in a separate tab. To switch between the different\nsites , a user need only click on the appropriate tab. This arrangement is\nillustrated below:\nAp r o b l e mw i t ht h i sa p p r o a c hi st h a ti faw e ba p p l i c a t i o ni na n yt a bc r a s h e s ,\nthe entire process\u2014including all other tabs displaying additional websites\n\u2014crashes as well.\nGoogle\u2019s Chrome web browser was designed to address this issue by\nusing a multiprocess architecture. Chrome identi\ufb01es three different types of\nprocesses: browser, renderers, and plug-ins.\n\u2022 The browser process is responsible for managing the user interface as\nwell as disk and network I/O.An e wb r o w s e rp r o c e s si sc r e a t e dw h e n\nChrome is started. Only one browser process is created.\n\u2022 Renderer processes contain logic for rendering web pages. Thus, they\ncontain the logic for handlingHTML,J a v a s c r i p t ,i m a g e s ,a n ds of o r t h .A s\nag e n e r a lr u l e ,an e wr e n d e r e rp r o c e s si sc r e a t e df o re a c hw e b s i t eo p e n e d\nin a new tab, and so several renderer processes may be active at the same\ntime.\n\u2022 A plug-in process is created for each type of plug-in (such as Flash or\nQuickTime) in use. Plug-in processes contain the code for the plug-in as\nwell as additional code that enables the plug-in to communicate with\nassociated renderer processes and the browser process.\nThe advantage of the multiprocess approach is that websites run in\nisolation from one another. If one website crashes, only its renderer process\nis affected; all other processes remain unharmed. Furthermore, renderer\nprocesses run in a sandbox,w h i c hm e a n st h a ta c c e s st od i s ka n dn e t w o r k\nI/O is restricted, minimizing the effects of any security exploits.\nand thus require the more time-con suming task of kernel intervention. In\nshared-memory systems, system calls are required only to establish shared-124 Chapter 3 Processes\nprocess A\nmessage queue\nkernel\n(a) (b)\nprocess A\nshared memory\nkernel\nprocess B\nm0 m1 m2 ...m3 mn\nprocess B\nFigure 3.12 Communications models. (a) Message passing. (b) Shared memory.\nmemory regions. Once shared memory is established, all accesses are treated\nas routine memory accesses, and no assistance from the kernel is required.\nRecent research on systems with several processing cores indicates that\nmessage passing provides better performance than shared memory on such\nsystems. Shared memory suffers from cache coherency issues, which arise\nbecause shared data migrate among the several caches. As the number of\nprocessing cores on systems increases, it is possible that we will see message\npassing as the preferred mechanism for IPC.\nIn the remainder of this section, we explore shared-memory and message-\npassing systems in more detail.\n3.4.1 Shared-Memory Systems\nInterprocess communication using shared memory requires communicating\nprocesses to establish a region of shared memory. Typically, a shared-memory\nregion resides in the address space of the process creating the shared-memory\nsegment. Other processes that wish to communicate using this shared-memory\nsegment must attach it to their address space. Recall that, normally, the\noperating system tries to prevent one process from accessing another process\u2019s\nmemory. Shared memory requires that two or more processes agree to remove\nthis restriction. They can then exchange information by reading and writing\ndata in the shared areas. The form of the data and the location are determined by\nthese processes and are not under the operating system\u2019s control. The processes\nare also responsible for ensuring that they are not writing to the same location\nsimultaneously.\nTo illustrate the concept of cooperating processes, let\u2019s consider the\nproducer\u2013consumer problem, which is a common paradigm for cooperating\nprocesses. A producer process produces information that is consumed by a\nconsumer process. For example, a compiler may produce assembly code that\nis consumed by an assembler. The assembler, in turn, may produce object\nmodules that are consumed by the loader. The producer\u2013consumer problem", "3.4 Interprocess Communication 125item next\n produced;\nwhile (true) {\n/* produce an item in next\n produced */\nwhile (((in + 1) %B U F F E R\nSIZE)= =o u t )\n;/ *d on o t h i n g* /\nbuffer[in] = next\n produced;\nin = (in + 1) %B U F F E R\nSIZE;\n}\nFigure 3.13 The producer process using shared memory.\nalso provides a useful metaphor for the client\u2013server paradigm. We generally\nthink of a server as a producer and a client as a consumer. For example, a web\nserver produces (that is, provides)HTML \ufb01les and images, which are consumed\n(that is, read) by the client web browser requesting the resource.\nOne solution to the producer\u2013consumer problem uses shared memory. To\nallow producer and consumer processes to run concurrently, we must have\navailable a buffer of items that can be \ufb01lled by the producer and emptied by\nthe consumer. This buffer will reside in a region of memory that is shared by\nthe producer and consumer processes. A producer can produce one item while\nthe consumer is consuming another item. The producer and consumer must\nbe synchronized, so that the consumer does not try to consume an item that\nhas not yet been produced.\nTwo types of buffers can be used. Theunbounded bufferplaces no practical\nlimit on the size of the buffer. The consumer may have to wait for new items,\nbut the producer can always produce new items. Thebounded bufferassumes\na\ufb01 x e db u f f e rs i z e .I nt h i sc a s e ,t h ec o n s u m e rm u s tw a i ti ft h eb u f f e ri se m p t y ,\nand the producer must wait if the buffer is full.\nLet\u2019s look more closely at how the bounded buffer illustrates interprocess\ncommunication using shared memory. The following variables reside in a\nregion of memory shared by the producer and consumer processes:\n#define BUFFER\n SIZE 10\ntypedef struct {\n...\n}item;\nitem buffer[BUFFER\n SIZE];\nint in = 0;\nint out = 0;\nThe sharedbuffer is implemented as a circular array with two logical pointers:\nin and out.T h ev a r i a b l ein points to the next free position in the buffer; out\npoints to the \ufb01rst full position in the buffer. The buffer is empty when in ==\nout;t h eb u f f e ri sf u l lw h e n( (in +1 ) %B U F F E R\nSIZE)= = out.\nThe code for the producer process is shown in Figure 3.13, and the code\nfor the consumer process is shown in Figure 3.14. The producer process has a126 Chapter 3 Processes\nitem next\n consumed;\nwhile (true) {\nwhile (in == out)\n;/ *d on o t h i n g* /\nnext\n consumed = buffer[out];\nout = (out + 1) %B U F F E R\nSIZE;\n/* consume the item in next\n consumed */\n}\nFigure 3.14 The consumer process using shared memory.\nlocal variable next\n produced in which the new item to be produced is stored.\nThe consumer process has a local variable next\n consumed in which the item\nto be consumed is stored.\nThis scheme allows at most BUFFER\n SIZE \u2212 1 items in the buffer at the\nsame time. We leave it as an exercise for you to provide a solution in which\nBUFFER\n SIZE items can be in the buffer at the same time. In Section 3.5.1, we\nillustrate the POSIX API for shared memory.\nOne issue this illustration does not address concerns the situation in which\nboth the producer process and the consumer process attempt to access the\nshared buffer concurrently. In Chapter 5, we discuss how synchronization\namong cooperating processes can be implemented effectively in a shared-\nmemory environment.\n3.4.2 Message-Passing Systems\nIn Section 3.4.1, we showed how cooperating processes can communicate in a\nshared-memory environment. The scheme requires that these processes share a\nregion of memory and that the code for accessing and manipulating the shared\nmemory be written explicitly by the application programmer. Another way to\nachieve the same effect is for the operating system to provide the means for\ncooperating processes to communicate with each other via a message-passing\nfacility.\nMessage passing provides a mechanism to allow processes to communicate\nand to synchronize their actions without sharing the same address space. It is\nparticularly useful in a distributed environment ,w h e r et h ec o m m u n i c a t i n g\nprocesses may reside on different computers connected by a network. For\nexample, an Internetchat program could be designed so that chat participants\ncommunicate with one another by exchanging messages.\nAm e s s a g e - p a s s i n gf a c i l i t yp r o v i d e sa tl e a s tt w oo p e r a t i o n s :\nsend(message) receive(message)\nMessages sent by a process can be either \ufb01xed or variable in size. If only\n\ufb01xed-sized messages can be sent, the system-level implementation is straight-\nforward. This restriction, however, m akes the task of programming more\ndif\ufb01cult. Conversely, variable-sized messages require a more complex system-3.4 Interprocess Communication 127\nlevel implementation, but the programming task becomes simpler. This is a\ncommon kind of tradeoff seen throughout operating-system design.\nIf processesP and Q want to communicate, they must send messages to and\nreceive messages from each other: a communication link must exist between\nthem. This link can be implemented in a varietyof ways. We are concerned here\nnot with the link\u2019s physical implementation (such as shared memory, hardware\nbus, or network, which are covered in Chapter 17) but rather with its logical\nimplementation. Here are several methods fo rl o g i c a l l yi m p l e m e n t i n gal i n k\nand the send()/receive() operations:\n\u2022 Direct or indirect communication\n\u2022 Synchronous or asynchronous communication\n\u2022 Automatic or explicit buffering\nWe look at issues related to each of these features next.\n3.4.2.1 Naming\nProcesses that want to communicate must have a way to refer to each other.\nThey can use either direct or indirect communication.\nUnder direct communication ,e a c hp r o c e s st h a tw a n t st oc o m m u n i c a t e\nmust explicitly name the recipient or sender of the communication. In this\nscheme, the send() and receive() primitives are de\ufb01ned as:\n\u2022 send(P, message)\u2014Send a message to process P.\n\u2022 receive(Q, message)\u2014Receive a message from process Q.\nAc o m m u n i c a t i o nl i n ki nt h i ss c h e m eh a st h ef o l l o w i n gp r o p e r t i e s :\n\u2022 A link is established automatically be tween every pair of processes that\nwant to communicate. The processes need to know only each other\u2019s\nidentity to communicate.\n\u2022 Al i n ki sa s s o c i a t e dw i t he x a c t l yt w op r o c e s s e s .\n\u2022 Between each pair of processes, there exists exactly one link.\nThis scheme exhibits symmetry in addressing; that is, both the sender\nprocess and the receiver process must name the other to communicate. A\nvariant of this scheme employsasymmetry in addressing. Here, only the sender\nnames the recipient; the recipient is not required to name the sender. In this\nscheme, the send() and receive() primitives are de\ufb01ned as follows:\n\u2022 send(P, message)\u2014Send a message to process P.\n\u2022 receive(id, message)\u2014Receive a message from any process. The\nvariable id is set to the name of the process with which communication\nhas taken place.", "128 Chapter 3 Processes\nThe disadvantage in both of these schemes (symmetric and asymmetric)\nis the limited modularity of the resulting process de\ufb01nitions. Changing the\nidenti\ufb01er of a process may necessitate examining all other process de\ufb01nitions.\nAll references to the old identi\ufb01er must be found, so that they can be modi\ufb01ed\nto the new identi\ufb01er. In general, any such hard-coding techniques, where\nidenti\ufb01ers must be explicitly stated, are less desirable than techniques involving\nindirection, as described next.\nWith indirect communication,t h em e s s a g e sa r es e n tt oa n dr e c e i v e df r o m\nmailboxes,o rports.Am a i l b o xc a nb ev i e w e da b s t r a c t l ya sa no b j e c ti n t ow h i c h\nmessages can be placed by processes and from which messages can be removed.\nEach mailbox has a unique identi\ufb01cation. For example, POSIX message queues\nuse an integer value to identify a mailbox. A process can communicate with\nanother process via a number of different mailboxes, but two processes can\ncommunicate only if they have a shared mailbox. The send() and receive()\nprimitives are de\ufb01ned as follows:\n\u2022 send(A, message)\u2014Send a message to mailbox A.\n\u2022 receive(A, message)\u2014Receive a message from mailbox A.\nIn this scheme, a communication link has the following properties:\n\u2022 Al i n ki se s t a b l i s h e db e t w e e nap a i ro fp rocesses only if both members of\nthe pair have a shared mailbox.\n\u2022 Al i n km a yb ea s s o c i a t e dw i t hm o r et h a nt w op r o c e s s e s .\n\u2022 Between each pair of communicating processes, a number of different links\nmay exist, with each link corresponding to one mailbox.\nNow suppose that processes P1, P2,a n d P3 all share mailbox A.P r o c e s s\nP1 sends a message to A, while both P2 and P3 execute a receive() from A.\nWhich process will receive the message sent by P1?T h ea n s w e rd e p e n d so n\nwhich of the following methods we choose:\n\u2022 Allow a link to be associated with two processes at most.\n\u2022 Allow at most one process at a time to execute a receive() operation.\n\u2022 Allow the system to select arbitrarily which process will receive the\nmessage (that is, eitherP2 or P3,b u tn o tb o t h ,w i l lr e c e i v et h em e s s a g e ) .T h e\nsystem may de\ufb01ne an algorithm for selecting which process will receive the\nmessage (for example, round robin, where processes take turns receiving\nmessages). The system may identify the receiver to the sender.\nA mailbox may be owned either by a process or by the operating system.\nIf the mailbox is owned by a process (that is, the mailbox is part of the address\nspace of the process), then we distinguish between the owner (which can\nonly receive messages through this mailbox) and the user (which can only\nsend messages to the mailbox). Since e ach mailbox has a unique owner, there\ncan be no confusion about which process should receive a message sent to\nthis mailbox. When a process that owns a mailbox terminates, the mailbox3.4 Interprocess Communication 129\ndisappears. Any process that subsequently sends a message to this mailbox\nmust be noti\ufb01ed that the mailbox no longer exists.\nIn contrast, a mailbox that is owned by the operating system has an\nexistence of its own. It is independent and is not attached to any particular\nprocess. The operating system then must provide a mechanism that allows a\nprocess to do the following:\n\u2022 Create a new mailbox.\n\u2022 Send and receive messages through the mailbox.\n\u2022 Delete a mailbox.\nThe process that creates a new mailbox is that mailbox\u2019s owner by default.\nInitially, the owner is the only process that can receive messages through this\nmailbox. However, the ownership and receiving privilege may be passed to\nother processes through appropriate system calls. Of course, this provision\ncould result in multiple receivers for each mailbox.\n3.4.2.2 Synchronization\nCommunication between processes takes place through calls to send() and\nreceive() primitives. There are different design options for implementing\neach primitive. Message passing may be either blocking or nonblocking\u2014\nalso known as synchronous and asynchronous.( T h r o u g h o u tt h i st e x t ,y o u\nwill encounter the concepts of synchronous and asynchronous behavior in\nrelation to various operating-system algorithms.)\n\u2022 Blocking send .T h es e n d i n gp r o c e s si sb l o c k e du n t i lt h em e s s a g ei s\nreceived by the receiving process or by the mailbox.\n\u2022 Nonblocking send.T h es e n d i n gp r o c e s ss e n d st h em e s s a g ea n dr e s u m e s\noperation.\n\u2022 Blocking receive.T h er e c e i v e rb l o c k su n t i lam e s s a g ei sa v a i l a b l e .\n\u2022 Nonblocking receive.T h er e c e i v e rr e t r i e v e se i t h e rav a l i dm e s s a g eo ra\nnull.\nDifferent combinations ofsend() and receive() are possible. When both\nsend() and receive() are blocking, we have a rendezvous between the\nsender and the receiver. The solution to the producer\u2013consumer problem\nbecomes trivial when we use blocking send() and receive() statements.\nThe producer merely invokes the blocking send() call and waits until the\nmessage is delivered to either the receiver or the mailbox. Likewise, when the\nconsumer invokes receive(),i tb l o c k su n t i lam e s s a g ei sa v a i l a b l e .T h i si s\nillustrated in Figures 3.15 and 3.16.\n3.4.2.3 Buffering\nWhether communication is direct or indirect, messages exchanged by commu-\nnicating processes reside in a temporary queue. Basically, such queues can be\nimplemented in three ways:130 Chapter 3 Processes\nmessage next\n produced;\nwhile (true) {\n/* produce an item in next\n produced */\nsend(next\n produced);\n}\nFigure 3.15 The producer process using message passing.\n\u2022 Zero capacity.T h eq u e u eh a sam a x i m u ml e n g t ho fz e r o ;t h u s ,t h el i n k\ncannot have any messages waiting in it. In this case, the sender must block\nuntil the recipient receives the message.\n\u2022 Bounded capacity.T h eq u e u eh a s\ufb01 n i t el e n g t hn; thus, at most n messages\ncan reside in it. If the queue is not full when a new message is sent, the\nmessage is placed in the queue (either the message is copied or a pointer\nto the message is kept), and the sender can continue execution without\nwaiting. The link\u2019s capacity is \ufb01nite, however. If the link is full, the sender\nmust block until space is available in the queue.\n\u2022 Unbounded capacity. The queue\u2019s length is potentially in\ufb01nite; thus, any\nnumber of messages can wait in it. The sender never blocks.\nThe zero-capacity case is sometimes referred to as a message system with no\nbuffering. The other cases are referred to as systems with automatic buffering.\n3.5 Examples of IPC Systems\nIn this section, we explore three differentIPC systems. We \ufb01rst cover the POSIX\nAPI for shared memory and then discuss message passing in the Mach operating\nsystem. We conclude with Windows, which interestingly uses shared memory\nas a mechanism for providing certain types of message passing.\n3.5.1 An Example: POSIX Shared Memory\nSeveral IPC mechanisms are available for POSIX systems, including shared\nmemory and message passing. Here, we explore the POSIX API for shared\nmemory.\nPOSIX shared memory is organized using memory-mapped \ufb01les, which\nassociate the region of shared memory with a \ufb01le. A process must \ufb01rst create\nmessage next\n consumed;\nwhile (true) {\nreceive(next\n consumed);\n/* consume the item in next\n consumed */\n}\nFigure 3.16 The consumer process using message passing.", "3.5 Examples of IPC Systems 131\nas h a r e d - m e m o r yo b j e c tu s i n gt h eshm\n open() system call, as follows:\nshm\n fd = shm\n open(name, O\n CREAT | O\n RDRW, 0666);\nThe \ufb01rst parameter speci\ufb01es the name of the shared-memory object. Processes\nthat wish to access this shared memory must refer to the object by this name.\nThe subsequent parameters specify that the shared-memory object is to be\ncreated if it does not yet exist (O\n CREAT)a n dt h a tt h eo b j e c ti so p e nf o rr e a d i n g\nand writing (O\n RDRW). The last parameter establishes the directory permissions\nof the shared-memory object. A successful call toshm\n open() returns an integer\n\ufb01le descriptor for the shared-memory object.\nOnce the object is established, the ftruncate() function is used to\ncon\ufb01gure the size of the object in bytes. The call\nftruncate(shm\n fd, 4096);\nsets the size of the object to 4,096 bytes.\nFinally, themmap() function establishes a memory-mapped \ufb01le containing\nthe shared-memory object. It alsoreturns a pointer to the memory-mapped \ufb01le\nthat is used for accessing the shared-memory object.\nThe programs shown in Figure 3.17 and 3.18 use the producer\u2013consumer\nmodel in implementing shared memory. The producer establishes a shared-\nmemory object and writes to shared memory, and the consumer reads from\nshared memory.\nThe producer, shown in Figure 3.17, creates a shared-memory object named\nOS and writes the infamous string \"Hello World!\" to shared memory. The\nprogram memory-maps a shared-memory object of the speci\ufb01ed size and\nallows writing to the object. (Obviously, only writing is necessary for the\nproducer.) The \ufb02ag MAP\n SHARED speci\ufb01es that changes to the shared-memory\nobject will be visible to all processes sharing the object. Notice that we write to\nthe shared-memory object by calling the sprintf() function and writing the\nformatted string to the pointer ptr.A f t e re a c hw r i t e ,w em u s ti n c r e m e n tt h e\npointer by the number of bytes written.\nThe consumer process, shown in Figure 3.18, reads and outputs the contents\nof the shared memory. The consumer also invokes theshm\n unlink() function,\nwhich removes the shared-memory segment after the consumer has accessed\nit. We provide further exercises using the POSIX shared-memory API in the\nprogramming exercises at the end of this chapter. Additionally, we provide\nmore detailed coverage of memory mapping in Section 9.7.\n3.5.2 An Example: Mach\nAs an example of message passing, we next consider the Mach operating\nsystem. You may recall that we introduced Mach in Chapter 2 as part of the Mac\nOS X operating system. The Mach kernel supports the creation and destruction\nof multiple tasks, which are similar to processes but have multiple threads\nof control and fewer associated resources. Most communication in Mach\u2014\nincluding all intertask information\u2014is carried out by messages. Messages are\nsent to and received from mailboxes, called ports in Mach.132 Chapter 3 Processes\n#include <stdio.h>\n#include <stlib.h>\n#include <string.h>\n#include <fcntl.h>\n#include <sys/shm.h>\n#include <sys/stat.h>\nint main()\n{\n/* the size (in bytes) of shared memory object */\nconst int SIZE 4096;\n/* name of the shared memory object */\nconst char *name = \"OS\";\n/* strings written to shared memory */\nconst char *message\n 0=\" H e l l o \" ;\nconst char *message\n 1=\" W o r l d ! \" ;\n/* shared memory file descriptor */\nint shm\n fd;\n/* pointer to shared memory obect */\nvoid *ptr;\n/* create the shared memory object */\nshm\n fd = shm\n open(name, O\n CREAT | O\n RDRW, 0666);\n/* configure the size of the shared memory object */\nftruncate(shm\n fd, SIZE);\n/* memory map the shared memory object */\nptr = mmap(0, SIZE, PROT\n WRITE, MAP\n SHARED, shm\n fd, 0);\n/* write to the shared memory object */\nsprintf(ptr,\"%s\",message\n 0);\nptr += strlen(message\n 0);\nsprintf(ptr,\"%s\",message\n 1);\nptr += strlen(message\n 1);\nreturn 0;\n}\nFigure 3.17 Producer process illustrating POSIX shared-memory API.\nEven system calls are made by messages. When a task is created, two\nspecial mailboxes\u2014the Kernel mailbox and the Notify mailbox\u2014are also\ncreated. The kernel uses the Kernel mailbox to communicate with the task and\nsends noti\ufb01cation of event occurren ces to the Notify port. Only three system\ncalls are needed for message transfer. The msg\n send() call sends a message\nto a mailbox. A message is received via msg\n receive().R e m o t ep r o c e d u r e\ncalls (RPCs) are executed via msg\n rpc(),w h i c hs e n d sam e s s a g ea n dw a i t sf o r\nexactly one return message from the sender. In this way, the RPC models a3.5 Examples of IPC Systems 133\n#include <stdio.h>\n#include <stlib.h>\n#include <fcntl.h>\n#include <sys/shm.h>\n#include <sys/stat.h>\nint main()\n{\n/* the size (in bytes) of shared memory object */\nconst int SIZE 4096;\n/* name of the shared memory object */\nconst char *name = \"OS\";\n/* shared memory file descriptor */\nint shm\n fd;\n/* pointer to shared memory obect */\nvoid *ptr;\n/* open the shared memory object */\nshm\n fd = shm\n open(name, O\n RDONLY, 0666);\n/* memory map the shared memory object */\nptr = mmap(0, SIZE, PROT\n READ, MAP\n SHARED, shm\n fd, 0);\n/* read from the shared memory object */\nprintf(\"%s\",(char *)ptr);\n/* remove the shared memory object */\nshm\n unlink(name);\nreturn 0;\n}\nFigure 3.18 Consumer process illustrating POSIX shared-memory API.\ntypical subroutine procedure call but can work between systems\u2014hence the\nterm remote.R e m o t ep r o c e d u r ec a l l sa r ec o v e r e di nd e t a i li nS e c t i o n3 . 6 . 2 .\nThe port\n allocate() system call creates a new mailbox and allocates\nspace for its queue of messages. The maximum size of the message queue\ndefaults to eight messages. The task that creates the mailbox is that mailbox\u2019s\nowner. The owner is also allowed to receive from the mailbox. Only one task\nat a time can either own or receive from a mailbox, but these rights can be sent\nto other tasks.\nThe mailbox\u2019s message queue is initially empty. As messages are sent to\nthe mailbox, the messages are copied into the mailbox. All messages have the\nsame priority. Mach guarantees that multiple messages from the same sender\nare queued in \ufb01rst-in, \ufb01rst-out (FIFO) order but does not guarantee an absolute\nordering. For instance, messages from two senders may be queued in any order.\nThe messages themselves consist of a \ufb01xed-length header followed by a\nvariable-length data portion. The header indicates the length of the message\nand includes two mailbox names. One mailbox name speci\ufb01es the mailbox", "134 Chapter 3 Processes\nto which the message is being sent. Commonly, the sending thread expects a\nreply, so the mailbox name of the sender is passed on to the receiving task,\nwhich can use it as a \u201creturn address.\u201d\nThe variable part of a message is a list of typed data items. Each entry\nin the list has a type, size, and value. The type of the objects speci\ufb01ed in the\nmessage is important, since objects de\ufb01ned by the operating system\u2014such as\nownership or receive access rights, task states, and memory segments\u2014may\nbe sent in messages.\nThe send and receive operations themselves are \ufb02exible. For instance, when\na message is sent to a mailbox, the mailbox may be full. If the mailbox is not\nfull, the message is copied to the mailbox, and the sending thread continues. If\nthe mailbox is full, the sending thread has four options:\n1. Wait inde\ufb01nitely until there is room in the mailbox.\n2. Wait at most n milliseconds.\n3. Do not wait at all but rather return immediately.\n4. Temporarily cache a message. Here, a message is given to the operating\nsystem to keep, even though the mailbox to which that message is being\nsent is full. When the message can be put in the mailbox, a message is sent\nback to the sender. Only one message to a full mailbox can be pending at\nany time for a given sending thread.\nThe \ufb01nal option is meant for server tasks, such as a line-printer driver. After\n\ufb01nishing a request, such tasks may need to send a one-time reply to the task\nthat requested service, but they must alsocontinue with other service requests,\neven if the reply mailbox for a client is full.\nThe receive operation must specify the mailbox or mailbox set from which a\nmessage is to be received. Amailbox setis a collection of mailboxes, as declared\nby the task, which can be grouped together and treated as one mailbox for the\npurposes of the task. Threads in a task can receive only from a mailbox or\nmailbox set for which the task has receive access. A port\n status() system\ncall returns the number of messages in a given mailbox. The receive operation\nattempts to receive from (1) any mailbox in a mailbox set or (2) a speci\ufb01c\n(named) mailbox. If no message is waiting to be received, the receiving thread\ncan either wait at most n milliseconds or not wait at all.\nThe Mach system was especially designed for distributed systems, which\nwe discuss in Chapter 17, but Mach was shown to be suitable for systems\nwith fewer processing cores, as evidenced by its inclusion in the Mac OS X\nsystem. The major problem with message systems has generally been poor\nperformance caused by double copying of messages: the message is copied\n\ufb01rst from the sender to the mailbox and then from the mailbox to the receiver.\nThe Mach message system attempts to avoid double-copy operations by using\nvirtual-memory-management techniques (Chapter 9). Essentially, Mach maps\nthe address space containing the sender\u2019s message into the receiver\u2019s address\nspace. The message itself is never actually copied. This message-management\ntechnique provides a large performance boost but works for only intrasystem\nmessages. The Mach operating system is discussed in more detail in the online\nAppendix B.3.5 Examples of IPC Systems 135\n3.5.3 An Example: Windows\nThe Windows operating system is an example of modern design that employs\nmodularity to increase functionality and decrease the time needed to imple-\nment new features. Windows provides support for multiple operating envi-\nronments, or subsystems. Application programs communicate with these\nsubsystems via a message-passing mechanism. Thus, application programs\ncan be considered clients of a subsystem server.\nThe message-passing facility in Windows is called the advanced local\nprocedure call (ALPC) facility. It is used for communication between two\nprocesses on the same machine. It is similar to the standard remote procedure\ncall (RPC)m e c h a n i s mt h a ti sw i d e l yu s e d ,b u ti ti so p t i m i z e df o ra n ds p e c i \ufb01 c\nto Windows. (Remote procedure calls are covered in detail in Section 3.6.2.)\nLike Mach, Windows uses a port object to establish and maintain a connection\nbetween two processes. Windows uses two types of ports: connection ports\nand communication ports.\nServer processes publish connection-port objects that are visible to all\nprocesses. When a client wants services from a subsystem, it opens a handle to\nthe server\u2019s connection-port object and sendsac o n n e c t i o nr e q u e s tt ot h a tp o r t .\nThe server then creates a channel and returns a handle to the client. The channel\nconsists of a pair of private communication ports: one for client\u2014server\nmessages, the other for server\u2014client messages. Additionally, communication\nchannels support a callback mechanism that allows the client and server to\naccept requests when they would normally be expecting a reply.\nWhen an ALPC channel is created, one of three message-passing techniques\nis chosen:\n1. For small messages (up to 256 bytes), the port\u2019s message queue is used\nas intermediate storage, and the messages are copied from one process to\nthe other.\n2. Larger messages must be passed through a section object ,w h i c hi sa\nregion of shared memory associated with the channel.\n3. When the amount of data is too large to \ufb01t into a section object, an API is\navailable that allows server processes to read and write directly into the\naddress space of a client.\nThe client has to decide when it sets up the channel whether it will need\nto send a large message. If the client determines that it does want to send\nlarge messages, it asks for a section object to be created. Similarly, if the server\ndecides that replies will be large, it creates a section object. So that the section\nobject can be used, a small message is sent that contains a pointer and size\ninformation about the section object. This method is more complicated than\nthe \ufb01rst method listed above, but it a voids data copying. The structure of\nadvanced local procedure calls in Windows is shown in Figure 3.19.\nIt is important to note that the ALPC facility in Windows is not part of the\nWindows API and hence is not visible to the application programmer. Rather,\napplications using the Windows API invoke standard remote procedure calls.\nWhen the RPC is being invoked on a process on the same system, the RPC is\nhandled indirectly through anALPC.p r o c e d u r ec a l l .A d d i t i o n a l l y ,m a n yk e r n e l\nservices use ALPC to communicate with client processes.136 Chapter 3 Processes\nConnection\nPort\nConnection\nrequest Handle\nHandle\nHandle\nClient\nCommunication Port\nServer\nCommunication Port\nShared\nSection Object\n(> 256 bytes)\nServerClient\nFigure 3.19 Advanced local procedure calls in Windows.\n3.6 Communication in Client\u2013Server Systems\nIn Section 3.4, we described how processes can communicate using shared\nmemory and message passing. These techniques can be used for communica-\ntion in client\u2013server systems (Section 1.11.4) as well. In this section, we explore\nthree other strategies for communica tion in client\u2013server systems: sockets,\nremote procedure calls (RPCs), and pipes.\n3.6.1 Sockets\nA socket is de\ufb01ned as an endpoint for communication. A pair of processes\ncommunicating over a network employs a pair of sockets\u2014one for each\nprocess. A socket is identi\ufb01ed by an IP address concatenated with a port\nnumber. In general, sockets use a clien t\u2013server architecture. The server waits\nfor incoming client requests by listening to a speci\ufb01ed port. Once a request\nis received, the server accepts a connection from the client socket to complete\nthe connection. Servers implementing speci\ufb01c services (such as telnet,FTP,a n d\nHTTP)l i s t e nt ow e l l - k n o w np o r t s( at e l n e ts e r v e rl i s t e n st op o r t2 3 ;a nFTP\nserver listens to port 21; and a web, or HTTP,s e r v e rl i s t e n st op o r t8 0 ) .A l l\nports below 1024 are considered well known; we can use them to implement\nstandard services.\nWhen a client process initiates a request for a connection, it is assigned a\nport by its host computer. This port has some arbitrary number greater than\n1024. For example, if a client on host X with IP address 146.86.5.20 wishes to\nestablish a connection with a web server (which is listening on port 80) at\naddress 161.25.19.8, host X may be assigned port 1625. The connection will\nconsist of a pair of sockets: (146.86.5.20:1625) on host X and (161.25.19.8:80)\non the web server. This situation is illustrated in Figure 3.20. The packets\ntraveling between the hosts are delivered to the appropriate process based on\nthe destination port number.\nAll connections must be unique. Therefore, if another process also on host\nX wished to establish another connection with the same web server, it would be\nassigned a port number greater than 1024 and not equal to 1625. This ensures\nthat all connections consist of a unique pair of sockets.", "3.6 Communication in Client\u2013Server Systems 137\nsocket\n(146.86.5.20:1625)\nhost X\n(146.86.5.20)\nsocket\n(161.25.19.8:80)\nweb server\n(161.25.19.8)\nFigure 3.20 Communication using sockets.\nAlthough most program examples in this text use C, we will illustrate\nsockets using Java, as it provides a muc he a s i e ri n t e r f a c et os o c k e t sa n dh a sa\nrich library for networking utilities. Those interested in socket programming\nin C or C++ should consult the bibliographical notes at the end of the chapter.\nJava provides three different types of sockets. Connection-oriented (TCP)\nsockets are implemented with theSocket class. Connectionless (UDP)s o c k e t s\nuse theDatagramSocket class. Finally, theMulticastSocket class is a subclass\nof the DatagramSocket class. A multicast socket allows data to be sent to\nmultiple recipients.\nOur example describes a date server that uses connection-oriented TCP\nsockets. The operation allows clients to request the current date and time from\nthe server. The server listens to port 6013, although the port could have any\narbitrary number greater than 1024. When a connection is received, the server\nreturns the date and time to the client.\nThe date server is shown in Figure 3.21. The server creates aServerSocket\nthat speci\ufb01es that it will listen to port 6013. The server then begins listening\nto the port with the accept() method. The server blocks on the accept()\nmethod waiting for a client to request a connection. When a connection request\nis received, accept() returns a socket that the server can use to communicate\nwith the client.\nThe details of how the server communicates with the socket are as follows.\nThe server \ufb01rst establishes aPrintWriter object that it will use to communicate\nwith the client. A PrintWriter object allows the server to write to the socket\nusing the routine print() and println() methods for output. The server\nprocess sends the date to the client, calling the method println(). Once it\nhas written the date to the socket, the server closes the socket to the client and\nresumes listening for more requests.\nAc l i e n tc o m m u n i c a t e sw i t ht h es e r v e rb yc r e a t i n gas o c k e ta n dc o n n e c t i n g\nto the port on which the server is listening. We implement such a client in the\nJava program shown in Figure 3.22. The client creates a Socket and requests\nac o n n e c t i o nw i t ht h es e r v e ra tIP address 127.0.0.1 on port 6013. Once the\nconnection is made, the client can read from the socket using normal stream\nI/O statements. After it has received the date from the server, the client closes138 Chapter 3 Processes\nimport java.net.*;\nimport java.io.*;\npublic class DateServer\n{\npublic static void main(String[] args) {\ntry {\nServerSocket sock = new ServerSocket(6013);\n/* now listen for connections */\nwhile (true) {\nSocket client = sock.accept();\nPrintWriter pout = new\nPrintWriter(client.getOutputStream(), true);\n/* write the Date to the socket */\npout.println(new java.util.Date().toString());\n/* close the socket and resume */\n/* listening for connections */\nclient.close();\n}\n}\ncatch (IOException ioe) {\nSystem.err.println(ioe);\n}\n}\n}\nFigure 3.21 Date server.\nthe socket and exits. TheIP address 127.0.0.1 is a specialIP address known as the\nloopback.W h e nac o m p u t e rr e f e r st oIP address 127.0.0.1, it is referring to itself.\nThis mechanism allows a client and server on the same host to communicate\nusing the TCP/IP protocol. The IP address 127.0.0.1 could be replaced with the\nIP address of another host running the date server. In addition to anIP address,\nan actual host name, such as www.westminstercollege.edu,c a nb eu s e da s\nwell.\nCommunication using sockets\u2014although common and ef\ufb01cient\u2014is con-\nsidered a low-level form of communica tion between distributed processes.\nOne reason is that sockets allow only an unstructured stream of bytes to be\nexchanged between the communicating threads. It is the responsibility of the\nclient or server application to impose a structure on the data. In the next two\nsubsections, we look at two higher- level methods of communication: remote\nprocedure calls (RPCs) and pipes.\n3.6.2 Remote Procedure Calls\nOne of the most common forms of remote service is the RPC paradigm, which\nwe discussed brie\ufb02y in Section 3.5.2. The RPC was designed as a way to3.6 Communication in Client\u2013Server Systems 139\nimport java.net.*;\nimport java.io.*;\npublic class DateClient\n{\npublic static void main(String[] args) {\ntry {\n/* make connection to server socket */\nSocket sock = new Socket(\"127.0.0.1\",6013);\nInputStream in = sock.getInputStream();\nBufferedReader bin = new\nBufferedReader(new InputStreamReader(in));\n/* read the date from the socket */\nString line;\nwhile ( (line = bin.readLine()) != null)\nSystem.out.println(line);\n/* close the socket connection*/\nsock.close();\n}\ncatch (IOException ioe) {\nSystem.err.println(ioe);\n}\n}\n}\nFigure 3.22 Date client.\nabstract the procedure-call mechanism for use between systems with network\nconnections. It is similar in many respects to the IPC mechanism described in\nSection 3.4, and it is usually built on top of such a system. Here, however,\nbecause we are dealing with an environment in which the processes are\nexecuting on separate systems, we must use a message-based communication\nscheme to provide remote service.\nIn contrast toIPC messages, the messages exchanged inRPC communication\nare well structured and are thus no longer just packets of data. Each message is\naddressed to anRPC daemon listening to a port on the remote system, and each\ncontains an identi\ufb01er specifying the function to execute and the parameters\nto pass to that function. The function is then executed as requested, and any\noutput is sent back to the requester in a separate message.\nA port is simply a number included at the start of a message packet.\nWhereas a system normally has one network address, it can have many ports\nwithin that address to differentiate the many network services it supports. If a\nremote process needs a service, it addresses a message to the proper port. For\ninstance, if a system wished to allow other systems to be able to list its current\nusers, it would have a daemon supporting such an RPC attached to a port\u2014\nsay, port 3027. Any remote system could obtain the needed information (that", "140 Chapter 3 Processes\nis, the list of current users) by sending an RPC message to port 3027 on the\nserver. The data would be received in a reply message.\nThe semantics of RPCsa l l o w sac l i e n tt oi n v o k eap r o c e d u r eo nar e m o t e\nhost as it would invoke a procedure locally. The RPC system hides the details\nthat allow communication to take place by providing a stub on the client side.\nTypically, a separate stub exists for each separate remote procedure. When the\nclient invokes a remote procedure, the RPC system calls the appropriate stub,\npassing it the parameters provided to the remote procedure. This stub locates\nthe port on the server and marshals the parameters. Parameter marshalling\ninvolves packaging the parameters into a form that can be transmitted over\nan e t w o r k .T h es t u bt h e nt r a n s m i t sam e s s a g et ot h es e r v e ru s i n gm e s s a g e\npassing. A similar stub on the server side receives this message and invokes\nthe procedure on the server. If necessary, return values are passed back to the\nclient using the same technique. On Windows systems, stub code is compiled\nfrom a speci\ufb01cation written in the Microsoft Interface De\ufb01nition Language\n(MIDL),w h i c hi su s e df o rd e \ufb01 n i n gt h ei n t e r f a c e sb e t w e e nc l i e n ta n ds e r v e r\nprograms.\nOne issue that must be dealt with concernsd i f f e r e n c e si nd a t ar e p r e s e n t a -\ntion on the client and server machines. Consider the representation of 32-bit\nintegers. Some systems (known as big-endian)s t o r et h em o s ts i g n i \ufb01 c a n tb y t e\n\ufb01rst, while other systems (known as little-endian)s t o r et h el e a s ts i g n i \ufb01 c a n t\nbyte \ufb01rst. Neither order is \u201cbetter\u201d per se; rather, the choice is arbitrary within\nac o m p u t e ra r c h i t e c t u r e .T or e s o l v ed i f f e r e n c e sl i k et h i s ,m a n yRPC systems\nde\ufb01ne a machine-independent representation of data. One such representation\nis known as external data representation (XDR).O nt h ec l i e n ts i d e ,p a r a m e t e r\nmarshalling involves converting the machine-dependent data into XDR before\nthey are sent to the server. On the server side, the XDR data are unmarshalled\nand converted to the machine-dependent representation for the server.\nAnother important issue involves the semantics of a call. Whereas local\nprocedure calls fail only under extreme circumstances, RPCsc a nf a i l ,o rb e\nduplicated and executed more than on ce, as a result of common network\nerrors. One way to address this problem is for the operating system to ensure\nthat messages are acted on exactly once, rather than at most once. Most local\nprocedure calls have the \u201cexactly once\u201d functionality, but it is more dif\ufb01cult to\nimplement.\nFirst, consider \u201cat most once. \u201d This semantic can be implemented by\nattaching a timestamp to each message. The server must keep a history of\nall the timestamps of messages it has already processed or a history large\nenough to ensure that repeated messages are detected. Incoming messages\nthat have a timestamp already in the history are ignored. The client can then\nsend a message one or more times and be assured that it only executes once.\nFor \u201cexactly once,\u201d we need to remove the risk that the server will never\nreceive the request. To accomplish this, the server must implement the \u201cat\nmost once\u201d protocol described above but must also acknowledge to the client\nthat the RPC call was received and executed. TheseACK messages are common\nthroughout networking. The client must resend eachRPC call periodically until\nit receives the ACK for that call.\nYet another important issue concernsthe communication between a server\nand a client. With standard procedure calls, some form of binding takes place\nduring link, load, or execution time (Chapter 8) so that a procedure call\u2019s name3.6 Communication in Client\u2013Server Systems 141\nclient\nuser calls kernel\nto send RPC\nmessage to\nprocedure X\nmatchmaker\nreceives\nmessage, looks\nup answer\nmatchmaker\nreplies to client\nwith port P\ndaemon\nlistening to\nport P receives\nmessage\ndaemon\nprocesses\nrequest and\nprocesses send\noutput\nkernel sends\nmessage to\nmatchmaker to\nfind port number\nFrom: client\nTo: server\nPort: matchmaker\nRe: address\nfor RPC X\nFrom: client\nTo: server\nPort: port P\n<contents>\nFrom: RPC\nPort: P\nTo: client\nPort: kernel\n<output>\nFrom: server\nTo: client\nPort: kernel\nRe: RPC X\nPort: P\nkernel places\nport P in user\nRPC message\nkernel sends\nRPC\nkernel receives\nreply, passes\nit to user\nmessages server\nFigure 3.23 Execution of a remote procedure call (RPC).\nis replaced by the memory address of the procedure call. The RPC scheme\nrequires a similar binding of the client and the server port, but how does a client\nknow the port numbers on the server? Neither system has full information\nabout the other, because they do not share memory.\nTwo approaches are common. First, the binding information may be\npredetermined, in the form of \ufb01xed port addresses. At compile time, an RPC\ncall has a \ufb01xed port number associated with it. Once a program is compiled,\nthe server cannot change the port nu mber of the requested service. Second,\nbinding can be done dynamically by a rendezvous mechanism. Typically, an\noperating system provides a rendezvous (also called a matchmaker)d a e m o n\non a \ufb01xed RPC port. A client then sends a message containing the name of\nthe RPC to the rendezvous daemon requesting the port address of the RPC it\nneeds to execute. The port number is returned, and the RPC calls can be sent\nto that port until the process terminates (or the server crashes). This method\nrequires the extra overhead of the initial request but is more \ufb02exible than the\n\ufb01rst approach. Figure 3.23 shows a sample interaction.\nThe RPC scheme is useful in implementing a distributed \ufb01le system\n(Chapter 17). Such a system can be implemented as a set of RPC daemons142 Chapter 3 Processes\nand clients. The messages are addressed to the distributed \ufb01le system port on a\nserver on which a \ufb01le operation is to take place. The message contains the disk\noperation to be performed. The disk operation might be read, write, rename,\ndelete,o r status, corresponding to the usual \ufb01le-related system calls. The\nreturn message contains any data resulting from that call, which is executed by\nthe DFS daemon on behalf of the client. For instance, a message might contain\nar e q u e s tt ot r a n s f e raw h o l e\ufb01 l et oac l i e n to rb el i m i t e dt oas i m p l eb l o c k\nrequest. In the latter case, several requests may be needed if a whole \ufb01le is to\nbe transferred.\n3.6.3 Pipes\nA pipe acts as a conduit allowing two processes to communicate. Pipes were\none of the \ufb01rst IPC mechanisms in early UNIX systems. They typically provide\none of the simpler ways for processes to communicate with one another,\nalthough they also have some limitations. In implementing a pipe, four issues\nmust be considered:\n1. Does the pipe allow bidirectional communication, or is communication\nunidirectional?\n2. If two-way communication is allowed, is it half duplex (data can travel\nonly one way at a time) or full duplex (data can travel in both directions\nat the same time)?\n3. Must a relationship (such as parent\u2013child)e x i s tb e t w e e nt h ec o m m u n i -\ncating processes?\n4. Can the pipes communicate over a network, or must the communicating\nprocesses reside on the same machine?\nIn the following sections, we explore two common types of pipes used on both\nUNIX and Windows systems: ordinary pipes and named pipes.\n3.6.3.1 Ordinary Pipes\nOrdinary pipes allow two processes to communicate in standard producer\u2013\nconsumer fashion: the producer writes to one end of the pipe (the write-end)\nand the consumer reads from the other end (theread-end). As a result, ordinary\npipes are unidirectional, allowing only one-way communication. If two-way\ncommunication is required, two pipes must be used, with each pipe sending\ndata in a different direction. We next illustrate constructing ordinary pipes\non both UNIX and Windows systems. In both program examples, one process\nwrites the message Greetings to the pipe, while the other process reads this\nmessage from the pipe.\nOn UNIX systems, ordinary pipes are constructed using the function\npipe(int fd[])\nThis function creates a pipe that is accessed through the int fd[] \ufb01le\ndescriptors: fd[0] is the read-end of the pipe, and fd[1] is the write-end.", "3.6 Communication in Client\u2013Server Systems 143\nparent\nfd(0) fd(1)\nchild\nfd(0) fd(1)\npipe\nFigure 3.24 File descriptors for an ordinary pipe.\nUNIX treats a pipe as a special type of \ufb01le. Thus, pipes can be accessed using\nordinary read() and write() system calls.\nAn ordinary pipe cannot be accessed from outside the process that created\nit. Typically, a parent process creates a pipe and uses it to communicate with\nac h i l dp r o c e s st h a ti tc r e a t e sv i afork(). Recall from Section 3.3.1 that a child\nprocess inherits open \ufb01les from its parent. Since a pipe is a special type of \ufb01le,\nthe child inherits the pipe from its parent process. Figure 3.24 illustrates the\nrelationship of the \ufb01le descriptor fd to the parent and child processes.\nIn the UNIX program shown in Figure 3.25, the parent process creates a\npipe and then sends afork() call creating the child process. What occurs after\nthe fork() call depends on how the data are to \ufb02ow through the pipe. In\nthis instance, the parent writes to the pipe, and the child reads from it. It is\nimportant to notice that both the parent process and the child process initially\nclose their unused ends of the pipe. Although the program shown in Figure\n3.25 does not require this action, it is an important step to ensure that a process\nreading from the pipe can detect end-of-\ufb01le (read() returns 0) when the writer\nhas closed its end of the pipe.\nOrdinary pipes on Windows systems are termed anonymous pipes,a n d\nthey behave similarly to their UNIX counterparts: they are unidirectional and\n#include <sys/types.h>\n#include <stdio.h>\n#include <string.h>\n#include <unistd.h>\n#define BUFFER\n SIZE 25\n#define READ\n END 0\n#define WRITE\n END 1\nint main(void)\n{\nchar write\n msg[BUFFER\n SIZE] = \"Greetings\";\nchar read\n msg[BUFFER\n SIZE];\nint fd[2];\npid\n tp i d ;\n/* Program continues in Figure 3.26 */\nFigure 3.25 Ordinary pipe in UNIX.144 Chapter 3 Processes\n/* create the pipe */\nif (pipe(fd) == -1) {\nfprintf(stderr,\"Pipe failed\");\nreturn 1;\n}\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nif (pid > 0) { /* parent process */\n/* close the unused end of the pipe */\nclose(fd[READ\n END]);\n/* write to the pipe */\nwrite(fd[WRITE\n END], write\n msg, strlen(write\n msg)+1);\n/* close the write end of the pipe */\nclose(fd[WRITE\n END]);\n}\nelse { /* child process */\n/* close the unused end of the pipe */\nclose(fd[WRITE\n END]);\n/* read from the pipe */\nread(fd[READ\n END], read\n msg, BUFFER\n SIZE);\nprintf(\"read %s\",read\n msg);\n/* close the write end of the pipe */\nclose(fd[READ\n END]);\n}\nreturn 0;\n}\nFigure 3.26 Figure 3.25, continued.\nemploy parent\u2013child relationships betw een the communicating processes.\nIn addition, reading and writing to the pipe can be accomplished with the\nordinary ReadFile() and WriteFile() functions. The Windows API for\ncreating pipes is theCreatePipe() function, which is passed four parameters.\nThe parameters provide separate handles for (1) reading and (2) writing to the\npipe, as well as (3) an instance of the STARTUPINFO structure, which is used to\nspecify that the child process is to inherit the handles of the pipe. Furthermore,\n(4) the size of the pipe (in bytes) may be speci\ufb01ed.\nFigure 3.27 illustrates a parent process creating an anonymous pipe for\ncommunicating with its child. Unlike UNIX systems, in which a child process3.6 Communication in Client\u2013Server Systems 145\n#include <stdio.h>\n#include <stdlib.h>\n#include <windows.h>\n#define BUFFER\n SIZE 25\nint main(VOID)\n{\nHANDLE ReadHandle, WriteHandle;\nSTARTUPINFO si;\nPROCESS\n INFORMATION pi;\nchar message[BUFFER\n SIZE] = \"Greetings\";\nDWORD written;\n/* Program continues in Figure 3.28 */\nFigure 3.27 Windows anonymous pipe\u2014parent process.\nautomatically inherits a pipe created by its parent, Windows requires the\nprogrammer to specify which attributes the child process will inherit. This is\naccomplished by \ufb01rst initializing the SECURITY\n ATTRIBUTES structure to allow\nhandles to be inherited and then redire cting the child process\u2019s handles for\nstandard input or standard output to the read or write handle of the pipe.\nSince the child will be reading from the pipe, the parent must redirect the\nchild\u2019s standard input to the read handle of the pipe. Furthermore, as the\npipes are half duplex, it is necessary to prohibit the child from inheriting the\nwrite-end of the pipe. The program to create the child process is similar to the\nprogram in Figure 3.11, except that the \ufb01fth parameter is set toTRUE,i n d i c a t i n g\nthat the child process is to inherit d esignated handles from its parent. Before\nwriting to the pipe, the parent \ufb01rst closes its unused read end of the pipe. The\nchild process that reads from the pipe is shown in Figure 3.29. Before reading\nfrom the pipe, this program obtains the read handle to the pipe by invoking\nGetStdHandle().\nNote that ordinary pipes require a parent\u2013child relationship between the\ncommunicating processes on both UNIX and Windows systems. This means\nthat these pipes can be used only for communication between processes on the\nsame machine.\n3.6.3.2 Named Pipes\nOrdinary pipes provide a simple mechanism for allowing a pair of processes\nto communicate. However, ordinary pipes exist only while the processes are\ncommunicating with one another. On both UNIX and Windows systems, once\nthe processes have \ufb01nished communicating andhave terminated, the ordinary\npipe ceases to exist.\nNamed pipes provide a much more powerful communication tool. Com-\nmunication can be bidirectional, and no parent\u2013child relationship is required.\nOnce a named pipe is established, several processes can use it for communi-\ncation. In fact, in a typical scenario, a named pipe has several writers. Addi-\ntionally, named pipes continue to exist after communicating processes have", "146 Chapter 3 Processes\n/* set up security attributes allowing pipes to be inherited */\nSECURITY\n ATTRIBUTES sa = {sizeof(SECURITY\n ATTRIBUTES),NULL,TRUE};\n/* allocate memory */\nZeroMemory(&pi, sizeof(pi));\n/* create the pipe */\nif (!CreatePipe(&ReadHandle, &WriteHandle, &sa, 0)) {\nfprintf(stderr, \"Create Pipe Failed\");\nreturn 1;\n}\n/* establish the START\n INFO structure for the child process */\nGetStartupInfo(&si);\nsi.hStdOutput = GetStdHandle(STD\n OUTPUT\n HANDLE);\n/* redirect standard input to the read end of the pipe */\nsi.hStdInput = ReadHandle;\nsi.dwFlags = STARTF\n USESTDHANDLES;\n/* don\u2019t allow the child to inherit the write end of pipe */\nSetHandleInformation(WriteHandle, HANDLE\n FLAG\n INHERIT,0 ) ;\n/* create the child process */\nCreateProcess(NULL,\" c h i l d . e x e \" ,NULL, NULL,\nTRUE, /* inherit handles */\n0, NULL, NULL,& s i ,& p i ) ;\n/* close the unused end of the pipe */\nCloseHandle(ReadHandle);\n/* the parent writes to the pipe */\nif (!WriteFile(WriteHandle, message,BUFFER\n SIZE,&written,NULL))\nfprintf(stderr, \"Error writing to pipe.\");\n/* close the write end of the pipe */\nCloseHandle(WriteHandle);\n/* wait for the child to exit */\nWaitForSingleObject(pi.hProcess, INFINITE);\nCloseHandle(pi.hProcess);\nCloseHandle(pi.hThread);\nreturn 0;\n}\nFigure 3.28 Figure 3.27, continued.\n\ufb01nished. Both UNIX and Windows systems support named pipes, although the\ndetails of implementation differ greatly. Next, we explore named pipes in each\nof these systems.3.7 Summary 147\n#include <stdio.h>\n#include <windows.h>\n#define BUFFER\n SIZE 25\nint main(VOID)\n{\nHANDLE Readhandle;\nCHAR buffer[BUFFER\n SIZE];\nDWORD read;\n/* get the read handle of the pipe */\nReadHandle = GetStdHandle(STD\n INPUT\n HANDLE);\n/* the child reads from the pipe */\nif (ReadFile(ReadHandle, buffer, BUFFER\n SIZE,& r e a d ,NULL))\nprintf(\"child read %s\",buffer);\nelse\nfprintf(stderr, \"Error reading from pipe\");\nreturn 0;\n}\nFigure 3.29 Windows anonymous pipes\u2014child process.\nNamed pipes are referred to as FIFOsi n UNIX systems. Once created, they\nappear as typical \ufb01les in the \ufb01le system. A FIFO is created with the mkfifo()\nsystem call and manipulated with the ordinary open(), read(), write(),\nand close() system calls. It will continue to exist until it is explicitly deleted\nfrom the \ufb01le system. Although FIFOsa l l o wb i d i r e c t i o n a lc o m m u n i c a t i o n ,o n l y\nhalf-duplex transmission is permitted. If data must travel in both directions,\ntwo FIFOsa r et y p i c a l l yu s e d .A d d i t i o n a l l y ,t h ec o m m u n i c a t i n gp r o c e s s e sm u s t\nreside on the same machine. If intermachine communication is required,\nsockets (Section 3.6.1) must be used.\nNamed pipes on Windows systems provide a richer communication mech-\nanism than their UNIX counterparts. Full-duplex communication is allowed,\nand the communicating processes may reside on either the same or different\nmachines. Additionally, only byte-oriented data may be transmitted across a\nUNIX FIFO,w h e r e a sW i n d o w ss y s t e m sa l l o we i t h e rb y t e -o rm e s s a g e - o r i e n t e d\ndata. Named pipes are created with the CreateNamedPipe() function, and a\nclient can connect to a named pipe using ConnectNamedPipe().C o m m u n i -\ncation over the named pipe can be accomplished using the ReadFile() and\nWriteFile() functions.\n3.7 Summary\nAp r o c e s si sap r o g r a mi ne x e c u t i o n .A sap r o c e s se x e c u t e s ,i tc h a n g e ss t a t e .T h e\nstate of a process is de\ufb01ned by that process\u2019s current activity. Each process may\nbe in one of the following states: new, ready, running, waiting, or terminated.148 Chapter 3 Processes\nPIPES IN PRACTICE\nPipes are used quite often in the UNIX command-line environment for\nsituations in which the output of one command serves as input to another. For\nexample, the UNIX ls command produces a directory listing. For especially\nlong directory listings, the output may scroll through several screens. The\ncommand more manages output by displaying only one screen of output at\nat i m e ;t h eu s e rm u s tp r e s st h es p a c eb a rt om o v ef r o mo n es c r e e nt ot h en e x t .\nSetting up a pipe between thels and more commands (which are running as\nindividual processes) allows the output of ls to be delivered as the input to\nmore,e n a b l i n gt h eu s e rt od i s p l a yal a r g ed i r e c t o r yl i s t i n gas c r e e na tat i m e .\nAp i p ec a nb ec o n s t r u c t e do nt h ec o m m a n dl i n eu s i n gt h e| character. The\ncomplete command is\nls | more\nIn this scenario, the ls command serves as the producer, and its output is\nconsumed by the more command.\nWindows systems provide a more command for the DOS shell with\nfunctionality similar to that of its UNIX counterpart. The DOS shell also uses\nthe | character for establishing a pipe. The only difference is that to get\nad i r e c t o r yl i s t i n g ,DOS uses the dir command rather than ls,a ss h o w n\nbelow:\ndir | more\nEach process is represented in the operating system by its own process control\nblock (PCB).\nAp r o c e s s ,w h e ni ti sn o te x e c u t i n g ,i sp l a c e di ns o m ew a i t i n gq u e u e .T h e r e\nare two major classes of queues in an operating system: I/O request queues\nand the ready queue. The ready queue contains all the processes that are ready\nto execute and are waiting for the CPU.E a c hp r o c e s si sr e p r e s e n t e db yaPCB.\nThe operating system must select processes from various scheduling\nqueues. Long-term (job) scheduling is the selection of processes that will be\nallowed to contend for the CPU.N o r m a l l y ,l o n g - t e r ms c h e d u l i n gi sh e a v i l y\nin\ufb02uenced by resource-allocation considerations, especially memory manage-\nment. Short-term ( CPU)s c h e d u l i n gi st h es e l e c t i o no fo n ep r o c e s sf r o mt h e\nready queue.\nOperating systems must provide a mechanism for parent processes to\ncreate new child processes. The parent may wait for its children to terminate\nbefore proceeding, or the parent and children may execute concurrently. There\nare several reasons for allowing conc urrent execution: information sharing,\ncomputation speedup, modularity, and convenience.\nThe processes executing in the operating system may be either independent\nprocesses or cooperating processes. Cooperating processes require an interpro-\ncess communication mechanism to communicate with each other. Principally,\ncommunication is achieved through two schemes: shared memory and mes-\nsage passing. The shared-memory method requires communicating processes", "Practice Exercises 149\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint value = 5;\nint main()\n{\npid\n tp i d ;\npid = fork();\nif (pid == 0) { /* child process */\nvalue += 15;\nreturn 0;\n}\nelse if (pid > 0) { /* parent process */\nwait(NULL);\nprintf(\"PARENT: value = %d\",value); /* LINE A */\nreturn 0;\n}\n}\nFigure 3.30 What output will be at Line A?\nto share some variables. The processes are expected to exchange information\nthrough the use of these shared variables. In a shared-memory system, the\nresponsibility for providing communication rests with the application pro-\ngrammers; the operating system needs to provide only the shared memory.\nThe message-passing method allows the processes to exchange messages.\nThe responsibility for providing communication may rest with the operating\nsystem itself. These two schemes are not mutually exclusive and can be used\nsimultaneously within a single operating system.\nCommunication in client\u2013server systems may use (1) sockets, (2) remote\nprocedure calls ( RPCs), or (3) pipes. A socket is de\ufb01ned as an endpoint for\ncommunication. A connection between a pair of applications consists of a pair\nof sockets, one at each end of the communication channel. RPCsa r ea n o t h e r\nform of distributed communication. An RPC occurs when a process (or thread)\ncalls a procedure on a remote application. Pipes provide a relatively simple\nways for processes to communicate with one another. Ordinary pipes allow\ncommunication between parent and child processes, while named pipes permit\nunrelated processes to communicate.\nPractice Exercises\n3.1 Using the program shown in Figure 3.30, explain what the output will\nbe at LINE A.\n3.2 Including the initial parent process, how many processes are created by\nthe program shown in Figure 3.31?150 Chapter 3 Processes\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\n/* fork a child process */\nfork();\n/* fork another child process */\nfork();\n/* and fork another */\nfork();\nreturn 0;\n}\nFigure 3.31 How many processes are created?\n3.3 Original versions of Apple\u2019s mobile iOS operating system provided no\nmeans of concurrent processing. Discuss three major complications that\nconcurrent processing adds to an operating system.\n3.4 The Sun UltraSPARC processor has multiple register sets. Describe what\nhappens when a context switch occurs if the new context is already\nloaded into one of the register sets. What happens if the new context is\nin memory rather than in a register set and all the register sets are in\nuse?\n3.5 When a process creates a new process using thefork() operation, which\nof the following states is shared betweenthe parent process and the child\nprocess?\na. Stack\nb. Heap\nc. Shared memory segments\n3.6 Consider the\u201cexactly once\u201dsemantic with respect to theRPC mechanism.\nDoes the algorithm for implementing this semantic execute correctly\neven if the ACK message sent back to the client is lost due to a network\nproblem? Describe the sequence of messages, and discuss whether\n\u201cexactly once\u201d is still preserved.\n3.7 Assume that a distributed system is susceptible to server failure. What\nmechanisms would be required to guarantee the\u201cexactly once\u201d semantic\nfor execution of RPCs?\nExercises\n3.8 Describe the differences among short-term, medium-term, and long-\nterm scheduling.Exercises 151\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\nint i;\nfor (i = 0; i < 4; i++)\nfork();\nreturn 0;\n}\nFigure 3.32 How many processes are created?\n3.9 Describe the actions taken by a kernel to context-switch between\nprocesses.\n3.10 Construct a process tree similar to Figure 3.8. To obtain process infor-\nmation for the UNIX or Linux system, use the command ps -ael.\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\npid\n tp i d ;\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0) { /* child process */\nexeclp(\"/bin/ls\",\"ls\",NULL);\nprintf(\"LINE J\");\n}\nelse { /* parent process */\n/* parent will wait for the child to complete */\nwait(NULL);\nprintf(\"Child Complete\");\n}\nreturn 0;\n}\nFigure 3.33 When willLINE J be reached?", "152 Chapter 3 Processes\nUse the command man ps to get more information about the ps com-\nmand. The task manager on Windows systems does not provide the\nparent process ID,b u tt h eprocess monitor tool, available from tech-\nnet.microsoft.com,p r o v i d e sap r o c e s s - t r e et o o l .\n3.11 Explain the role of theinit process onUNIX and Linux systems in regard\nto process termination.\n3.12 Including the initial parent process, how many processes are created by\nthe program shown in Figure 3.32?\n3.13 Explain the circumstances under which which the line of code marked\nprintf(\"LINE J\") in Figure 3.33 will be reached.\n3.14 Using the program in Figure 3.34, identify the values ofpid at lines A, B,\nC,a n dD.( A s s u m et h a tt h ea c t u a lp i d so ft h ep a r e n ta n dc h i l da r e2 6 0 0\nand 2603, respectively.)\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\npid\n tp i d ,p i d 1 ;\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0) { /* child process */\npid1 = getpid();\nprintf(\"child: pid = %d\",pid); /* A */\nprintf(\"child: pid1 = %d\",pid1); /* B */\n}\nelse { /* parent process */\npid1 = getpid();\nprintf(\"parent: pid = %d\",pid); /* C */\nprintf(\"parent: pid1 = %d\",pid1); /* D */\nwait(NULL);\n}\nreturn 0;\n}\nFigure 3.34 What are the pid values?Exercises 153\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\n#define SIZE 5\nint nums[SIZE] = {0,1,2,3,4};\nint main()\n{\nint i;\npid\n tp i d ;\npid = fork();\nif (pid == 0) {\nfor (i = 0; i < SIZE; i++) {\nnums[i] *= -i;\nprintf(\"CHILD: %d \",nums[i]); /* LINE X */\n}\n}\nelse if (pid > 0) {\nwait(NULL);\nfor (i = 0; i < SIZE; i++)\nprintf(\"PARENT: %d \",nums[i]); /* LINE Y */\n}\nreturn 0;\n}\nFigure 3.35 What output will be at Line X and Line Y?\n3.15 Give an example of a situation in which ordinary pipes are more suitable\nthan named pipes and an example of a situation in which named pipes\nare more suitable than ordinary pipes.\n3.16 Consider the RPC mechanism. Describe the undesirable consequences\nthat could arise from not enforcing either the \u201cat most once\u201d or \u201cexactly\nonce\u201d semantic. Describe possible uses for a mechanism that has neither\nof these guarantees.\n3.17 Using the program shown in Figure 3.35, explain what the output will\nbe at lines X and Y.\n3.18 What are the bene\ufb01ts and the disadvantages of each of the following?\nConsider both the system level and the programmer level.\na. Synchronous and asynchronous communication\nb. Automatic and explicit buffering\nc. Send by copy and send by reference\nd. Fixed-sized and variable-sized messages154 Chapter 3 Processes\nProgramming Problems\n3.19 Using either a UNIX or a Linux system, write a C program that forks\nac h i l dp r o c e s st h a tu l t i m a t e l yb e c o m e saz o m b i ep r o c e s s .T h i sz o m b i e\nprocess must remain in the system for at least 10 seconds. Process states\ncan be obtained from the command\nps -l\nThe process states are shown below theS column; processes with a state\nof Z are zombies. The process identi\ufb01er (pid) of the child process is listed\nin the PID column, and that of the parent is listed in the PPID column.\nPerhaps the easiest way to determine that the child process is indeed\naz o m b i ei st or u nt h ep r o g r a mt h a ty o uh a v ew r i t t e ni nt h eb a c k g r o u n d\n(using the &)a n dt h e nr u nt h ec o m m a n dps -l to determine whether\nthe child is a zombie process. Because you do not want too many zombie\nprocesses existing in the system, you will need to remove the one that\nyou have created. The easiest way to do that is to terminate the parent\nprocess using the kill command. For example, if the process id of the\nparent is 4884, you would enter\nkill -9 4884\n3.20 An operating system\u2019spid manager is responsible for managing process\nidenti\ufb01ers. When a process is \ufb01rst created, it is assigned a unique pid\nby the pid manager. The pid is returned to the pid manager when the\nprocess completes execution, and the manager may later reassign this\npid. Process identi\ufb01ers are discussed more fully in Section 3.3.1. What\nis most important here is to recognize that process identi\ufb01ers must be\nunique; no two active processes can have the same pid.\nUse the following constants to identify the range of possible pid values:\n#define MIN\n PID 300\n#define MAX\n PID 5000\nYou may use any data structure of your choice to represent the avail-\nability of process identi\ufb01ers. One strategy is to adopt what Linux has\ndone and use a bitmap in which a value of 0 at position i indicates that\nap r o c e s si do fv a l u ei is available and a value of 1 indicates that the\nprocess id is currently in use.\nImplement the following API for obtaining and releasing a pid:\n\u2022 int allocate\n map(void)\u2014Createsandinitializesadatastructure\nfor representing pids; returns\u20141 if unsuccessful, 1 if successful\n\u2022 int allocate\n pid(void)\u2014Allocatesandreturnsapid;returns\u2014\n1i fu n a b l et oa l l o c a t eap i d( a l lp i d sa r ei nu s e )\n\u2022 void release\n pid(int pid)\u2014Releases a pid\nThis programming problem will be modi\ufb01ed later on in Chpaters 4 and\n5.", "Programming Problems 155\n3.21 The Collatz conjecture concerns what happens when we take any\npositive integer n and apply the following algorithm:\nn =\n{\nn/2, if n is even\n3 \u00d7 n + 1, if n is odd\nThe conjecture states that when this algorithm is continually applied,\nall positive integers will eventually reach 1. For example, if n = 35, the\nsequence is\n35, 106, 53, 160, 80, 40, 20, 10, 5, 16, 8, 4, 2, 1\nWrite a C program using the fork() system call that generates this\nsequence in the child process. The starting number will be provided\nfrom the command line. For example, if 8 is passed as a parameter on\nthe command line, the child process will output 8 , 4, 2, 1. Because the\nparent and child processes have their own copies of the data, it will be\nnecessary for the child to output the sequence. Have the parent invoke\nthe wait() call to wait for the child process to complete before exiting\nthe program. Perform necessary error checking to ensure that a positive\ninteger is passed on the command line.\n3.22 In Exercise 3.21, the child processmust output the sequence of numbers\ngenerated from the algorithm speci\ufb01ed by the Collatz conjecture because\nthe parent and child have their own copies of the data. Another\napproach to designing this program is to establish a shared-memory\nobject between the parent and child processes. This technique allows the\nchild to write the contents of thes e q u e n c et ot h es h a r e d - m e m o r yo b j e c t .\nThe parent can then output the sequence when the child completes.\nBecause the memory is shared, any changes the child makes will be\nre\ufb02ected in the parent process as well.\nThis program will be structured using POSIX shared memory as\ndescribed in Section 3.5.1. The parent process will progress through the\nfollowing steps:\na. Establish the shared-memory object ( shm\n open(), ftruncate(),\nand mmap()).\nb. Create the child process and wait for it to terminate.\nc. Output the contents of shared memory.\nd. Remove the shared-memory object.\nOne area of concern with c ooperating processes involves synchro-\nnization issues. In this exercise, the parent and child processes must be\ncoordinated so that the parent does not output the sequence until the\nchild \ufb01nishes execution. These two processes will be synchronized using\nthe wait() system call: the parent process will invoke wait(),w h i c h\nwill suspend it until the child process exits.\n3.23 Section 3.6.1 describes port numbers below 1024 as being well known\u2014\nthat is, they provide standard services. Port 17 is known as the quote-of-156 Chapter 3 Processes\nthe-day service. When a client connects to port 17 on a server, the server\nresponds with a quote for that day.\nModify the date server shown in Figure 3.21 so that it delivers a quote\nof the day rather than the current date. The quotes should be printable\nASCII characters and should contain fewer than 512 characters, although\nmultiple lines are allowed. Since port 17 is well known and therefore\nunavailable, have your server listen to port 6017. The date client shown\nin Figure 3.22 can be used to read the quotes returned by your server.\n3.24 A haiku is a three-line poem in which the \ufb01rst line contains \ufb01ve syllables,\nthe second line contains seven syllables, and the third line contains \ufb01ve\nsyllables. Write a haiku server that listens to port 5575. When a client\nconnects to this port, the server responds with a haiku. The date client\nshown in Figure 3.22 can be used to read the quotes returned by your\nhaiku server.\n3.25 An echo server echoes back whatever it receives from a client. For\nexample, if a client sends the server the stringHello there!,t h es e r v e r\nwill respond with Hello there!\nWrite an echo server using the Java networking API described in\nSection 3.6.1. This server will wait for a client connection using the\naccept() method. When a client connection is received, the server will\nloop, performing the following steps:\n\u2022 Read data from the socket into a buffer.\n\u2022 Write the contents of the buffer back to the client.\nThe server will break out of the loop only when it has determined that\nthe client has closed the connection.\nThe date server shown in Figure 3.21 uses the\njava.io.BufferedReader class. BufferedReader extends the\njava.io.Reader class, which is used for reading character streams.\nHowever, the echo server cannot guarantee that it will read\ncharacters from clients; it may receive binary data as well. The\nclass java.io.InputStream deals with data at the byte level rather\nthan the character level. Thus, your echo server must use an object\nthat extends java.io.InputStream.T h e read() method in the\njava.io.InputStream class returns \u22121w h e nt h ec l i e n th a sc l o s e di t s\nend of the socket connection.\n3.26 Design a program using ordinary pipes in which one process sends a\nstring message to a second process, and the second process reverses\nthe case of each character in the message and sends it back to the \ufb01rst\nprocess. For example, if the \ufb01rst process sends the messageHi There,t h e\nsecond process will return hI tHERE.T h i sw i l lr e q u i r eu s i n gt w op i p e s ,\none for sending the original message from the \ufb01rst to the second process\nand the other for sending the modi\ufb01ed message from the second to the\n\ufb01rst process. You can write this program using either UNIX or Windows\npipes.\n3.27 Design a \ufb01le-copying program named filecopy using ordinary pipes.\nThis program will be passed two parameters: the name of the \ufb01le to beProgramming Projects 157\ncopied and the name of the copied \ufb01le. The program will then create\nan ordinary pipe and write the contents of the \ufb01le to be copied to the\npipe. The child process will read this \ufb01le from the pipe and write it to\nthe destination \ufb01le. For example, if we invoke the program as follows:\nfilecopy input.txt copy.txt\nthe \ufb01le input.txt will be written to the pipe. The child process will\nread the contents of this \ufb01le and write it to the destination \ufb01lecopy.txt.\nYou may write this program using either UNIX or Windows pipes.\nProgramming Projects\nProject 1\u2014UNIX Shell and History Feature\nThis project consists of designing a C p rogram to serve as a shell interface\nthat accepts user commands and t hen executes each command in a separate\nprocess. This project can be completed on any Linux,UNIX,o rM a cOS X system.\nAs h e l li n t e r f a c eg i v e st h eu s e rap r o m p t ,a f t e rw h i c ht h en e x tc o m m a n d\nis entered. The example below illustrates the prompt osh> and the user\u2019s\nnext command: cat prog.c. (This command displays the \ufb01le prog.c on the\nterminal using the UNIX cat command.)\nosh> cat prog.c\nOne technique for implementing a shell interface is to have the parent process\n\ufb01rst read what the user enters on the command line (in this case, cat\nprog.c), and then create a separate child process that performs the command.\nUnless otherwise speci\ufb01ed, the parent process waits for the child to exit\nbefore continuing. This is similar in functionality to the new process creation\nillustrated in Figure 3.10. However, UNIX shells typically also allow the child\nprocess to run in the background, or concurrently. To accomplish this, we add\nan ampersand ( &)a tt h ee n do ft h ec o m m a n d .T h u s ,i fw er e w r i t et h ea b o v e\ncommand as\nosh> cat prog.c &\nthe parent and child processes will run concurrently.\nThe separate child process is created using thefork() system call, and the\nuser\u2019s command is executed using one of the system calls in theexec() family\n(as described in Section 3.3.1).\nA C program that provides the general operations of a command-line shell\nis supplied in Figure 3.36. The main() function presents the prompt osh->\nand outlines the steps to be taken after input from the user has been read. The\nmain() function continually loops as long as should\n run equals 1; when the\nuser enters exit at the prompt, your program will set should\n run to 0 and\nterminate.\nThis project is organized into two parts: (1) creating the child process and\nexecuting the command in the child, and (2) modifying the shell to allow a\nhistory feature.", "158 Chapter 3 Processes\n#include <stdio.h>\n#include <unistd.h>\n#define MAX\n LINE 80 /* The maximum length command */\nint main(void)\n{\nchar *args[MAX\n LINE/2 + 1]; /* command line arguments */\nint should\n run = 1; /* flag to determine when to exit program */\nwhile (should\n run) {\nprintf(\"osh>\");\nfflush(stdout);\n/**\n*A f t e rr e a d i n gu s e ri n p u t ,t h es t e p sa r e :\n*( 1 )f o r kac h i l dp r o c e s su s i n gf o r k ( )\n*( 2 )t h ec h i l dp r o c e s sw i l li n v o k ee x e c v p ( )\n*( 3 )i fc o m m a n di n c l u d e d& ,p a r e n tw i l li n v o k ew a i t ( )\n*/\n}\nreturn 0;\n}\nFigure 3.36 Outline of simple shell.\nPart I\u2014 Creating a Child Process\nThe \ufb01rst task is to modify the main() function in Figure 3.36 so that a child\nprocess is forked and executes the comman d speci\ufb01ed by the user. This will\nrequire parsing what the user has entered into separate tokens and storing the\ntokens in an array of character strings (args in Figure 3.36). For example, if the\nuser enters the commandps -ael at the osh> prompt, the values stored in the\nargs array are:\nargs[0] = \"ps\"\nargs[1] = \"-ael\"\nargs[2] = NULL\nThis args array will be passed to the execvp() function, which has the\nfollowing prototype:\nexecvp(char *command, char *params[]);\nHere, command represents the command to be performed andparams stores the\nparameters to this command. For this project, the execvp() function should\nbe invoked as execvp(args[0], args). Be sure to check whether the user\nincluded an & to determine whether or not the parent process is to wait for the\nchild to exit.Programming Projects 159\nPart II\u2014Creating a History Feature\nThe next task is to modify the shell interface program so that it provides\na history feature that allows the user to access the most recently entered\ncommands. The user will be able to access up to 10 commands by using the\nfeature. The commands will be consec utively numbered starting at 1, and\nthe numbering will continue past 10. For example, if the user has entered 35\ncommands, the 10 most recent commands will be numbered 26 to 35.\nThe user will be able to list the command history by entering the command\nhistory\nat the osh> prompt. As an example, assume that the history consists of the\ncommands (from most to least recent):\nps, ls -l, top, cal, who, date\nThe command history will output:\n6p s\n5l s- l\n4t o p\n3c a l\n2w h o\n1d a t e\nYour program should support two techniques for retrieving commands\nfrom the command history:\n1. When the user enters !!,t h em o s tr e c e n tc o m m a n di nt h eh i s t o r yi s\nexecuted.\n2. When the user enters a single ! followed by an integer N,t h e Nth\ncommand in the history is executed.\nContinuing our example from above, if the user enters!!,t h eps command\nwill be performed; if the user enters !3,t h ec o m m a n dcal will be executed.\nAny command executed in this fashion should be echoed on the user\u2019s screen.\nThe command should also be placed in the history buffer as the next command.\nThe program should also manage basic error handling. If there are\nno commands in the history, entering !! should result in a message \u201cNo\ncommands in history.\u201d If there is no command corresponding to the number\nentered with the single !,t h ep r o g r a ms h o u l do u t p u t\"No such command in\nhistory.\"\nProject 2\u2014Linux Kernel Module for Listing Tasks\nIn this project, you will write a kernel module that lists all current tasks in a\nLinux system. Be sure to review the programming project in Chapter 2, which\ndeals with creating Linux kernel modules, before you begin this project. The\nproject can be completed using the Linux virtual machine provided with this\ntext.160 Chapter 3 Processes\nPart I\u2014Iterating over Tasks Linearly\nAs illustrated in Section 3.1, the PCB in Linux is represented by the structure\ntask\n struct,w h i c hi sf o u n di nt h e<linux/sched.h> include \ufb01le. In Linux,\nthe for\n each\n process() macro easily allows iteration over all current tasks\nin the system:\n#include <linux/sched.h>\nstruct task\n struct *task;\nfor\n each\n process(task) {\n/* on each iteration task points to the next task */\n}\nThe various \ufb01elds in task\n struct can then be displayed as the program loops\nthrough the for\n each\n process() macro.\nPart I Assignment\nDesign a kernel module that iterates through all tasks in the system using the\nfor\n each\n process() macro. In particular, output the task name (known as\nexecutable name), state, and process id of each task. (You will probably have\nto read through the task\n struct structure in <linux/sched.h> to obtain the\nnames of these \ufb01elds.) Write this code in the module entry point so that its\ncontents will appear in the kernel log buffer, which can be viewed using the\ndmesg command. To verify that your code is working correctly, compare the\ncontents of the kernel log buffer with the output of the following command,\nwhich lists all tasks in the system:\nps -el\nThe two values should be very similar. Because tasks are dynamic, however, it\nis possible that a few tasks may appear in one listing but not the other.\nPart II\u2014Iterating over Tasks with a Depth-First Search Tree\nThe second portion of this project involves iterating over all tasks in the system\nusing a depth-\ufb01rst search ( DFS)t r e e .( A sa ne x a m p l e :t h eDFS iteration of the\nprocesses in Figure 3.8 is 1, 8415, 8416, 9298, 9204, 2, 6, 200, 3028, 3610, 4005.)\nLinux maintains its process tree as a series of lists. Examining the\ntask\n struct in <linux/sched.h>,w es e et w ostruct list\n head objects:\nchildren\nand\nsibling", "Bibliographical Notes 161\nThese objects are pointers to a list of the task\u2019s children, as well as its sib-\nlings. Linux also maintains references to the init task (struct task\n struct\ninit\n task). Using this information as well as macro operations on lists, we\ncan iterate over the children of init as follows:\nstruct task\n struct *task;\nstruct list\n head *list;\nlist\n for\n each(list, &init\n task->children) {\ntask = list\n entry(list, struct task\n struct, sibling);\n/* task points to the next child in the list */\n}\nThe list\n for\n each() macro is passed two parameters, both of type struct\nlist\n head:\n\u2022 Ap o i n t e rt ot h eh e a do ft h el i s tt ob et r a v e r s e d\n\u2022 Ap o i n t e rt ot h eh e a dn o d eo ft h el i s tt ob et r a v e r s e d\nAt each iteration of list\n for\n each(),t h e\ufb01 r s tp a r a m e t e ri ss e tt ot h elist\nstructure of the next child. We then use this value to obtain each structure in\nthe list using the list\n entry() macro.\nPart II Assignment\nBeginning from theinit task, design a kernel module that iterates over all tasks\nin the system using a DFS tree. Just as in the \ufb01rst part of this project, output\nthe name, state, and pid of each task. Perform this iteration in the kernel entry\nmodule so that its output appears in the kernel log buffer.\nIf you output all tasks in the system, you may see many more tasks than\nappear with the ps -ael command. This is because some threads appear as\nchildren but do not show up as ordinary processes. Therefore, to check the\noutput of the DFS tree, use the command\nps -eLf\nThis command lists all tasks\u2014including threads\u2014in the system. To verify\nthat you have indeed performed an appropriateDFS iteration, you will have to\nexamine the relationships among the various tasks output by theps command.\nBibliographical Notes\nProcess creation, management, and IPC in UNIX and Windows systems,\nrespectively, are discussed in [Robbins and Robbins (2003)] and [Russinovich\nand Solomon (2009)]. [Love (2010)] covers support for processes in the Linux\nkernel, and [Hart (2005)] covers Windows systems programming in detail.\nCoverage of the multiprocess model used in Google\u2019s Chrome can be found at\nhttp://blog.chromium.org/2008/09/multi-process-architecture.html.162 Chapter 3 Processes\nMessage passing for multicore systems is discussed in [Holland and\nSeltzer (2011)]. [Baumann et al. (2009)] describe performance issues in shared-\nmemory and message-passing systems. [Vahalia (1996)] describes interprocess\ncommunication in the Mach system.\nThe implementation of RPCs is discussed by [Birrell and Nelson (1984)].\n[Staunstrup (1982)] discusses procedure calls versus message-passing com-\nmunication. [Harold (2005)] provides coverage of socket programming in\nJava.\n[Hart (2005)] and [Robbins and Robbins (2003)] cover pipes in Windows\nand UNIX systems, respectively.\nBibliography\n[Baumann et al. (2009)] A. Baumann, P . Barham, P .-E. Dagand, T. Harris,\nR. Isaacs, P . Simon, T. Roscoe, A. Sch\u00a8upbach, and A. Singhania,\u201cThe multikernel:\nan e wO Sa r c h i t e c t u r ef o rs c a l a b l em u l t i c o r es y s t e m s\u201d (2009), pages 29\u201344.\n[Birrell and Nelson (1984)] A. D. Birrell and B. J. Nelson, \u201cImplementing\nRemote Procedure Calls \u201d, ACM Transactions on Computer Systems,V o l u m e2 ,\nNumber 1 (1984), pages 39\u201359.\n[Harold (2005)] E. R. Harold, Java Network Programming,Third Edition, O\u2019Reilly\n&A s s o c i a t e s( 2 0 0 5 ) .\n[Hart (2005)] J. M. Hart, Windows System Programming,Third Edition, Addison-\nWesley (2005).\n[Holland and Seltzer (2011)] D. Holland and M. Seltzer,\u201cMulticore OSes: look-\ning forward from 1991, er, 2011 \u201d, Proceedings of the 13th USENIX conference on\nHot topics in operating systems(2011), pages 33\u201333.\n[Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developer\u2019s\nLibrary (2010).\n[Robbins and Robbins (2003)] K. Robbins and S. Robbins, Unix Systems Pro-\ngramming: Communication, Concurrency and Threads,Second Edition, Prentice\nHall (2003).\n[Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon,Win-\ndows Internals: Including Windows Server 2008 and Windows Vista,Fifth Edition,\nMicrosoft Press (2009).\n[Staunstrup (1982)] J. Staunstrup, \u201cMessage Passing Communication Versus\nProcedure Call Communication\u201d, Software\u2014Practice and Experience,V o l u m e1 2 ,\nNumber 3 (1982), pages 223\u2013234.\n[Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers,P r e n t i c eH a l l\n(1996).4\nCHAPTER\nThreads\nThe process model introduced in Chapter 3 assumed that a process was\nan executing program with a single thread of control. Virtually all modern\noperating systems, however, provide features enabling a process to contain\nmultiple threads of control. In this chapter, we introduce many concepts\nassociated with multithreaded computer systems, including a discussion of\nthe APIsf o rt h eP t h r e a d s ,W i n d o w s ,a n dJ a v at h r e a dl i b r a r i e s .W el o o ka ta\nnumber of issues related to multithreaded programming and its effect on the\ndesign of operating systems. Finally, we explore how the Windows and Linux\noperating systems support threads at the kernel level.\nCHAPTER OBJECTIVES\n\u2022 To introduce the notion of a thread \u2014 a fundamental unit ofCPU utilization\nthat forms the basis of multithreaded computer systems.\n\u2022 To discuss theAPIsf o rt h eP t h r e a d s ,W i n d o w s ,a n dJ a v at h r e a dl i b r a r i e s .\n\u2022 To explore several strategies that provide implicit threading.\n\u2022 To examine issues related to multithreaded programming.\n\u2022 To cover operating system support for threads in Windows and Linux.\n4.1 Overview\nAt h r e a di sab a s i cu n i to fCPU utilization; it comprises a thread ID,ap r o g r a m\ncounter, a register set, and a stack. It shares with other threads belonging\nto the same process its code section, data section, and other operating-system\nresources, such as open \ufb01les and signals. A traditional (orheavyweight) process\nhas a single thread of control. If a process has multiple threads of control, it\ncan perform more than one task at a time. Figure 4.1 illustrates the difference\nbetween a traditional single-threaded process and a multithreaded process.\n4.1.1 Motivation\nMost software applications that run on modern computers are multithreaded.\nAn application typically is implemented as a separate process with several\n163", "164 Chapter 4 Threads\nregisters\ncode data files\nstack registers registers registers\ncode data files\nstackstackstack\nthread thread\nsingle-threaded process multithreaded process\nFigure 4.1 Single-threaded and multithreaded processes.\nthreads of control. A web browser might have one thread display images or\ntext while another thread retrieves data from the network, for example. A\nword processor may have a thread for displaying graphics, another thread for\nresponding to keystrokes from the user, and a third thread for performing\nspelling and grammar checking in the background. Applications can also\nbe designed to leverage processing capabilities on multicore systems. Such\napplications can perform several CPU-intensive tasks in parallel across the\nmultiple computing cores.\nIn certain situations, a single application may be required to perform\nseveral similar tasks. For example, a web server accepts client requests for\nweb pages, images, sound, and so forth. A busy web server may have several\n(perhaps thousands of) clients concurrently accessing it. If the web server ran\nas a traditional single-threaded process, it would be able to service only one\nclient at a time, and a client might have to wait a very long time for its request\nto be serviced.\nOne solution is to have the server run as a single process that accepts\nrequests. When the server receives a request, it creates a separate process\nto service that request. In fact, this process-creation method was in common\nuse before threads became popular. Process creation is time consuming and\nresource intensive, however. If the new process will perform the same tasks as\nthe existing process, why incur all that overhead? It is generally more ef\ufb01cient\nto use one process that contains multiple threads. If the web-server process is\nmultithreaded, the server will create a separate thread that listens for client\nrequests. When a request is made, rather th an creating another process, the\nserver creates a new thread to service the request and resume listening for\nadditional requests. This is illustrated in Figure 4.2.\nThreads also play a vital role in remote procedure call (RPC)s y s t e m s .R e c a l l\nfrom Chapter 3 that RPCsa l l o wi n t e r p r o c e s sc o m m u n i c a t i o nb yp r o v i d i n ga\ncommunication mechanism similar to ordinary function or procedure calls.\nTypically,RPC servers are multithreaded. When a server receives a message, it4.1 Overview 165\nclient\n(1) request\n(2) create new\nthread to service\nthe request\n(3) resume listening\nfor additional\nclient requests\nserver thread\nFigure 4.2 Multithreaded server architecture.\nservices the message using a separate thread. This allows the server to service\nseveral concurrent requests.\nFinally, most operating-system kernels are now multithreaded. Several\nthreads operate in the kernel, and ea ch thread performs a speci\ufb01c task, such\nas managing devices, managing memory, or interrupt handling. For example,\nSolaris has a set of threads in the kernel speci\ufb01cally for interrupt handling;\nLinux uses a kernel thread for managing the amount of free memory in the\nsystem.\n4.1.2 Bene\ufb01ts\nThe bene\ufb01ts of multithreaded programmin gc a nb eb r o k e nd o w ni n t of o u r\nmajor categories:\n1. Responsiveness.M u l t i t h r e a d i n ga ni n t e r a c t i v ea p p l i c a t i o nm a ya l l o w\na program to continue running even if part of it is blocked or is\nperforming a lengthy operation, thereby increasing responsiveness to\nthe user. This quality is especially useful in designing user interfaces. For\ninstance, consider what happens when a user clicks a button that results\nin the performance of a time-consuming operation. A single-threaded\napplication would be unresponsive to the user until the operation had\ncompleted. In contrast, if the time-consuming operation is performed in\nas e p a r a t et h r e a d ,t h ea p p l i c a t i o nr e m a i n sr e s p o n s i v et ot h eu s e r .\n2. Resource sharing.P r o c e s s e sc a no n l ys h a r er e s o u r c e st h r o u g ht e c h n i q u e s\nsuch as shared memory and message passing. Such techniques must\nbe explicitly arranged by the programmer. However, threads share the\nmemory and the resources of the process to which they belong by default.\nThe bene\ufb01t of sharing code and data is that it allows an application to\nhave several different threads of activity within the same address space.\n3. Economy.A l l o c a t i n gm e m o r ya n dr e s o u r c e sf o rp r o c e s sc r e a t i o ni sc o s t l y .\nBecause threads share the resources of the process to which they belong,\nit is more economical to create and context-switch threads. Empirically\ngauging the difference in overhead can be dif\ufb01cult, but in general it is\nsigni\ufb01cantly more time consuming to create and manage processes than\nthreads. In Solaris, for example, creating a process is about thirty times166 Chapter 4 Threads\nT1 T2 T3 T4 T1 T2 T3 T4 T1single core\ntime\n\u2026\nFigure 4.3 Concurrent execution on a single-core system.\nslower than is creating a thread, and context switching is about \ufb01ve times\nslower.\n4. Scalability. The bene\ufb01ts of multithreading can be even greater in a\nmultiprocessor architecture, where threads may be running in parallel\non different processing cores. A single-threaded process can run on only\none processor, regardless how many are available. We explore this issue\nfurther in the following section.\n4.2 Multicore Programming\nEarlier in the history of computer design, in response to the need for more\ncomputing performance, single-CPU systems evolved into multi-CPU systems.\nAm o r er e c e n t ,s i m i l a rt r e n di ns y s t e md e s i g ni st op l a c em u l t i p l ec o m p u t i n g\ncores on a single chip. Each core appears as a separate processor to the\noperating system (Section 1.3.2). Whether the cores appear acrossCPU chips or\nwithin CPU chips, we call these systems multicore or multiprocessor systems.\nMultithreaded programming provides a mechanism for more ef\ufb01cient use\nof these multiple computing cores and improved concurrency. Consider an\napplication with four threads. On a system with a single computing core,\nconcurrency merely means that the execution of the threads will be interleaved\nover time (Figure 4.3), because the processing core is capable of executing only\none thread at a time. On a system with multiple cores, however, concurrency\nmeans that the threads can run in parallel, because the system can assign a\nseparate thread to each core (Figure 4.4).\nNotice the distinction between parallelism and concurrency in this discus-\nsion. A system is parallel if it can perform more than one task simultaneously.\nIn contrast, a concurrent system supports more than one task by allowing all\nthe tasks to make progress. Thus, it is possible to have concurrency without\nparallelism. Before the advent of SMP and multicore architectures, most com-\nputer systems had only a single processor. CPU schedulers were designed to\nprovide the illusion of parallelism by rapidly switching between processes in\nT1 T3 T1 T3 T1core 1\nT2 T4 T2 T4 T2core 2\ntime\n\u2026\n\u2026\nFigure 4.4 Parallel execution on a multicore system.", "4.2 Multicore Programming 167\nAMDAHL\u2019S LAW\nAmdahl\u2019s Law is a formula that identi\ufb01es potential performance gains from\nadding additional computing cores to an application that has both serial\n(nonparallel) and parallel components. If S is the portion of the application\nthat must be performed serially on a system with N processing cores, the\nformula appears as follows:\nspeedup \u2264 1\nS + (1\u2212S)\nN\nAs an example, assume we have an application that is 75 percent parallel and\n25 percent serial. If we run this application on a system with two processing\ncores, we can get a speedup of 1.6 times. If we add two additional cores (for\na total of four), the speedup is 2.28 times.\nOne interesting fact about Amdahl\u2019s Law is that asN approaches in\ufb01nity,\nthe speedup converges to 1 /S.F o re x a m p l e ,i f4 0p e r c e n to fa na p p l i c a t i o n\nis performed serially, the maximum speedup is 2.5 times, regardless of\nthe number of processing cores we add. This is the fundamental principle\nbehind Amdahl\u2019s Law: the serial portion of an application can have a\ndisproportionate effect on the performance we gain by adding additional\ncomputing cores.\nSome argue that Amdahl\u2019s Law does not take into account the hardware\nperformance enhancements used in the design of contemporary multicore\nsystems. Such arguments suggest Amdahl\u2019s Law may cease to be applicable\nas the number of processing cores continues to increase on modern computer\nsystems.\nthe system, thereby allowing each process to make progress. Such processes\nwere running concurrently, but not in parallel.\nAs systems have grown from tens of threads to thousands of threads, CPU\ndesigners have improved system performance by adding hardware to improve\nthread performance. Modern Intel CPUs frequently support two threads per\ncore, while the Oracle T4 CPU supports eight threads per core. This support\nmeans that multiple threads can be loaded into the core for fast switching.\nMulticore computers will no doubt continue to increase in core counts and\nhardware thread support.\n4.2.1 Programming Challenges\nThe trend towards multicore systems continues to place pressure on system\ndesigners and application programmers to make better use of the multiple\ncomputing cores. Designers of operating systems must write scheduling\nalgorithms that use multiple processing cores to allow the parallel execution\nshown in Figure 4.4. For application programmers, the challenge is to modify\nexisting programs as well as design new programs that are multithreaded.\nIn general, \ufb01ve areas present challenges in programming for multicore\nsystems:168 Chapter 4 Threads\n1. Identifying tasks . This involves examining applications to \ufb01nd areas\nthat can be divided into separate, concurrent tasks. Ideally, tasks are\nindependent of one another and thus can run in parallel on individual\ncores.\n2. Balance.W h i l ei d e n t i f y i n gt a s k st h a tc a nr u ni np a r a l l e l ,p r o g r a m m e r s\nmust also ensure that the tasks perform equal work of equal value. In\nsome instances, a certain task may not contribute as much value to the\noverall process as other tasks. Using a separate execution core to run that\ntask may not be worth the cost.\n3. Data splitting.J u s ta sa p p l i c a t i o n sa r ed i v i d e di n t os e p a r a t et a s k s ,t h e\ndata accessed and manipulated by the tasks must be divided to run on\nseparate cores.\n4. Data dependency.T h ed a t aa c c e s s e db yt h et a s k sm u s tb ee x a m i n e df o r\ndependencies between two or mor et a s k s .W h e no n et a s kd e p e n d so n\ndata from another, programmers must ensure that the execution of the\ntasks is synchronized to accommodate the data dependency. We examine\nsuch strategies in Chapter 5.\n5. Testing and debugging .W h e nap r o g r a mi sr u n n i n gi np a r a l l e lo n\nmultiple cores, many different execution paths are possible. Testing and\ndebugging such concurrent programs is inherently more dif\ufb01cult than\ntesting and debugging single-threaded applications.\nBecause of these challenges, many software developers argue that the advent of\nmulticore systems will require an entirely new approach to designing software\nsystems in the future. (Similarly, many computer science educators believe that\nsoftware development must be taught with increased emphasis on parallel\nprogramming.)\n4.2.2 Types of Parallelism\nIn general, there are two types of parallelism: data parallelism and task\nparallelism. Data parallelism focuses on distributing subsets of the same data\nacross multiple computing cores and performing the same operation on each\ncore. Consider, for example, summing the contents of an array of size N.O na\nsingle-core system, one thread would simply sum the elements [0] . . . [N \u2212 1].\nOn a dual-core system, however, thread A,r u n n i n go nc o r e0 ,c o u l ds u mt h e\nelements [0] . . . [N/2 \u2212 1] while thread B,r u n n i n go nc o r e1 ,c o u l ds u mt h e\nelements [ N/2] . . . [N \u2212 1]. The two threads would be running in parallel on\nseparate computing cores.\nTask parallelism involves distributing not data but tasks (threads) across\nmultiple computing cores. Each thread is performing a unique operation.\nDifferent threads may be operating on the same data, or they may be operating\non different data. Consider again our example above. In contrast to that\nsituation, an example of task parallelism might involve two threads, each\nperforming a unique statistical operation on the array of elements. The threads\nagain are operating in parallel on separate computing cores, but each is\nperforming a unique operation.4.3 Multithreading Models 169\nFundamentally, then, data parallelism involves the distribution of data\nacross multiple cores and task parallelism on the distribution of tasks across\nmultiple cores. In practice, however, few applications strictly follow either data\nor task parallelism. In most instances, applications use a hybrid of these two\nstrategies.\n4.3 Multithreading Models\nOur discussion so far has treated threads in a generic sense. However, support\nfor threads may be provided either at the user level, foruser threads,o rb yt h e\nkernel, for kernel threads.U s e rt h r e a d sa r es u p p o r t e da b o v et h ek e r n e la n d\nare managed without kernel support, whereas kernel threads are supported\nand managed directly by the operating system. Virtually all contemporary\noperating systems\u2014including Windows, Linux, Mac OS X ,a n dS o l a r i s \u2014\nsupport kernel threads.\nUltimately, a relationship must exist betw een user threads and kernel\nthreads. In this section, we look at three common ways of establishing such a\nrelationship: the many-to-one model, the one-to-one model, and the many-to-\nmany model.\n4.3.1 Many-to-One Model\nThe many-to-one model (Figure 4.5) maps many user-level threads to one\nkernel thread. Thread management is done by the thread library in user space,\nso it is ef\ufb01cient (we discuss thread libraries in Section 4.4). However, the entire\nprocess will block if a thread makes a blocking system call. Also, because only\none thread can access the kernel at a time, multiple threads are unable to run in\nparallel on multicore systems. Green threads \u2014a thread library available for\nSolaris systems and adopted in early versions of Java\u2014used the many-to-one\nmodel. However, very few systems continue to use the model because of its\ninability to take advantage of multiple processing cores.\nuser thread\nkernel threadk\nFigure 4.5 Many-to-one model.", "170 Chapter 4 Threads\nuser thread\nkernel threadkkkk\nFigure 4.6 One-to-one model.\n4.3.2 One-to-One Model\nThe one-to-one model (Figure 4.6) maps each user thread to a kernel thread. It\nprovides more concurrency than the many-to-one model by allowing another\nthread to run when a thread makes a blocking system call. It also allows\nmultiple threads to run in parallel on multiprocessors. The only drawback to\nthis model is that creating a user thread requires creating the corresponding\nkernel thread. Because the overhead of creating kernel threads can burden the\nperformance of an application, most implementations of this model restrict the\nnumber of threads supported by the system. Linux, along with the family of\nWindows operating systems, implement the one-to-one model.\n4.3.3 Many-to-Many Model\nThe many-to-many model (Figure 4.7) multiplexes many user-level threads to\nas m a l l e ro re q u a ln u m b e ro fk e r n e lt h r eads. The number of kernel threads\nmay be speci\ufb01c to either a particular application or a particular machine (an\napplication may be allocated more kernel threads on a multiprocessor than on\nas i n g l ep r o c e s s o r ) .\nLet\u2019s consider the effect of this design on concurrency. Whereas the many-\nto-one model allows the developer to create as many user threads as she wishes,\nit does not result in true concurrency, because the kernel can schedule only\none thread at a time. The one-to-one model allows greater concurrency, but the\ndeveloper has to be careful not to create too many threads within an application\n(and in some instances may be limited in the number of threads she can\nuser thread\nkernel threadkkk\nFigure 4.7 Many-to-many model.4.4 Thread Libraries 171\nuser thread\nkernel threadkkk k\nFigure 4.8 Two-level model.\ncreate). The many-to-many model suffers from neither of these shortcomings:\ndevelopers can create as many user threads as necessary, and the corresponding\nkernel threads can run in parallel on a multiprocessor. Also, when a thread\nperforms a blocking system call, the kernel can schedule another thread for\nexecution.\nOne variation on the many-to-many model still multiplexes many user-\nlevel threads to a smaller or equal number of kernel threads but also allows a\nuser-level thread to be bound to a kernel thread. This variation is sometimes\nreferred to as the two-level model (Figure 4.8). The Solaris operating system\nsupported the two-level model in versions older than Solaris 9. However,\nbeginning with Solaris 9, this system uses the one-to-one model.\n4.4 Thread Libraries\nA thread library provides the programmer with an API for creating and\nmanaging threads. There are two primary ways of implementing a thread\nlibrary. The \ufb01rst approach is to provide a library entirely in user space with no\nkernel support. All code and data structures for the library exist in user space.\nThis means that invoking a function in the library results in a local function\ncall in user space and not a system call.\nThe second approach is t oi m p l e m e n tak e r n e l - l e v e ll i b r a r ys u p p o r t e d\ndirectly by the operating system. In this case, code and data structures for\nthe library exist in kernel space. Invoking a function in the API for the library\ntypically results in a system call to the kernel.\nThree main thread libraries are in use today:POSIX Pthreads, Windows, and\nJava. Pthreads, the threads extension of the POSIX standard, may be provided\nas either a user-level or a kernel-level library. The Windows thread library\nis a kernel-level library available on Windows systems. The Java thread API\nallows threads to be created and managed directly in Java programs. However,\nbecause in most instances theJVM is running on top of a host operating system,\nthe Java thread API is generally implemented using a thread library available\non the host system. This means that on Windows systems, Java threads are\ntypically implemented using the Windows API; UNIX and Linux systems often\nuse Pthreads.172 Chapter 4 Threads\nFor POSIX and Windows threading, any data declared globally\u2014that is,\ndeclared outside of any function\u2014are shared among all threads belonging to\nthe same process. Because Java has no notion of global data, access to shared\ndata must be explicitly arranged between threads. Data declared local to a\nfunction are typically stored on the stack. Since each thread has its own stack,\neach thread has its own copy of local data.\nIn the remainder of this section, we describe basic thread creation using\nthese three thread libraries. As an illustrative example, we design a multi-\nthreaded program that performs the summation of a non-negative integer in a\nseparate thread using the well-known summation function:\nsum =\nN\u2211\ni=0\ni\nFor example, if N were 5, this function would represent the summation of\nintegers from 0 to 5, which is 15. Each of the three programs will be run with\nthe upper bounds of the summation entered on the command line. Thus, if the\nuser enters 8, the summation of the integer values from 0 to 8 will be output.\nBefore we proceed with our examples of thread creation, we introduce\ntwo general strategies for creating multiple th reads: asynchronous threading\nand synchronous threading. With asynchronous threading, once the parent\ncreates a child thread, the parent resumes its execution, so that the parent\nand child execute concurrently. Each thread runs independently of every other\nthread, and the parent thread need notk n o ww h e ni t sc h i l dt e r m i n a t e s .B e c a u s e\nthe threads are independent, there is typically little data sharing between\nthreads. Asynchronous threading is the strategy used in the multithreaded\nserver illustrated in Figure 4.2.\nSynchronous threading occurs when the parent thread creates one or more\nchildren and then must wait for all of its children to terminate before it resumes\n\u2014the so-called fork-join strategy. Here, the threads created by the parent\nperform work concurrently, but the parent cannot continue until this work\nhas been completed. Once each thread has \ufb01nished its work, it terminates\nand joins with its parent. Only after all of the children have joined can the\nparent resume execution. Typically, synchronous threading involves signi\ufb01cant\ndata sharing among threads. For example, the parent thread may combine the\nresults calculated by its various children. All of the following examples use\nsynchronous threading.\n4.4.1 Pthreads\nPthreads refers to the POSIX standard (IEEE 1003.1c) de\ufb01ning an API for thread\ncreation and synchronization. This is a speci\ufb01cation for thread behavior,\nnot an implementation.O p e r a t i n g - s y s t e md e s i g n e r sm a yi m p l e m e n tt h e\nspeci\ufb01cation in any way they wish. Numerous systems implement the Pthreads\nspeci\ufb01cation; most are UNIX-type systems, including Linux, Mac OS X ,a n d\nSolaris. Although Windows doesn\u2019t support Pthreads natively, some third-\nparty implementations for Windows are available.\nThe C program shown in Figure 4.9 demonstrates the basic PthreadsAPI for\nconstructing a multithreaded program that calculates the summation of a non-\nnegative integer in a separate thread. In a Pthreads program, separate threads"]